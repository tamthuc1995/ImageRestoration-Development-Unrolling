{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6e5485",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca97087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "# ROOT_PROJECT = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling/\"\n",
    "# ROOT_DATASET = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling\"\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader import ImageSuperResolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe5ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, nchannels):\n",
    "        super(CustomLayerNorm, self).__init__()\n",
    "        \n",
    "        self.nchannels = nchannels\n",
    "        self.weighted_transform = nn.Conv2d(nchannels, nchannels, kernel_size=1, stride=1, groups=nchannels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bz, nchannels, h, w = x.shape\n",
    "        sigma = x.var(dim=1, keepdim=True, correction=1)\n",
    "        # bz, 1, h, w = sigma.shape\n",
    "        return self.weighted_transform(x / torch.sqrt(sigma+1e-5))\n",
    "    \n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "        # hidden_features = dim\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = nn.functional.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class FFBlock(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FFBlock, self).__init__()\n",
    "\n",
    "        self.norm = CustomLayerNorm(dim)\n",
    "\n",
    "        self.skip_connect_weight_final = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32) * torch.tensor([0.5, 0.5]),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.skip_connect_weight_final[0]*x + self.skip_connect_weight_final[1]*self.ffn(self.norm(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "        # self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        #                           nn.PixelUnshuffle(2))\n",
    "        \n",
    "        # self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat, kernel_size=3, stride=2, padding=1, padding_mode=\"replicate\", bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "        # self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        #                           nn.PixelShuffle(2))\n",
    "\n",
    "        # self.body = nn.Sequential(nn.ConvTranspose2d(n_feat, n_feat, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "##########################################################################\n",
    "# class FeatureExtraction(nn.Module):\n",
    "#     def __init__(self, \n",
    "#         inp_channels=3, \n",
    "#         out_channels=48, \n",
    "#         dim = 48,\n",
    "#         num_blocks = [2,2,2,2], \n",
    "#         num_refinement_blocks = 4,\n",
    "#         ffn_expansion_factor = 2.0,\n",
    "#         bias = False,\n",
    "#     ):\n",
    "\n",
    "#         super(FeatureExtraction, self).__init__()\n",
    "\n",
    "#         self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "#         self.encoder_level1 = nn.Sequential(*[FFBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "#         self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "#         self.encoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "#         self.down2_3 = Downsample(int(dim)) ## From Level 2 to Level 3\n",
    "#         self.encoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "#         self.down3_4 = Downsample(int(dim)) ## From Level 3 to Level 4\n",
    "#         self.latent = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[3])])\n",
    "        \n",
    "#         self.up4_3 = Upsample(int(dim)) ## From Level 4 to Level 3\n",
    "#         self.reduce_chan_level3 = nn.Conv2d(int(dim*2), int(dim), kernel_size=1, bias=bias)\n",
    "#         self.decoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "#         self.up3_2 = Upsample(int(dim)) ## From Level 3 to Level 2\n",
    "#         self.reduce_chan_level2 = nn.Conv2d(int(dim*2), int(dim), kernel_size=1, bias=bias)\n",
    "#         self.decoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "#         self.up2_1 = Upsample(int(dim))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "\n",
    "#         self.decoder_level1 = nn.Sequential(*[FFBlock(dim=int(dim*2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "#         self.refinement = nn.Sequential(*[FFBlock(dim=int(dim*2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_refinement_blocks)])\n",
    "\n",
    "#         ###########################\n",
    "#         self.output = nn.Conv2d(int(dim*2), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "#     def forward(self, inp_img):\n",
    "\n",
    "#         inp_enc_level1 = self.patch_embed(inp_img)\n",
    "#         out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "        \n",
    "#         inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "#         out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "#         inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "#         out_enc_level3 = self.encoder_level3(inp_enc_level3) \n",
    "\n",
    "#         inp_enc_level4 = self.down3_4(out_enc_level3)        \n",
    "#         latent = self.latent(inp_enc_level4) \n",
    "                        \n",
    "#         inp_dec_level3 = self.up4_3(latent)\n",
    "#         inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
    "#         inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
    "#         out_dec_level3 = self.decoder_level3(inp_dec_level3) \n",
    "\n",
    "#         inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "#         inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
    "#         inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
    "#         out_dec_level2 = self.decoder_level2(inp_dec_level2) \n",
    "\n",
    "#         inp_dec_level1 = self.up2_1(out_dec_level2)\n",
    "#         inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "#         out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "        \n",
    "#         out_dec_level1 = self.refinement(out_dec_level1)\n",
    "#         out_dec_level1 = self.output(out_dec_level1)\n",
    "\n",
    "#         # return [latent, out_dec_level3, out_dec_level2, out_dec_level1]\n",
    "#         return [out_dec_level1]\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(self, \n",
    "        inp_channels=3, \n",
    "        out_channels=48, \n",
    "        dim = 48,\n",
    "        num_blocks = [1,2,2,4], \n",
    "        num_refinement_blocks = 4,\n",
    "        ffn_expansion_factor = 2.66,\n",
    "        bias = False,\n",
    "    ):\n",
    "\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[FFBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "        self.encoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "        # self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
    "        # self.encoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim*2**2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "        # self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
    "        # self.latent = nn.Sequential(*[FFBlock(dim=int(dim*2**3), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[3])])\n",
    "        \n",
    "        # self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
    "        # self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
    "        # self.decoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim*2**2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "        # self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
    "        # self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
    "        # self.decoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "\n",
    "        self.decoder_level1 = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.refinement = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_refinement_blocks)])\n",
    "\n",
    "        ###########################\n",
    "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "        \n",
    "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "        latent = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "        # inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "        # out_enc_level3 = self.encoder_level3(inp_enc_level3) \n",
    "\n",
    "        # inp_enc_level4 = self.down3_4(out_enc_level3)        \n",
    "        # latent = self.latent(inp_enc_level4) \n",
    "                        \n",
    "        # inp_dec_level3 = self.up4_3(latent)\n",
    "        # inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
    "        # inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
    "        # out_dec_level3 = self.decoder_level3(inp_dec_level3) \n",
    "\n",
    "        # inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "        # inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
    "        # inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
    "        # out_dec_level2 = self.decoder_level2(inp_dec_level2) \n",
    "\n",
    "        inp_dec_level1 = self.up2_1(latent)\n",
    "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "        \n",
    "        out_dec_level1 = self.refinement(out_dec_level1)\n",
    "        out_dec_level1 = self.output(out_dec_level1)\n",
    "\n",
    "        return [out_dec_level1]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GLRFast(nn.Module):\n",
    "    def __init__(self, \n",
    "            n_channels, n_node_fts, n_graphs, connection_window, device, M_diag_init=0.4\n",
    "        ):\n",
    "        super(GLRFast, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.n_channels        = n_channels\n",
    "        self.n_node_fts        = n_node_fts\n",
    "        self.n_graphs          = n_graphs\n",
    "        self.n_edges           = (connection_window == 1).sum()\n",
    "        self.connection_window = connection_window\n",
    "        self.buffer_size       = connection_window.sum()\n",
    "\n",
    "        # edges type from connection_window\n",
    "        window_size = connection_window.shape[0]\n",
    "        connection_window = connection_window.reshape((-1))\n",
    "        m = np.arange(window_size)-window_size//2\n",
    "        edge_delta = np.array(\n",
    "            list(itertools.product(m, m)),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.edge_delta = edge_delta[connection_window == 1]\n",
    "        \n",
    "        self.pad_dim_hw = np.abs(self.edge_delta.min(axis=0))\n",
    "\n",
    "\n",
    "        kernel01 = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel01[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p01 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 1.0,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel01 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02a = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 1.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02a[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02a = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02a = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02b = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02b[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02b = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02b = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel03 = torch.tensor([\n",
    "            [0.0, -1.0, 0.0],\n",
    "            [-1.0, 4.0, -1.0],\n",
    "            [0.0, -1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel03[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p03 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel03 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        ### Trainable parameters\n",
    "        # features on nodes #self.n_node_fts\n",
    "        self.multiM = Parameter(\n",
    "            torch.ones((self.n_graphs, self.n_node_fts), device=self.device, dtype=torch.float32)*M_diag_init,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def get_neighbors_pixels(self, img_features):\n",
    "        _, _, H, W = img_features.shape\n",
    "        padH, padW = self.pad_dim_hw\n",
    "        img_features_frame = nn.functional.pad(img_features, (padW, padW, padH, padH), \"replicate\")\n",
    "        # neighbors_pixels = []\n",
    "        # for shift_h, shift_w in self.edge_delta:\n",
    "        #     fromh = padH + shift_h\n",
    "        #     toh = padH + shift_h + H\n",
    "        #     fromw = padW + shift_w\n",
    "        #     tow = padW + shift_w + W\n",
    "            \n",
    "        #     neighbors_pixels.append(\n",
    "        #         img_features_frame[:, :, fromh:toh, fromw:tow]\n",
    "        #     )\n",
    "        # neighbors_pixels_features = torch.stack(neighbors_pixels, axis=-3)\n",
    "\n",
    "        neighbors_pixels_features = torch.stack([\n",
    "            img_features_frame[:, :, padH + self.edge_delta[0, 0]:padH + self.edge_delta[0, 0] + H, padW + self.edge_delta[0, 1]:padW + self.edge_delta[0, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[1, 0]:padH + self.edge_delta[1, 0] + H, padW + self.edge_delta[1, 1]:padW + self.edge_delta[1, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[2, 0]:padH + self.edge_delta[2, 0] + H, padW + self.edge_delta[2, 1]:padW + self.edge_delta[2, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[3, 0]:padH + self.edge_delta[3, 0] + H, padW + self.edge_delta[3, 1]:padW + self.edge_delta[3, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[4, 0]:padH + self.edge_delta[4, 0] + H, padW + self.edge_delta[4, 1]:padW + self.edge_delta[4, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[5, 0]:padH + self.edge_delta[5, 0] + H, padW + self.edge_delta[5, 1]:padW + self.edge_delta[5, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[6, 0]:padH + self.edge_delta[6, 0] + H, padW + self.edge_delta[6, 1]:padW + self.edge_delta[6, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[7, 0]:padH + self.edge_delta[7, 0] + H, padW + self.edge_delta[7, 1]:padW + self.edge_delta[7, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[8, 0]:padH + self.edge_delta[8, 0] + H, padW + self.edge_delta[8, 1]:padW + self.edge_delta[8, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[9, 0]:padH + self.edge_delta[9, 0] + H, padW + self.edge_delta[9, 1]:padW + self.edge_delta[9, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[10, 0]:padH + self.edge_delta[10, 0] + H, padW + self.edge_delta[10, 1]:padW + self.edge_delta[10, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[11, 0]:padH + self.edge_delta[11, 0] + H, padW + self.edge_delta[11, 1]:padW + self.edge_delta[11, 1] + W]\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[12, 0]:padH + self.edge_delta[12, 0] + H, padW + self.edge_delta[12, 1]:padW + self.edge_delta[12, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[13, 0]:padH + self.edge_delta[13, 0] + H, padW + self.edge_delta[13, 1]:padW + self.edge_delta[13, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[14, 0]:padH + self.edge_delta[14, 0] + H, padW + self.edge_delta[14, 1]:padW + self.edge_delta[14, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[15, 0]:padH + self.edge_delta[15, 0] + H, padW + self.edge_delta[15, 1]:padW + self.edge_delta[15, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[16, 0]:padH + self.edge_delta[16, 0] + H, padW + self.edge_delta[16, 1]:padW + self.edge_delta[16, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[17, 0]:padH + self.edge_delta[17, 0] + H, padW + self.edge_delta[17, 1]:padW + self.edge_delta[17, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[18, 0]:padH + self.edge_delta[18, 0] + H, padW + self.edge_delta[18, 1]:padW + self.edge_delta[18, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[19, 0]:padH + self.edge_delta[19, 0] + H, padW + self.edge_delta[19, 1]:padW + self.edge_delta[19, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[20, 0]:padH + self.edge_delta[20, 0] + H, padW + self.edge_delta[20, 1]:padW + self.edge_delta[20, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[21, 0]:padH + self.edge_delta[21, 0] + H, padW + self.edge_delta[21, 1]:padW + self.edge_delta[21, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[22, 0]:padH + self.edge_delta[22, 0] + H, padW + self.edge_delta[22, 1]:padW + self.edge_delta[22, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[23, 0]:padH + self.edge_delta[23, 0] + H, padW + self.edge_delta[23, 1]:padW + self.edge_delta[23, 1] + W]\n",
    "        ], axis=-3)\n",
    "        return neighbors_pixels_features\n",
    "\n",
    "    def normalize_and_transform_features(self, img_features):\n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "        # img_features = img_features.view(batch_size, self.n_graphs, self.n_node_fts, h_size, w_size)\n",
    "        img_features = torch.nn.functional.normalize(img_features, dim=2)\n",
    "\n",
    "        img_features_transform = torch.einsum(\n",
    "            \"bhcHW, hc -> bhcHW\", img_features, self.multiM\n",
    "        )\n",
    "    \n",
    "        img_features_transform = img_features_transform.view(batch_size, self.n_graphs*self.n_node_fts, h_size, w_size)\n",
    "\n",
    "        return img_features_transform\n",
    "\n",
    "\n",
    "    def extract_edge_weights(self, img_features):\n",
    "        \n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "\n",
    "        img_features = self.normalize_and_transform_features(img_features)\n",
    "        img_features_neighbors = self.get_neighbors_pixels(img_features)\n",
    "\n",
    "        features_similarity = (img_features[:, :, None, :, :] * img_features_neighbors)\n",
    "        features_similarity = features_similarity.view(\n",
    "            batch_size, self.n_graphs, self.n_node_fts, self.n_edges, h_size, w_size\n",
    "        ).sum(axis=2)\n",
    "\n",
    "        edge_weights_norm = nn.functional.softmax(features_similarity, dim=2) \n",
    "        node_degree = edge_weights_norm.sum(axis=2)\n",
    "\n",
    "        return edge_weights_norm, node_degree\n",
    "    \n",
    "    def stats_conv(self, patchs):\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.view(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_patch = nn.functional.pad(temp_patch, (1,1,1,1), 'reflect')\n",
    "        temp_out_patch = nn.functional.conv2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "    def stats_conv_transpose(self, patchs):\n",
    "\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.reshape(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_out_patch = nn.functional.conv_transpose2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "\n",
    "    def op_L_norm(self, img_signals, edge_weights, node_degree):\n",
    "\n",
    "        batch_size, n_graphs, n_channels, h_size, w_size = img_signals.shape\n",
    "        img_features_neighbors = self.get_neighbors_pixels(\n",
    "            img_signals.view(batch_size, n_graphs*n_channels, h_size, w_size)\n",
    "        ).view(batch_size, n_graphs, n_channels, self.n_edges, h_size, w_size)\n",
    "        Wx = torch.einsum(\n",
    "            \"bhceHW, bheHW -> bhcHW\", img_features_neighbors, edge_weights\n",
    "        )\n",
    "        output = img_signals - Wx\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, patchs, edge_weights, node_degree):\n",
    "        # output_patchs = self.op_L_norm(patchs, edge_weights, node_degree)\n",
    "        patchs = self.stats_conv(patchs)\n",
    "        output_patchs = self.op_L_norm(patchs, edge_weights, node_degree)\n",
    "        output_patchs = self.stats_conv_transpose(output_patchs)\n",
    "\n",
    "        return output_patchs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GTVFast(nn.Module):\n",
    "    def __init__(self, \n",
    "            n_channels, n_node_fts, n_graphs, connection_window, device, M_diag_init=0.4\n",
    "        ):\n",
    "        super(GTVFast, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.n_channels        = n_channels\n",
    "        self.n_node_fts        = n_node_fts\n",
    "        self.n_graphs          = n_graphs\n",
    "        self.n_edges           = (connection_window == 1).sum()\n",
    "        self.connection_window = connection_window\n",
    "        self.buffer_size       = connection_window.sum()\n",
    "\n",
    "        # edges type from connection_window\n",
    "        window_size = connection_window.shape[0]\n",
    "        connection_window = connection_window.reshape((-1))\n",
    "        m = np.arange(window_size)-window_size//2\n",
    "        edge_delta = np.array(\n",
    "            list(itertools.product(m, m)),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.edge_delta = edge_delta[connection_window == 1]\n",
    "        \n",
    "        self.pad_dim_hw = np.abs(self.edge_delta.min(axis=0))\n",
    "\n",
    "        kernel01 = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel01[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p01 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 1.0,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel01 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02a = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 1.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02a[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02a = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02a = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02b = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02b[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02b = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02b = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel03 = torch.tensor([\n",
    "            [0.0, -1.0, 0.0],\n",
    "            [-1.0, 4.0, -1.0],\n",
    "            [0.0, -1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel03[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p03 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel03 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        ### Trainable parameters\n",
    "        # features on nodes #self.n_node_fts\n",
    "        self.multiM = Parameter(\n",
    "            torch.ones((self.n_graphs, self.n_node_fts), device=self.device, dtype=torch.float32)*M_diag_init,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_neighbors_pixels(self, img_features):\n",
    "        _, _, H, W = img_features.shape\n",
    "        padH, padW = self.pad_dim_hw\n",
    "        img_features_frame = nn.functional.pad(img_features, (padW, padW, padH, padH), \"replicate\")\n",
    "        # neighbors_pixels = []\n",
    "        # for shift_h, shift_w in self.edge_delta:\n",
    "        #     fromh = padH + shift_h\n",
    "        #     toh = padH + shift_h + H\n",
    "        #     fromw = padW + shift_w\n",
    "        #     tow = padW + shift_w + W\n",
    "            \n",
    "        #     neighbors_pixels.append(\n",
    "        #         img_features_frame[:, :, fromh:toh, fromw:tow]\n",
    "        #     )\n",
    "        # neighbors_pixels_features = torch.stack(neighbors_pixels, axis=-3)\n",
    "        neighbors_pixels_features = torch.stack([\n",
    "            img_features_frame[:, :, padH + self.edge_delta[0, 0]:padH + self.edge_delta[0, 0] + H, padW + self.edge_delta[0, 1]:padW + self.edge_delta[0, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[1, 0]:padH + self.edge_delta[1, 0] + H, padW + self.edge_delta[1, 1]:padW + self.edge_delta[1, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[2, 0]:padH + self.edge_delta[2, 0] + H, padW + self.edge_delta[2, 1]:padW + self.edge_delta[2, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[3, 0]:padH + self.edge_delta[3, 0] + H, padW + self.edge_delta[3, 1]:padW + self.edge_delta[3, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[4, 0]:padH + self.edge_delta[4, 0] + H, padW + self.edge_delta[4, 1]:padW + self.edge_delta[4, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[5, 0]:padH + self.edge_delta[5, 0] + H, padW + self.edge_delta[5, 1]:padW + self.edge_delta[5, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[6, 0]:padH + self.edge_delta[6, 0] + H, padW + self.edge_delta[6, 1]:padW + self.edge_delta[6, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[7, 0]:padH + self.edge_delta[7, 0] + H, padW + self.edge_delta[7, 1]:padW + self.edge_delta[7, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[8, 0]:padH + self.edge_delta[8, 0] + H, padW + self.edge_delta[8, 1]:padW + self.edge_delta[8, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[9, 0]:padH + self.edge_delta[9, 0] + H, padW + self.edge_delta[9, 1]:padW + self.edge_delta[9, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[10, 0]:padH + self.edge_delta[10, 0] + H, padW + self.edge_delta[10, 1]:padW + self.edge_delta[10, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[11, 0]:padH + self.edge_delta[11, 0] + H, padW + self.edge_delta[11, 1]:padW + self.edge_delta[11, 1] + W]\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[12, 0]:padH + self.edge_delta[12, 0] + H, padW + self.edge_delta[12, 1]:padW + self.edge_delta[12, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[13, 0]:padH + self.edge_delta[13, 0] + H, padW + self.edge_delta[13, 1]:padW + self.edge_delta[13, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[14, 0]:padH + self.edge_delta[14, 0] + H, padW + self.edge_delta[14, 1]:padW + self.edge_delta[14, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[15, 0]:padH + self.edge_delta[15, 0] + H, padW + self.edge_delta[15, 1]:padW + self.edge_delta[15, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[16, 0]:padH + self.edge_delta[16, 0] + H, padW + self.edge_delta[16, 1]:padW + self.edge_delta[16, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[17, 0]:padH + self.edge_delta[17, 0] + H, padW + self.edge_delta[17, 1]:padW + self.edge_delta[17, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[18, 0]:padH + self.edge_delta[18, 0] + H, padW + self.edge_delta[18, 1]:padW + self.edge_delta[18, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[19, 0]:padH + self.edge_delta[19, 0] + H, padW + self.edge_delta[19, 1]:padW + self.edge_delta[19, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[20, 0]:padH + self.edge_delta[20, 0] + H, padW + self.edge_delta[20, 1]:padW + self.edge_delta[20, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[21, 0]:padH + self.edge_delta[21, 0] + H, padW + self.edge_delta[21, 1]:padW + self.edge_delta[21, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[22, 0]:padH + self.edge_delta[22, 0] + H, padW + self.edge_delta[22, 1]:padW + self.edge_delta[22, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[23, 0]:padH + self.edge_delta[23, 0] + H, padW + self.edge_delta[23, 1]:padW + self.edge_delta[23, 1] + W]\n",
    "        ], axis=-3)\n",
    "        return neighbors_pixels_features\n",
    "    \n",
    "\n",
    "    def normalize_and_transform_features(self, img_features):\n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "        # img_features = img_features.view(batch_size, self.n_graphs, self.n_node_fts, h_size, w_size)\n",
    "        img_features = torch.nn.functional.normalize(img_features, dim=2)\n",
    "\n",
    "        img_features_transform = torch.einsum(\n",
    "            \"bhcHW, hc -> bhcHW\", img_features, self.multiM\n",
    "        )\n",
    "    \n",
    "        img_features_transform = img_features_transform.view(batch_size, self.n_graphs*self.n_node_fts, h_size, w_size)\n",
    "\n",
    "        return img_features_transform\n",
    "\n",
    "\n",
    "    def extract_edge_weights(self, img_features):\n",
    "        \n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "\n",
    "        img_features = self.normalize_and_transform_features(img_features)\n",
    "        img_features_neighbors = self.get_neighbors_pixels(img_features)\n",
    "\n",
    "        features_similarity = (img_features[:, :, None, :, :] * img_features_neighbors)\n",
    "        features_similarity = features_similarity.view(\n",
    "            batch_size, self.n_graphs, self.n_node_fts, self.n_edges, h_size, w_size\n",
    "        ).sum(axis=2)\n",
    "\n",
    "        edge_weights_norm = nn.functional.softmax(features_similarity, dim=2) \n",
    "        node_degree = edge_weights_norm.sum(axis=2)\n",
    "\n",
    "        return edge_weights_norm, node_degree\n",
    "\n",
    "\n",
    "    def stats_conv(self, patchs):\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.view(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_patch = nn.functional.pad(temp_patch, (1,1,1,1), 'reflect')\n",
    "        temp_out_patch = nn.functional.conv2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "    def stats_conv_transpose(self, patchs):\n",
    "\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        \n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.reshape(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_out_patch = nn.functional.conv_transpose2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "\n",
    "    def op_C(self, img_signals, edge_weights, node_degree):\n",
    "\n",
    "\n",
    "        batch_size, n_graphs, n_channels, h_size, w_size = img_signals.shape\n",
    "        # img_signals = self.stats_conv(img_signals)\n",
    "\n",
    "        img_signals = self.stats_conv(img_signals)\n",
    "\n",
    "        img_features_neighbors = self.get_neighbors_pixels(\n",
    "            img_signals.view(batch_size, n_graphs*n_channels, h_size, w_size)\n",
    "        ).view(batch_size, n_graphs, n_channels, self.n_edges, h_size, w_size)\n",
    "        Cx1 = img_signals[:, :, :, None, :, :] * edge_weights[:, :, None, :, :, :]\n",
    "        Cx2 = img_features_neighbors * edge_weights[:, :, None, :, :, :]\n",
    "        \n",
    "        output = Cx1 - Cx2\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def op_C_transpose(self, edge_signals, edge_weights, node_degree):\n",
    "\n",
    "        batch_size, n_graphs, n_channels, n_edges, H, W = edge_signals.shape\n",
    "        edge_signals = edge_signals * edge_weights[:, :, None, :, :, :]\n",
    "\n",
    "        output = edge_signals.sum(axis=3)\n",
    "\n",
    "        padH, padW = self.pad_dim_hw\n",
    "        output = nn.functional.pad(\n",
    "            output.view(batch_size, n_graphs*n_channels, H, W),\n",
    "            (padW, padW, padH, padH), \"replicate\"\n",
    "        ).view(batch_size, n_graphs, n_channels, H + 2*padH, W + 2*padW)\n",
    "\n",
    "        i=0\n",
    "        for shift_h, shift_w in self.edge_delta:\n",
    "            fromh = padH + shift_h\n",
    "            toh = padH + shift_h + H\n",
    "            fromw = padW + shift_w\n",
    "            tow = padW + shift_w + W\n",
    "            \n",
    "            output[:, :, :, fromh:toh, fromw:tow] = output[:, :, :, fromh:toh, fromw:tow] - edge_signals[:, :, :, i, :, :]\n",
    "            i+=1\n",
    "\n",
    "        output = output[:, :, :, padH:-padH, padW:-padW]\n",
    "\n",
    "        output = self.stats_conv_transpose(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, patchs, edge_weights, node_degree):\n",
    "        # C^T C\n",
    "        edges_signals = self.op_C(patchs, edge_weights, node_degree)\n",
    "        output_patchs = self.op_C_transpose(edges_signals, edge_weights, node_degree)\n",
    "\n",
    "        return output_patchs\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "class DCestimator(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, hidden_features):\n",
    "        super(DCestimator, self).__init__()\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim_in, hidden_features*2, kernel_size=1, bias=False)\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=False)\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim_out, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, patchs):\n",
    "\n",
    "        out = self.project_in(patchs)\n",
    "        out01, out02 = self.dwconv(out).chunk(2, dim=1)\n",
    "        out = nn.functional.gelu(out01) * out02\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MixtureGTV(nn.Module):\n",
    "    def __init__(self, \n",
    "            nchannels_in,\n",
    "            n_graphs,\n",
    "            n_node_fts,\n",
    "            n_cnn_fts,\n",
    "            connection_window,\n",
    "            n_cgd_iters,\n",
    "            alpha_init,\n",
    "            beta_init,\n",
    "            muy_init, ro_init, gamma_init,\n",
    "            device\n",
    "        ):\n",
    "        super(MixtureGTV, self).__init__()\n",
    "\n",
    "        self.device       = device\n",
    "        self.n_graphs     = n_graphs\n",
    "        self.n_node_fts   = n_node_fts\n",
    "        self.n_total_fts  = n_graphs * n_node_fts\n",
    "        self.n_cnn_fts    = n_cnn_fts\n",
    "        self.n_levels     = 4\n",
    "        self.n_cgd_iters  = n_cgd_iters\n",
    "        self.nchannels_in = nchannels_in\n",
    "        self.connection_window = connection_window\n",
    "\n",
    "        self.alphaCGD =  Parameter(\n",
    "            torch.ones((self.n_cgd_iters, n_graphs), device=self.device, dtype=torch.float32) * alpha_init,\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.betaCGD =  Parameter(\n",
    "            torch.ones((self.n_cgd_iters, n_graphs), device=self.device, dtype=torch.float32) * beta_init,\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.patchs_features_extraction = FeatureExtraction(\n",
    "            inp_channels=3, \n",
    "            out_channels=self.n_total_fts + 12, \n",
    "            dim=self.n_cnn_fts,\n",
    "            num_blocks = [2, 3, 3, 4], \n",
    "            num_refinement_blocks = 4,\n",
    "            ffn_expansion_factor = 2.6666,\n",
    "            bias = False,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.combination_weight = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.n_total_fts, \n",
    "                out_channels=self.n_graphs, \n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                padding_mode=\"zeros\",\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Softmax(dim=1)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.dc_estimator = DCestimator(12, 3, 12*2).to(self.device)\n",
    "\n",
    "        self.ro00 = Parameter(\n",
    "            torch.ones((n_graphs), device=self.device, dtype=torch.float32) * ro_init[0],\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.gamma00 = Parameter(\n",
    "            torch.ones((n_graphs), device=self.device, dtype=torch.float32) * torch.log(gamma_init[0]),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.GTVmodule00 = GTVFast(\n",
    "            n_channels=self.nchannels_in,\n",
    "            n_node_fts=self.n_node_fts,\n",
    "            n_graphs=self.n_graphs,\n",
    "            connection_window=self.connection_window,\n",
    "            device=self.device,\n",
    "            M_diag_init=1.0\n",
    "        )\n",
    "\n",
    "        self.muys00 = Parameter(\n",
    "            torch.ones((n_graphs), device=self.device, dtype=torch.float32) * muy_init[0],\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.GLRmodule00 = GLRFast(\n",
    "            n_channels=self.nchannels_in,\n",
    "            n_node_fts=self.n_node_fts,\n",
    "            n_graphs=self.n_graphs,\n",
    "            connection_window=self.connection_window,\n",
    "            device=self.device,\n",
    "            M_diag_init=1.0\n",
    "        )\n",
    "\n",
    "    def apply_lightweight_transformer(self, patchs, list_graph_weightGTV, list_graph_weightGLR):\n",
    "\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape \n",
    "        patchs = patchs.contiguous()\n",
    "        graph_weights, graph_degree = list_graph_weightGLR[0]\n",
    "\n",
    "        Lpatchs = self.GLRmodule00(patchs, graph_weights, graph_degree)\n",
    "        Lpatchs = torch.einsum(\n",
    "            \"bHchw, H -> bHchw\", Lpatchs, self.muys00\n",
    "        )\n",
    "\n",
    "        graph_weights, graph_degree = list_graph_weightGTV[0]\n",
    "        CtCpatchs = self.GTVmodule00(patchs, graph_weights, graph_degree)\n",
    "        CtCpatchs = torch.einsum(\n",
    "            \"bHchw, H -> bHchw\", CtCpatchs, self.ro00\n",
    "        )\n",
    "\n",
    "        output = patchs + Lpatchs + CtCpatchs\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def soft_threshold(self, delta, gamma):\n",
    "        # batch_size, n_graphs, n_channels, n_edges, H, W = delta.shape\n",
    "        # n_graphs = gamma.shape\n",
    "\n",
    "        gamma = gamma[None, :, None, None, None, None]\n",
    "        # print(f\"Gamma.shape={gamma.shape}\")\n",
    "\n",
    "        condA = (delta < -gamma) \n",
    "        outputA = torch.where(\n",
    "            condA,\n",
    "            delta+gamma,\n",
    "            0.0\n",
    "        )\n",
    "        condB = (delta > gamma) \n",
    "        outputB = torch.where(\n",
    "            condB,\n",
    "            delta-gamma,\n",
    "            0.0\n",
    "        )\n",
    "        output = outputA + outputB\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, patchs):\n",
    "        # with record_function(\"MultiScaleMixtureGLR:forward\"): \n",
    "        # print(\"#\"*80)\n",
    "        # patchs = patchs.permute(dims=(0, 3, 1, 2))\n",
    "        # patchs = patchs.contiguous()\n",
    "        # patchs = self.images_domain_to_abtract_domain(patchs)\n",
    "        # print(f\"patchs.shape={patchs.shape}\")\n",
    "        batch_size, c_size, h_size, w_size = patchs.shape\n",
    "\n",
    "        #####\n",
    "        ## Graph low pass filter\n",
    "        \n",
    "        list_features_patchs = self.patchs_features_extraction(patchs)\n",
    "        list_graph_weightGTV = [None] * self.n_levels\n",
    "        list_graph_weightGLR = [None] * self.n_levels\n",
    "        bz, nfts, h, w = list_features_patchs[0].shape\n",
    "\n",
    "        list_graph_weightGTV[0] = self.GTVmodule00.extract_edge_weights(\n",
    "            list_features_patchs[0][:, :-12, :, :].view((bz, self.GTVmodule00.n_graphs, self.GTVmodule00.n_node_fts, h, w))\n",
    "        )\n",
    "        list_graph_weightGLR[0] = self.GLRmodule00.extract_edge_weights(\n",
    "            list_features_patchs[0][:, :-12, :, :].view((bz, self.GLRmodule00.n_graphs, self.GLRmodule00.n_node_fts, h, w))\n",
    "        )\n",
    "\n",
    "\n",
    "        dc_term = self.dc_estimator(list_features_patchs[0][:, -12:, :, :])\n",
    "        y_tilde = patchs - dc_term\n",
    "        ###########################################################\n",
    "\n",
    "\n",
    "        epsilon = self.GTVmodule00.op_C(y_tilde[:, None, :, :, :], list_graph_weightGTV[0][0], list_graph_weightGTV[0][1])\n",
    "        bias    = torch.zeros_like(epsilon)\n",
    "\n",
    "        left_hand_size = self.GTVmodule00.op_C_transpose(epsilon - bias, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) \n",
    "        left_hand_size *= self.ro00[None, :, None, None, None]\n",
    "        left_hand_size += y_tilde[:, None, :, :, :]\n",
    "        ############################################################\n",
    "        output = left_hand_size\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual\n",
    "        output = output + self.alphaCGD[0, None, :, None, None, None] * update\n",
    "\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual + self.betaCGD[1, None, :, None, None, None] * update\n",
    "        output = output + self.alphaCGD[1, None, :, None, None, None] * update\n",
    "\n",
    "        epsilon = self.soft_threshold(\n",
    "            self.GTVmodule00.op_C(output, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) + bias,\n",
    "            torch.exp(self.gamma00)\n",
    "        )\n",
    "        bias  = bias + (self.GTVmodule00.op_C(output, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) - epsilon)\n",
    "\n",
    "        left_hand_size = self.GTVmodule00.op_C_transpose(epsilon - bias, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) \n",
    "        left_hand_size *= self.ro00[None, :, None, None, None]\n",
    "        left_hand_size += y_tilde[:, None, :, :, :]\n",
    "        ############################################################\n",
    "\n",
    "        output = left_hand_size\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual\n",
    "        output = output + self.alphaCGD[2, None, :, None, None, None] * update\n",
    "\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual + self.betaCGD[3, None, :, None, None, None] * update\n",
    "        output = output + self.alphaCGD[3, None, :, None, None, None] * update\n",
    "\n",
    "        # system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        # update = system_residual + self.betaCGD[4, None, :, None, None, None] * update\n",
    "        # output = output + self.alphaCGD[4, None, :, None, None, None] * update\n",
    "        \n",
    "        # system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        # update = system_residual + self.betaCGD[5, None, :, None, None, None] * update\n",
    "        # output = output + self.alphaCGD[5, None, :, None, None, None] * update\n",
    "        \n",
    "\n",
    "        score = self.combination_weight(list_features_patchs[0][:, :-12, :, :])\n",
    "        output = torch.einsum(\n",
    "            \"bgchw, bghw -> bchw\", output, score\n",
    "        ) + dc_term\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "class SharpeningBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, hidden_features):\n",
    "        super(SharpeningBlock, self).__init__()\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim_in, hidden_features*2, kernel_size=1, bias=False)\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=False)\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim_out, kernel_size=1, bias=False)\n",
    "        self.skip_connect_weight = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32) * torch.tensor([0.5, 0.5]),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, patchs):\n",
    "\n",
    "        out = self.project_in(patchs)\n",
    "        out01, out02 = self.dwconv(out).chunk(2, dim=1)\n",
    "        out = nn.functional.gelu(out01) * out02\n",
    "        out = self.project_out(out)\n",
    "        out = self.skip_connect_weight[0] * patchs + self.skip_connect_weight[1] * out\n",
    "        return out\n",
    "\n",
    "class MultiScaleSequenceDenoiser(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(MultiScaleSequenceDenoiser, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CONNECTION_FLAGS_5x5 = np.array([\n",
    "        #     1,1,1,1,1,\n",
    "        #     1,1,1,1,1,\n",
    "        #     1,1,0,1,1,\n",
    "        #     1,1,1,1,1,\n",
    "        #     1,1,1,1,1,\n",
    "        # ]).reshape((5,5))\n",
    "        CONNECTION_FLAGS_5x5_small = np.array([\n",
    "            0,0,1,0,0,\n",
    "            0,1,1,1,0,\n",
    "            1,1,0,1,1,\n",
    "            0,1,1,1,0,\n",
    "            0,0,1,0,0,\n",
    "        ]).reshape((5,5))\n",
    "        self.skip_connect_weight02 = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32, device=self.device) * torch.tensor([0.1, 0.9]).to(device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.mixtureGLR_block02 = MixtureGTV(\n",
    "            nchannels_in=3,\n",
    "            n_graphs=16,\n",
    "            n_node_fts=3,\n",
    "            n_cnn_fts=48,\n",
    "            connection_window=CONNECTION_FLAGS_5x5_small,\n",
    "            n_cgd_iters=4,\n",
    "            alpha_init=0.5,\n",
    "            beta_init=0.1,\n",
    "            muy_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            ro_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            gamma_init=torch.tensor([[0.001], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.skip_connect_weight03 = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32, device=self.device) * torch.tensor([0.5, 0.5]).to(device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.mixtureGLR_block03 = MixtureGTV(\n",
    "            nchannels_in=3,\n",
    "            n_graphs=16,\n",
    "            n_node_fts=3,\n",
    "            n_cnn_fts=48,\n",
    "            connection_window=CONNECTION_FLAGS_5x5_small,\n",
    "            n_cgd_iters=4,\n",
    "            alpha_init=0.5,\n",
    "            beta_init=0.1,\n",
    "            muy_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            ro_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            gamma_init=torch.tensor([[0.001], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def forward(self, patchs):\n",
    "        \n",
    "        output = self.skip_connect_weight02[0] * patchs + self.skip_connect_weight02[1] * self.mixtureGLR_block03(patchs)\n",
    "        output = self.skip_connect_weight03[0] * output + self.skip_connect_weight03[1] * self.mixtureGLR_block03(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30bccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "VERBOSE_RATE = 1000\n",
    "\n",
    "(H_train01, W_train01) = (64, 64)\n",
    "(H_train02, W_train02) = (128, 128)\n",
    "(H_train03, W_train03) = (256, 256)\n",
    "(H_train04, W_train04) = (512, 512)\n",
    "\n",
    "(H_val, W_val) = (128, 128)\n",
    "(H_test, W_test) = (496, 496)\n",
    "\n",
    "train_dataset01 = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"vary_addictive_noise\",\n",
    "    lambda_noise=[[1.0, 10.0, 15.0, 20.0, 25.0], [0.1, 0.1, 0.1, 0.1, 0.6]],\n",
    "    use_data_aug=True,\n",
    "    patch_size=(H_train01,H_train01),\n",
    "    patch_overlap_size=(H_train01//4,H_train01//4),\n",
    "    max_num_patchs=3200000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset01, batch_size=16, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eca785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 1951844\n",
      "iter=0 time=1.0051274299621582 psnr=26.239173782617534 mse=0.0023772925082516183\n",
      "iter=1 time=0.566617488861084 psnr=26.69866545149302 mse=0.002150600226300952\n",
      "iter=2 time=0.5650675296783447 psnr=26.765420934834214 mse=0.00211448020051078\n",
      "iter=3 time=0.5655529499053955 psnr=26.64339318672556 mse=0.0021749872688009418\n",
      "iter=4 time=0.5646185874938965 psnr=26.50952083318059 mse=0.0022453800586613865\n",
      "iter=5 time=0.5650131702423096 psnr=26.366323327174403 mse=0.0023248985149751113\n",
      "iter=6 time=0.5675699710845947 psnr=26.239467201627587 mse=0.0023974102518003726\n",
      "iter=7 time=0.5648601055145264 psnr=26.15526484740804 mse=0.0024447305357977183\n",
      "iter=8 time=0.564903974533081 psnr=26.153788018718394 mse=0.0024432159309070783\n",
      "iter=9 time=0.5648205280303955 psnr=26.15587111698976 mse=0.0024401836580156947\n",
      "iter=10 time=0.5658564567565918 psnr=26.149005140768228 mse=0.002442516346370907\n",
      "iter=11 time=0.5651144981384277 psnr=26.150208900725374 mse=0.0024405655191493477\n",
      "iter=12 time=0.5650787353515625 psnr=26.194924969104246 mse=0.002416099137039433\n",
      "iter=13 time=0.5648431777954102 psnr=26.23765350262763 mse=0.0023929925153506155\n",
      "iter=14 time=0.5644938945770264 psnr=26.275598042718492 mse=0.0023725274797809963\n",
      "iter=15 time=0.5650649070739746 psnr=26.332777069690533 mse=0.002343597630527534\n",
      "iter=16 time=0.5648653507232666 psnr=26.383717166195563 mse=0.0023178569116647616\n",
      "iter=17 time=0.56451416015625 psnr=26.4296804375767 mse=0.0022946768644008294\n",
      "iter=18 time=0.5647897720336914 psnr=26.460633388592544 mse=0.00227848899993426\n",
      "iter=19 time=0.5661828517913818 psnr=26.488310867092462 mse=0.002264002396013043\n",
      "iter=20 time=0.5649688243865967 psnr=26.47244036718677 mse=0.002271611854083432\n",
      "iter=21 time=0.5666189193725586 psnr=26.451263163423775 mse=0.0022823615681538474\n",
      "iter=22 time=0.564777135848999 psnr=26.424371167308276 mse=0.0022966288686804203\n",
      "iter=23 time=0.5648157596588135 psnr=26.400558298963006 mse=0.002309204600285722\n",
      "iter=24 time=0.5663361549377441 psnr=26.408178517518834 mse=0.0023045271320541656\n",
      "iter=25 time=0.5648372173309326 psnr=26.400526661488463 mse=0.0023079583955494362\n",
      "iter=26 time=0.5649294853210449 psnr=26.42180407741436 mse=0.0022968033914491375\n",
      "iter=27 time=0.5649831295013428 psnr=26.431457157883294 mse=0.0022912696789301637\n",
      "iter=28 time=0.5667097568511963 psnr=26.41659861975706 mse=0.002298865994027\n",
      "iter=29 time=0.5649685859680176 psnr=26.406761125532828 mse=0.002303657177790762\n",
      "iter=30 time=0.5648837089538574 psnr=26.401626642965397 mse=0.002305884004873447\n",
      "iter=31 time=0.5649392604827881 psnr=26.393386945197108 mse=0.002309867280197902\n",
      "iter=32 time=0.5645980834960938 psnr=26.367605921840898 mse=0.0023244429401083947\n",
      "iter=33 time=0.5662307739257812 psnr=26.346795760662438 mse=0.0023359711319812363\n",
      "iter=34 time=0.56485915184021 psnr=26.330943782277398 mse=0.0023445185807330123\n",
      "iter=35 time=0.565216064453125 psnr=26.315280323937422 mse=0.002353012345502986\n",
      "iter=36 time=0.564943790435791 psnr=26.30210648400864 mse=0.0023600513603629935\n",
      "iter=37 time=0.5646588802337646 psnr=26.30519281623035 mse=0.002357962216789225\n",
      "iter=38 time=0.5664892196655273 psnr=26.314768762238838 mse=0.0023525912855504824\n",
      "iter=39 time=0.5647237300872803 psnr=26.314565773703606 mse=0.00235229256476484\n",
      "iter=40 time=0.5652024745941162 psnr=26.318460457419626 mse=0.0023498472688773513\n",
      "iter=41 time=0.5654418468475342 psnr=26.331075919547256 mse=0.002343093383694084\n",
      "iter=42 time=0.5647544860839844 psnr=26.334485954076467 mse=0.002340934053188368\n",
      "iter=43 time=0.5649902820587158 psnr=26.346072563944333 mse=0.002334733147925731\n",
      "iter=44 time=0.5647528171539307 psnr=26.337323880261923 mse=0.0023392852900540185\n",
      "iter=45 time=0.5647175312042236 psnr=26.332595394781357 mse=0.0023415515980225953\n",
      "iter=46 time=0.5666649341583252 psnr=26.33372928356238 mse=0.0023406318541359028\n",
      "iter=47 time=0.5649323463439941 psnr=26.321303619817726 mse=0.0023474628309991794\n",
      "iter=48 time=0.5652451515197754 psnr=26.310327679270188 mse=0.002353438910047639\n",
      "iter=49 time=0.5649018287658691 psnr=26.30679604732768 mse=0.0023550843108292637\n",
      "iter=50 time=0.5650622844696045 psnr=26.29833527716927 mse=0.0023595936025204273\n",
      "iter=51 time=0.5651416778564453 psnr=26.297183926989113 mse=0.002359941427175937\n",
      "iter=52 time=0.56461501121521 psnr=26.3123639566636 mse=0.0023521891372090837\n",
      "iter=53 time=0.564612627029419 psnr=26.31795930099063 mse=0.002349009025731183\n",
      "iter=54 time=0.5650622844696045 psnr=26.3210828781128 mse=0.00234709991568962\n",
      "iter=55 time=0.5667431354522705 psnr=26.330657071219928 mse=0.0023420077918972215\n",
      "iter=56 time=0.5645911693572998 psnr=26.32790574384405 mse=0.002343259073066015\n",
      "iter=57 time=0.5647263526916504 psnr=26.34243751131058 mse=0.002335932851062733\n",
      "iter=58 time=0.5651726722717285 psnr=26.349319154714557 mse=0.002332175367197504\n",
      "iter=59 time=0.5647456645965576 psnr=26.356867296790092 mse=0.002328109683316981\n",
      "iter=60 time=0.564631462097168 psnr=26.36946922828469 mse=0.0023217208600166896\n",
      "iter=61 time=0.5648548603057861 psnr=26.384276813602845 mse=0.002314393707146719\n",
      "iter=62 time=0.5652716159820557 psnr=26.402450150507196 mse=0.00230569482880011\n",
      "iter=63 time=0.5647742748260498 psnr=26.41774621776282 mse=0.0022982232166536554\n",
      "iter=64 time=0.5648279190063477 psnr=26.421678514294978 mse=0.002295960152167777\n",
      "iter=65 time=0.5646412372589111 psnr=26.42895671539479 mse=0.002292093774253956\n",
      "iter=66 time=0.5649566650390625 psnr=26.44083926420758 mse=0.0022861593201721642\n",
      "iter=67 time=0.5661337375640869 psnr=26.44614369548239 mse=0.0022832533760163837\n",
      "iter=68 time=0.5645060539245605 psnr=26.45950483680288 mse=0.0022767298788296496\n",
      "iter=69 time=0.5646171569824219 psnr=26.470920020171928 mse=0.0022710614052487217\n",
      "iter=70 time=0.5648419857025146 psnr=26.48625330522631 mse=0.002263779537696652\n",
      "iter=71 time=0.5652012825012207 psnr=26.50161774840374 mse=0.0022565160332475237\n",
      "iter=72 time=0.5644478797912598 psnr=26.500416938494606 mse=0.0022568859014638534\n",
      "iter=73 time=0.5648720264434814 psnr=26.49880288548767 mse=0.0022574810126457165\n",
      "iter=74 time=0.5651416778564453 psnr=26.500340093207047 mse=0.0022564569129848687\n",
      "iter=75 time=0.5648539066314697 psnr=26.498841641355888 mse=0.0022570038249960195\n",
      "iter=76 time=0.5645453929901123 psnr=26.50792374520262 mse=0.0022524487959659136\n",
      "iter=77 time=0.5649371147155762 psnr=26.509859100496858 mse=0.0022512417628887857\n",
      "iter=78 time=0.5647649765014648 psnr=26.515410011752863 mse=0.00224830349862508\n",
      "iter=79 time=0.56459641456604 psnr=26.525955170269164 mse=0.0022431616007925976\n",
      "iter=80 time=0.564802885055542 psnr=26.53353174011008 mse=0.0022393215904331736\n",
      "iter=81 time=0.5652227401733398 psnr=26.53931652555624 mse=0.0022363011249138124\n",
      "iter=82 time=0.5663111209869385 psnr=26.54205327116693 mse=0.0022347250975322873\n",
      "iter=83 time=0.5646042823791504 psnr=26.55014600546711 mse=0.0022306914798763656\n",
      "iter=84 time=0.5647993087768555 psnr=26.559999699854068 mse=0.0022259169644612445\n",
      "iter=85 time=0.5650784969329834 psnr=26.567280675870474 mse=0.002222261430952043\n",
      "iter=86 time=0.5649189949035645 psnr=26.57039834910902 mse=0.0022205210440759786\n",
      "iter=87 time=0.5647773742675781 psnr=26.569560930373072 mse=0.0022207471691055927\n",
      "iter=88 time=0.56475830078125 psnr=26.57272850645417 mse=0.002218993590928329\n",
      "iter=89 time=0.5647456645965576 psnr=26.582351311071502 mse=0.0022143772613017094\n",
      "iter=90 time=0.5647983551025391 psnr=26.590347108209635 mse=0.0022104590252441654\n",
      "iter=91 time=0.5649032592773438 psnr=26.600146462530294 mse=0.002205797559823042\n",
      "iter=92 time=0.5648355484008789 psnr=26.594175016815214 mse=0.002208811638748504\n",
      "iter=93 time=0.5648190975189209 psnr=26.59680232799402 mse=0.0022073306312974826\n",
      "iter=94 time=0.5646255016326904 psnr=26.597786511216874 mse=0.0022066507584601706\n",
      "iter=95 time=0.5647778511047363 psnr=26.601032764891425 mse=0.0022048868018383146\n",
      "iter=96 time=0.5650241374969482 psnr=26.599706811447334 mse=0.0022053826687311604\n",
      "iter=97 time=0.5649876594543457 psnr=26.607566687979528 mse=0.0022015759587617483\n",
      "iter=98 time=0.5655605792999268 psnr=26.61353273967748 mse=0.002198592848880973\n",
      "iter=99 time=0.5656628608703613 psnr=26.620557613385532 mse=0.0021951592302389506\n",
      "iter=100 time=0.566882848739624 psnr=26.627869204468357 mse=0.0021914756933525646\n",
      "iter=101 time=0.5657408237457275 psnr=26.633698403858567 mse=0.0021890591819703586\n",
      "iter=102 time=0.5651092529296875 psnr=26.64051382977102 mse=0.0021860930946990694\n",
      "iter=103 time=0.5650227069854736 psnr=26.659333354205287 mse=0.002177806265212904\n",
      "iter=104 time=0.564887285232544 psnr=26.666614403566406 mse=0.0021739058595795563\n",
      "iter=105 time=0.5652186870574951 psnr=26.6783897154744 mse=0.0021674402801085705\n",
      "iter=106 time=0.5650362968444824 psnr=26.686774015781467 mse=0.0021624674663971253\n",
      "iter=107 time=0.5645074844360352 psnr=26.69694313274176 mse=0.0021566723763055025\n",
      "iter=108 time=0.5646517276763916 psnr=26.697811154209376 mse=0.002156191297817501\n",
      "iter=109 time=0.5654544830322266 psnr=26.700738638096137 mse=0.0021546184268275382\n",
      "iter=110 time=0.5652065277099609 psnr=26.708276283553026 mse=0.002150689503150132\n",
      "iter=111 time=0.5646109580993652 psnr=26.715972783219897 mse=0.002146760796626623\n",
      "iter=112 time=0.5645811557769775 psnr=26.718362661939317 mse=0.002145624360126966\n",
      "iter=113 time=0.5649118423461914 psnr=26.715205033467317 mse=0.0021472025093707122\n",
      "iter=114 time=0.5648963451385498 psnr=26.712679533738484 mse=0.002148451529806343\n",
      "iter=115 time=0.5645184516906738 psnr=26.70483083102621 mse=0.002152234227989608\n",
      "iter=116 time=0.5644538402557373 psnr=26.694981170976142 mse=0.0021570864396886\n",
      "iter=117 time=0.5647153854370117 psnr=26.687825999128762 mse=0.002160490498737096\n",
      "iter=118 time=0.5646884441375732 psnr=26.681670420178474 mse=0.0021635163481410653\n",
      "iter=119 time=0.5643501281738281 psnr=26.676330039968487 mse=0.0021661185744172888\n",
      "iter=120 time=0.5644800662994385 psnr=26.691849047986537 mse=0.002158835919095769\n",
      "iter=121 time=0.5649430751800537 psnr=26.704033171526216 mse=0.002152700298230099\n",
      "iter=122 time=0.5665557384490967 psnr=26.725004699557438 mse=0.0021427020304410835\n",
      "iter=123 time=0.5645887851715088 psnr=26.739298489046483 mse=0.002135414735980637\n",
      "iter=124 time=0.5647711753845215 psnr=26.746518476749447 mse=0.002132057005444749\n",
      "iter=125 time=0.565385103225708 psnr=26.75884637320279 mse=0.002126141399128267\n",
      "iter=126 time=0.5657248497009277 psnr=26.755109995997824 mse=0.0021279443360384545\n",
      "iter=127 time=0.5650687217712402 psnr=26.76277950001024 mse=0.0021244770425371336\n",
      "iter=128 time=0.5650815963745117 psnr=26.781622997646263 mse=0.002115635967348358\n",
      "iter=129 time=0.5649316310882568 psnr=26.79391966861582 mse=0.0021096128372275706\n",
      "iter=130 time=0.5649476051330566 psnr=26.80633225697728 mse=0.0021037144529303053\n",
      "iter=131 time=0.5646066665649414 psnr=26.81960138545419 mse=0.00209730819037866\n",
      "iter=132 time=0.5647034645080566 psnr=26.835196358400253 mse=0.0020888885227684753\n",
      "iter=133 time=0.5653626918792725 psnr=26.846064475794552 mse=0.0020828746255053786\n",
      "iter=134 time=0.5647983551025391 psnr=26.856460466462032 mse=0.002077264916050192\n",
      "iter=135 time=0.5647270679473877 psnr=26.874531424585562 mse=0.002068243708232677\n",
      "iter=136 time=0.5645809173583984 psnr=26.896689043438286 mse=0.0020577997288928527\n",
      "iter=137 time=0.5648729801177979 psnr=26.91153239483935 mse=0.002051197296575677\n",
      "iter=138 time=0.5645601749420166 psnr=26.919402523990488 mse=0.0020476362675198654\n",
      "iter=139 time=0.5646755695343018 psnr=26.94223131502009 mse=0.002038067009506255\n",
      "iter=140 time=0.5649361610412598 psnr=26.94660392651067 mse=0.0020359099994812324\n",
      "iter=141 time=0.5651326179504395 psnr=26.955838094609685 mse=0.0020319524220776554\n",
      "iter=142 time=0.5648424625396729 psnr=26.968136787964124 mse=0.002026402835003952\n",
      "iter=143 time=0.5645108222961426 psnr=26.974424674920883 mse=0.0020236152343267393\n",
      "iter=144 time=0.5647726058959961 psnr=26.984614006613086 mse=0.0020183042847563212\n",
      "iter=145 time=0.5654528141021729 psnr=26.988946243511855 mse=0.0020159844054213672\n",
      "iter=146 time=0.5652568340301514 psnr=27.00325697694708 mse=0.002009532348845819\n",
      "iter=147 time=0.5653729438781738 psnr=27.014088291778247 mse=0.002003642074485076\n",
      "iter=148 time=0.5671975612640381 psnr=27.034653189128477 mse=0.0019936829896756167\n",
      "iter=149 time=0.5652987957000732 psnr=27.05508024728591 mse=0.001984543803137438\n",
      "iter=150 time=0.5652370452880859 psnr=27.074878173610283 mse=0.0019750799045479763\n",
      "iter=151 time=0.5649166107177734 psnr=27.09118210523538 mse=0.001967637913074542\n",
      "iter=152 time=0.5651557445526123 psnr=27.11032065811669 mse=0.0019606913822028844\n",
      "iter=153 time=0.5650730133056641 psnr=27.129191414092602 mse=0.0019530069619875662\n",
      "iter=154 time=0.5651545524597168 psnr=27.144690639807685 mse=0.0019462716724363636\n",
      "iter=155 time=0.5649771690368652 psnr=27.170874508581495 mse=0.0019369355991776336\n",
      "iter=156 time=0.5647485256195068 psnr=27.190002116632687 mse=0.0019283383470074511\n",
      "iter=157 time=0.5646202564239502 psnr=27.199470995078354 mse=0.0019245803634474813\n",
      "iter=158 time=0.5651447772979736 psnr=27.208562999625798 mse=0.001920586780898929\n",
      "iter=159 time=0.5646071434020996 psnr=27.22380332400487 mse=0.0019144064229942857\n",
      "iter=160 time=0.5647518634796143 psnr=27.235121324790413 mse=0.0019099594430041424\n",
      "iter=161 time=0.56494140625 psnr=27.242359316044155 mse=0.0019070926763825832\n",
      "iter=162 time=0.5651483535766602 psnr=27.254615052686795 mse=0.0019027496063686628\n",
      "iter=163 time=0.5649571418762207 psnr=27.269135244125604 mse=0.0018975560107760195\n",
      "iter=164 time=0.5648846626281738 psnr=27.29067681137836 mse=0.0018891441214928797\n",
      "iter=165 time=0.5648002624511719 psnr=27.302963580428685 mse=0.0018841153376761338\n",
      "iter=166 time=0.5651750564575195 psnr=27.320902699910445 mse=0.0018777047833476938\n",
      "iter=167 time=0.5648500919342041 psnr=27.344461773941426 mse=0.0018689602529007676\n",
      "iter=168 time=0.5650525093078613 psnr=27.35910039752296 mse=0.0018637149503857907\n",
      "iter=169 time=0.5649621486663818 psnr=27.37057328768873 mse=0.0018593505014344708\n",
      "iter=170 time=0.5645618438720703 psnr=27.378282269648167 mse=0.0018564976477721251\n",
      "iter=171 time=0.5649278163909912 psnr=27.384645597449403 mse=0.001854125059174852\n",
      "iter=172 time=0.565392017364502 psnr=27.400521092263116 mse=0.0018471333243142416\n",
      "iter=173 time=0.5652804374694824 psnr=27.424128138011337 mse=0.0018374848638865512\n",
      "iter=174 time=0.5647108554840088 psnr=27.438549034952302 mse=0.0018313233488536438\n",
      "iter=175 time=0.5649030208587646 psnr=27.462882185744128 mse=0.0018214658119815568\n",
      "iter=176 time=0.5644943714141846 psnr=27.481561733028602 mse=0.0018148021890065063\n",
      "iter=177 time=0.5649456977844238 psnr=27.504777705788438 mse=0.001805865156246688\n",
      "iter=178 time=0.5651490688323975 psnr=27.527135269221603 mse=0.0017977406046257582\n",
      "iter=179 time=0.5664498805999756 psnr=27.54110264826716 mse=0.001792688625769736\n",
      "iter=180 time=0.5651540756225586 psnr=27.56112760214113 mse=0.0017855512733408988\n",
      "iter=181 time=0.5649614334106445 psnr=27.579105092871863 mse=0.001778800318566991\n",
      "iter=182 time=0.5652506351470947 psnr=27.59472065626575 mse=0.0017724413620489815\n",
      "iter=183 time=0.5648360252380371 psnr=27.61110606199153 mse=0.0017664828975573934\n",
      "iter=184 time=0.5648653507232666 psnr=27.61983266572405 mse=0.0017631609918352376\n",
      "iter=185 time=0.5649724006652832 psnr=27.637847549198394 mse=0.0017566706802758694\n",
      "iter=186 time=0.5660970211029053 psnr=27.660382885471726 mse=0.0017482874263091911\n",
      "iter=187 time=0.5652468204498291 psnr=27.683995766734288 mse=0.0017388909295929361\n",
      "iter=188 time=0.5649158954620361 psnr=27.695502039090066 mse=0.0017340854037290012\n",
      "iter=189 time=0.5650088787078857 psnr=27.69982547977219 mse=0.0017323764542760127\n",
      "iter=190 time=0.5652940273284912 psnr=27.70187900133645 mse=0.0017315184470988252\n",
      "iter=191 time=0.5650615692138672 psnr=27.71335318779794 mse=0.0017273818779786562\n",
      "iter=192 time=0.5650651454925537 psnr=27.73820380188316 mse=0.0017165493872005305\n",
      "iter=193 time=0.5649292469024658 psnr=27.755349512989017 mse=0.0017097987435264928\n",
      "iter=194 time=0.5653235912322998 psnr=27.77580974601946 mse=0.0017017485926785556\n",
      "iter=195 time=0.565488338470459 psnr=27.78203903185382 mse=0.0016990262859401094\n",
      "iter=196 time=0.5648901462554932 psnr=27.80368591976719 mse=0.001690182838513361\n",
      "iter=197 time=0.5650203227996826 psnr=27.81890666413857 mse=0.001684765689358587\n",
      "iter=198 time=0.5654935836791992 psnr=27.82968440267611 mse=0.0016805763269114428\n",
      "iter=199 time=0.5651881694793701 psnr=27.835663992175498 mse=0.0016781899991785596\n",
      "iter=200 time=0.5651562213897705 psnr=27.841268196302185 mse=0.0016757579261818705\n",
      "iter=201 time=0.5650317668914795 psnr=27.848525085899517 mse=0.0016731692413305263\n",
      "iter=202 time=0.56508469581604 psnr=27.8521225821301 mse=0.0016717815104088354\n",
      "iter=203 time=0.5652275085449219 psnr=27.846255543248816 mse=0.0016739914164559545\n",
      "iter=204 time=0.5649459362030029 psnr=27.874126965592442 mse=0.001663870227894685\n",
      "iter=205 time=0.5649697780609131 psnr=27.888327065991636 mse=0.0016580805718338137\n",
      "iter=206 time=0.5650737285614014 psnr=27.91253188786547 mse=0.0016481030227161295\n",
      "iter=207 time=0.5650546550750732 psnr=27.925242443597217 mse=0.001642529877690733\n",
      "iter=208 time=0.5651247501373291 psnr=27.942049152995413 mse=0.0016348828280710224\n",
      "iter=209 time=0.5654268264770508 psnr=27.962303603171875 mse=0.001626475539399696\n",
      "iter=210 time=0.5649375915527344 psnr=27.983182318584696 mse=0.0016185634873146207\n",
      "iter=211 time=0.5647847652435303 psnr=28.003194086287223 mse=0.0016110823493456913\n",
      "iter=212 time=0.5653140544891357 psnr=28.02743080818284 mse=0.0016024906990046437\n",
      "iter=213 time=0.5649557113647461 psnr=28.05075698367218 mse=0.0015931387969212898\n",
      "iter=214 time=0.5650227069854736 psnr=28.060596605861942 mse=0.0015886565340353393\n",
      "iter=215 time=0.5649757385253906 psnr=28.095912746031345 mse=0.0015759229752975363\n",
      "iter=216 time=0.5654089450836182 psnr=28.121402010987495 mse=0.001565306926057246\n",
      "iter=217 time=0.5651917457580566 psnr=28.143611906287077 mse=0.001556335093912739\n",
      "iter=218 time=0.565039873123169 psnr=28.162678544860793 mse=0.0015481989992490565\n",
      "iter=219 time=0.565345048904419 psnr=28.192316312228826 mse=0.0015370752084908365\n",
      "iter=220 time=0.5655174255371094 psnr=28.21329017076055 mse=0.0015305807355217233\n",
      "iter=221 time=0.5651133060455322 psnr=28.2362094926224 mse=0.00152281194319668\n",
      "iter=222 time=0.5648763179779053 psnr=28.245675532606157 mse=0.0015196574966962152\n",
      "iter=223 time=0.5651001930236816 psnr=28.269839144626673 mse=0.0015116789685481024\n",
      "iter=224 time=0.5648660659790039 psnr=28.27082852601119 mse=0.0015112608151267146\n",
      "iter=225 time=0.5650084018707275 psnr=28.286052959934473 mse=0.0015059317115254612\n",
      "iter=226 time=0.5649027824401855 psnr=28.297745529979625 mse=0.0015007695001951116\n",
      "iter=227 time=0.5652406215667725 psnr=28.304880095904412 mse=0.001498049965581433\n",
      "iter=228 time=0.5649878978729248 psnr=28.320284502976342 mse=0.0014931901256342573\n",
      "iter=229 time=0.5650856494903564 psnr=28.334121130467008 mse=0.0014881691978723631\n",
      "iter=230 time=0.5668368339538574 psnr=28.349194629194812 mse=0.0014829409657712565\n",
      "iter=231 time=0.5651776790618896 psnr=28.357948663634456 mse=0.0014796682986864546\n",
      "iter=232 time=0.5649733543395996 psnr=28.376356168076924 mse=0.0014729353080453287\n",
      "iter=233 time=0.5649487972259521 psnr=28.3951626547393 mse=0.0014655018361896136\n",
      "iter=234 time=0.5650444030761719 psnr=28.410653045020336 mse=0.0014592792885598468\n",
      "iter=235 time=0.5653741359710693 psnr=28.43182088287382 mse=0.001452535127822666\n",
      "iter=236 time=0.5650360584259033 psnr=28.437148620760652 mse=0.0014507236578939562\n",
      "iter=237 time=0.5650162696838379 psnr=28.454955888873577 mse=0.0014452730812740582\n",
      "iter=238 time=0.565082311630249 psnr=28.466821444614013 mse=0.001440987999052187\n",
      "iter=239 time=0.5649642944335938 psnr=28.473786185584657 mse=0.0014389377368376846\n",
      "iter=240 time=0.5647187232971191 psnr=28.4818516684199 mse=0.0014354863655103877\n",
      "iter=241 time=0.565413236618042 psnr=28.493093095025596 mse=0.001431676856150203\n",
      "iter=242 time=0.5655257701873779 psnr=28.513801262236726 mse=0.0014252475323725791\n",
      "iter=243 time=0.5649392604827881 psnr=28.53274355067356 mse=0.0014189224648520308\n",
      "iter=244 time=0.5647311210632324 psnr=28.560471133031434 mse=0.0014094446216921705\n",
      "iter=245 time=0.5649895668029785 psnr=28.58692466456341 mse=0.001399356247539032\n",
      "iter=246 time=0.5653934478759766 psnr=28.598439516291947 mse=0.0013955061211682203\n",
      "iter=247 time=0.5655422210693359 psnr=28.628845862476215 mse=0.0013850362933748223\n",
      "iter=248 time=0.5650177001953125 psnr=28.64650698767408 mse=0.0013795419013861749\n",
      "iter=249 time=0.5649986267089844 psnr=28.662608172142665 mse=0.0013748277143063614\n",
      "iter=250 time=0.5649495124816895 psnr=28.694184830469116 mse=0.001366361018167921\n",
      "iter=251 time=0.5653884410858154 psnr=28.701378446795175 mse=0.0013638675408713418\n",
      "iter=252 time=0.5652177333831787 psnr=28.703682250288942 mse=0.0013632194514320523\n",
      "iter=253 time=0.565223217010498 psnr=28.70829485630268 mse=0.0013617966499431327\n",
      "iter=254 time=0.564950704574585 psnr=28.719307815643116 mse=0.0013582790206673686\n",
      "iter=255 time=0.56504225730896 psnr=28.704186510964757 mse=0.0013629784003642814\n",
      "iter=256 time=0.5649087429046631 psnr=28.72504756615646 mse=0.0013570524956788717\n",
      "iter=257 time=0.565190315246582 psnr=28.75539695992456 mse=0.0013492961628612293\n",
      "iter=258 time=0.5652194023132324 psnr=28.771854417705022 mse=0.001343887111597572\n",
      "iter=259 time=0.565169095993042 psnr=28.7979571769647 mse=0.0013372453739948797\n",
      "iter=260 time=0.565011739730835 psnr=28.818750508840257 mse=0.0013315624109491839\n",
      "iter=261 time=0.5651865005493164 psnr=28.830160996930637 mse=0.001327909979839439\n",
      "iter=262 time=0.5652132034301758 psnr=28.839770217305414 mse=0.0013252659509210664\n",
      "iter=263 time=0.5648648738861084 psnr=28.856426132936623 mse=0.0013210989729920586\n",
      "iter=264 time=0.5649266242980957 psnr=28.87176432045107 mse=0.0013172013184881161\n",
      "iter=265 time=0.565086841583252 psnr=28.898715129134136 mse=0.001310090686706318\n",
      "iter=266 time=0.5650463104248047 psnr=28.913103234822508 mse=0.0013065559003419063\n",
      "iter=267 time=0.5647664070129395 psnr=28.907866302145603 mse=0.0013081118503330028\n",
      "iter=268 time=0.5652503967285156 psnr=28.924770303682283 mse=0.0013038926442241253\n",
      "iter=269 time=0.5649428367614746 psnr=28.955439327293693 mse=0.0012965817289149076\n",
      "iter=270 time=0.5650546550750732 psnr=28.964571193046144 mse=0.001293796457642149\n",
      "iter=271 time=0.5651319026947021 psnr=28.990735848669807 mse=0.0012869923198185703\n",
      "iter=272 time=0.5656230449676514 psnr=29.012761527836773 mse=0.00128068985763583\n",
      "iter=273 time=0.5650856494903564 psnr=29.031976124337522 mse=0.0012759130010129916\n",
      "iter=274 time=0.5652716159820557 psnr=29.04438877395168 mse=0.0012720236618482845\n",
      "iter=275 time=0.5653131008148193 psnr=29.04608629915931 mse=0.0012715206309552504\n",
      "iter=276 time=0.5651354789733887 psnr=29.052330116604672 mse=0.0012698602454787801\n",
      "iter=277 time=0.5653824806213379 psnr=29.067059398138355 mse=0.0012662229177199348\n",
      "iter=278 time=0.5651423931121826 psnr=29.0719525997188 mse=0.0012649371611971514\n",
      "iter=279 time=0.5669631958007812 psnr=29.089566240825903 mse=0.0012604970589064502\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "\n",
    "model = MultiScaleSequenceDenoiser(device=DEVICE)\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "criterian = nn.L1Loss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,\n",
    "    eps=1e-08\n",
    ")\n",
    "lr_scheduler = MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[200000, 500000, 650000], gamma=0.5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "i = 0\n",
    "### TRAINING\n",
    "list_train_mse = []\n",
    "list_train_psnr = []\n",
    "for patchs_noisy, patchs_true in data_train_batched01:\n",
    "    s = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    patchs_noisy = patchs_noisy.to(DEVICE)\n",
    "    patchs_true = patchs_true.to(DEVICE) \n",
    "    reconstruct_patchs = model(patchs_noisy.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "    loss_value = criterian(reconstruct_patchs, patchs_true)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    img_true = np.clip(patchs_true.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    img_recon = np.clip(reconstruct_patchs.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    train_mse_value = np.square(img_true- img_recon).mean()\n",
    "    train_psnr = 10 * np.log10(1/train_mse_value)\n",
    "    list_train_psnr.append(train_psnr)\n",
    "    list_train_mse.append(train_mse_value)\n",
    "\n",
    "    print(f\"iter={i} time={time.time()-s} psnr={np.mean(list_train_psnr[-100:])} mse={np.mean(list_train_mse[-100:])}\")\n",
    "    i+=1\n",
    "\n",
    "    if (i%1000 == 0):\n",
    "\n",
    "        model.eval()\n",
    "        csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "        img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "        paths = img_infos[\"path\"].tolist()\n",
    "        paths = [\n",
    "            os.path.join(ROOT_DATASET,path)\n",
    "            for path in paths\n",
    "        ]\n",
    "\n",
    "        sigma_test = 25.0\n",
    "        factor = 8\n",
    "        list_test_mse = []\n",
    "        random_state = np.random.RandomState(seed=2204)\n",
    "        test_i = 0\n",
    "        s = time.time()\n",
    "        for file_ in paths:\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            img = Image.open(file_)\n",
    "            img_true_255 = np.array(img).astype(np.float32)\n",
    "            img_true = img_true_255 / 255.0\n",
    "\n",
    "            noisy_img_raw = img_true.copy()\n",
    "            noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "            noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "            noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "            h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "            H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "            padh = H-h if h%factor!=0 else 0\n",
    "            padw = W-w if w%factor!=0 else 0\n",
    "            noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "            restored = restored[:,:,:h,:w]\n",
    "            restored = torch.clamp(restored,0,1).cpu().detach().permute(0, 2, 3, 1).squeeze(0).numpy().copy()\n",
    "\n",
    "            restored = img_as_ubyte(restored).astype(np.float32)\n",
    "            test_mse_value = np.square(img_true_255- restored).mean()\n",
    "            list_test_mse.append(test_mse_value)\n",
    "            # print(f\"test_i={test_i} time={time.time()-s} test_i_psnr_value={20 * np.log10(255.0 / np.sqrt(test_mse_value))}\")  \n",
    "            test_i += 1\n",
    "            s = time.time()\n",
    "\n",
    "        psnr_testing = 20 * np.log10(255.0 / np.sqrt(list_test_mse))\n",
    "        print(f\"FINISH TESTING - iter={i} -  psnr_testing={np.mean(psnr_testing)}\")\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ad218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aaafe",
   "metadata": {},
   "source": [
    "# 02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278d112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "# ROOT_PROJECT = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling/\"\n",
    "# ROOT_DATASET = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling\"\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader import ImageSuperResolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d23ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, nchannels):\n",
    "        super(CustomLayerNorm, self).__init__()\n",
    "        \n",
    "        self.nchannels = nchannels\n",
    "        self.weighted_transform = nn.Conv2d(nchannels, nchannels, kernel_size=1, stride=1, groups=nchannels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bz, nchannels, h, w = x.shape\n",
    "        sigma = x.var(dim=1, keepdim=True, correction=1)\n",
    "        # bz, 1, h, w = sigma.shape\n",
    "        return self.weighted_transform(x / torch.sqrt(sigma+1e-5))\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class LocalGatedLinearBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super(LocalGatedLinearBlock, self).__init__()\n",
    "\n",
    "        self.channels_linear_op       = nn.Conv2d(dim, hidden_dim*2, kernel_size=1, bias=False)\n",
    "        self.channels_local_linear_op = nn.Conv2d(\n",
    "            hidden_dim*2, hidden_dim*2, \n",
    "            kernel_size=3, stride=1, \n",
    "            padding=1, padding_mode=\"replicate\",\n",
    "            groups=hidden_dim*2, \n",
    "            bias=False\n",
    "        )\n",
    "        self.project_out = nn.Conv2d(hidden_dim, dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channels_linear_op(x)\n",
    "        mask, x = self.channels_local_linear_op(x).chunk(2, dim=1)\n",
    "        x = nn.functional.sigmoid(mask) * x\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class LocalLowpassFilteringBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LocalLowpassFilteringBlock, self).__init__()\n",
    "\n",
    "        # Filter Layer\n",
    "        # self.norm_filter = CustomLayerNorm(dim)\n",
    "        # self.local_linear_filter = nn.Conv2d(\n",
    "        #     dim, dim, \n",
    "        #     kernel_size=3, stride=1, \n",
    "        #     padding=1, padding_mode=\"replicate\",\n",
    "        #     groups=dim, \n",
    "        #     bias=False\n",
    "        # )\n",
    "        # self.skip_weight_filter = Parameter(\n",
    "        #     torch.tensor([0.5, 0.5], dtype=torch.float32),\n",
    "        #     requires_grad=True\n",
    "        # )\n",
    "\n",
    "        # Linear Layer\n",
    "        self.norm = CustomLayerNorm(dim)\n",
    "        self.local_linear = LocalGatedLinearBlock(dim, dim*2)\n",
    "        self.skip_weight= Parameter(\n",
    "            torch.tensor([0.5, 0.5], dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x = self.skip_weight_filter[0] * x + self.skip_weight_filter[1] * self.local_linear_filter(self.norm_filter(x))\n",
    "        x = self.skip_weight[0] * x + self.skip_weight[1] * self.local_linear(self.norm(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class ReginalPixelEmbeding(nn.Module):\n",
    "    def __init__(self, n_channels_in=3, dim=48, bias=False):\n",
    "        super(ReginalPixelEmbeding, self).__init__()\n",
    "\n",
    "        self.channels_local_linear_op01 = nn.Conv2d(\n",
    "            n_channels_in, dim*2, \n",
    "            kernel_size=3, stride=1, \n",
    "            padding=1, padding_mode=\"replicate\",\n",
    "            bias=False\n",
    "        )\n",
    "        # self.project_out = nn.Conv2d(dim//2, dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mask, x = self.channels_local_linear_op01(x).chunk(2, dim=1)\n",
    "        x = nn.functional.sigmoid(mask) * mask * x\n",
    "        # x = self.channels_local_linear_op01(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ##########################################################################\n",
    "# ## Down/Up Sampling\n",
    "# class Downsampling(nn.Module):\n",
    "#     def __init__(self, dim_in, dim_out):\n",
    "#         super(Downsampling, self).__init__()\n",
    "\n",
    "#         self.local_linear = nn.Conv2d(dim_in, dim_out//4, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "#         self.local_concat = nn.PixelUnshuffle(2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.local_concat(self.local_linear(x))\n",
    "#         return x\n",
    "\n",
    "# class Upsampling(nn.Module):\n",
    "#     def __init__(self, dim_in, dim_out):\n",
    "#         super(Upsampling, self).__init__()\n",
    "\n",
    "#         self.local_linear = nn.Conv2d(dim_in, dim_out*4, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "#         self.local_concat = nn.PixelShuffle(2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.local_concat(self.local_linear(x))\n",
    "#         return x\n",
    "\n",
    "##########################################################################\n",
    "## Down/Up Sampling\n",
    "class Downsampling(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Downsampling, self).__init__()\n",
    "\n",
    "        self.local_linear = nn.Conv2d(dim_in, dim_out//2, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "        self.local_concat = nn.PixelUnshuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask, x = self.local_linear(x).chunk(2, dim=1)\n",
    "        x = nn.functional.sigmoid(mask) * mask * x\n",
    "        x = self.local_concat(x)\n",
    "        return x\n",
    "\n",
    "class Upsampling(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Upsampling, self).__init__()\n",
    "\n",
    "        self.local_linear = nn.Conv2d(dim_in, dim_out*8, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "        self.local_concat = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Upsampling in:{x.shape}\")\n",
    "        mask, x = self.local_linear(x).chunk(2, dim=1)\n",
    "        # print(f\"Upsampling out//4:{x.shape}\")\n",
    "        x = nn.functional.sigmoid(mask) * mask * x\n",
    "        x = self.local_concat(x)\n",
    "        # print(f\"Upsampling out:{x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class AbtractMultiScaleGraphFilter(nn.Module):\n",
    "    def __init__(self, \n",
    "        n_channels_in=3, \n",
    "        n_channels_out=3, \n",
    "        dims=[48, 64, 96, 128],\n",
    "        num_blocks=[4, 6, 6, 8], \n",
    "        num_blocks_out=4\n",
    "    ):\n",
    "\n",
    "        super(AbtractMultiScaleGraphFilter, self).__init__()\n",
    "\n",
    "        self.patch_3x3_embeding = ReginalPixelEmbeding(n_channels_in, dims[0])\n",
    "        self.encoder_scale_00 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[0]) for i in range(num_blocks[0])\n",
    "        ])\n",
    "        \n",
    "        self.down_sample_00_01 = Downsampling(dim_in=dims[0], dim_out=dims[1]) \n",
    "        self.encoder_scale_01 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[1]) for i in range(num_blocks[1])\n",
    "        ])\n",
    "\n",
    "        self.down_sample_01_02 = Downsampling(dim_in=dims[1], dim_out=dims[2]) \n",
    "        self.encoder_scale_02 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[2]) for i in range(num_blocks[2])\n",
    "        ])\n",
    "\n",
    "        self.down_sample_02_03 = Downsampling(dim_in=dims[2], dim_out=dims[3]) \n",
    "        self.encoder_scale_03 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[3]) for i in range(num_blocks[3])\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.up_sample_03_02 = Upsampling(dim_in=dims[3], dim_out=dims[2])\n",
    "        self.combine_channels_02 = nn.Conv2d(dims[2]*2, dims[2], kernel_size=1, bias=False)\n",
    "        self.decoder_scale_02 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[2]) for i in range(num_blocks[2])\n",
    "        ])\n",
    "\n",
    "        self.up_sample_02_01 = Upsampling(dim_in=dims[2], dim_out=dims[1])\n",
    "        self.combine_channels_01 = nn.Conv2d(dims[1]*2, dims[1], kernel_size=1, bias=False)\n",
    "        self.decoder_scale_01 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[1]) for i in range(num_blocks[1])\n",
    "        ])\n",
    "\n",
    "        self.up_sample_01_00 = Upsampling(dim_in=dims[1], dim_out=dims[0])\n",
    "        self.combine_channels_00 = nn.Conv2d(dims[0]*2, dims[0], kernel_size=1, bias=False)\n",
    "        self.decoder_scale_00 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[0]) for i in range(num_blocks[0])\n",
    "        ])\n",
    "\n",
    "        self.refining_block = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[0]) for i in range(num_blocks_out)\n",
    "        ])\n",
    "        self.linear_output = nn.Conv2d(dims[0], n_channels_out, kernel_size=1, bias=False)\n",
    "        self.skip_weight_output = Parameter(\n",
    "            torch.tensor([0.5, 0.5], dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        \n",
    "        # Downward\n",
    "        inp_enc_scale_00 = self.patch_3x3_embeding(img)\n",
    "        out_enc_scale_00 = self.encoder_scale_00(inp_enc_scale_00)\n",
    "        # print(f\"out_enc_scale_00:{out_enc_scale_00.shape}\")\n",
    "        \n",
    "        inp_enc_scale_01 = self.down_sample_00_01(out_enc_scale_00)\n",
    "        out_enc_scale_01 = self.encoder_scale_01(inp_enc_scale_01)\n",
    "        # print(f\"inp_enc_scale_01:{inp_enc_scale_01.shape}\")\n",
    "\n",
    "        inp_enc_scale_02 = self.down_sample_01_02(out_enc_scale_01)\n",
    "        out_enc_scale_02 = self.encoder_scale_02(inp_enc_scale_02)\n",
    "        # print(f\"inp_enc_scale_02:{inp_enc_scale_02.shape}\")\n",
    "\n",
    "        inp_enc_scale_03 = self.down_sample_02_03(out_enc_scale_02)\n",
    "        out_enc_scale_03 = self.encoder_scale_03(inp_enc_scale_03)\n",
    "        # print(f\"inp_enc_scale_03:{inp_enc_scale_03.shape}\")\n",
    "\n",
    "        # Upward\n",
    "        inp_dec_scale_02 = self.up_sample_03_02(out_enc_scale_03)\n",
    "        # print(f\"inp_dec_scale_02={inp_dec_scale_02.shape}\")\n",
    "        out_dec_scale_02 = torch.cat([inp_dec_scale_02, out_enc_scale_02], 1)\n",
    "        # print(f\"out_dec_scale_02={out_dec_scale_02.shape}\")\n",
    "        out_dec_scale_02 = self.combine_channels_02(out_dec_scale_02)\n",
    "        # print(f\"out_dec_scale_02={out_dec_scale_02.shape}\")\n",
    "        out_dec_scale_02 = self.decoder_scale_02(out_dec_scale_02)\n",
    "        # print(f\"out_dec_scale_02={out_dec_scale_02.shape}\")\n",
    "\n",
    "        inp_dec_scale_01 = self.up_sample_02_01(out_dec_scale_02)\n",
    "        # print(f\"inp_dec_scale_01={inp_dec_scale_01.shape}\")\n",
    "        out_dec_scale_01 = torch.cat([inp_dec_scale_01, out_enc_scale_01], 1)\n",
    "        # print(f\"out_dec_scale_01={out_dec_scale_01.shape}\")\n",
    "        out_dec_scale_01 = self.combine_channels_01(out_dec_scale_01)\n",
    "        # print(f\"out_dec_scale_01={out_dec_scale_01.shape}\")\n",
    "        out_dec_scale_01 = self.decoder_scale_01(out_dec_scale_01)\n",
    "        # print(f\"out_dec_scale_01={out_dec_scale_01.shape}\")\n",
    "\n",
    "        inp_dec_scale_00 = self.up_sample_01_00(out_dec_scale_01)\n",
    "        # print(f\"inp_dec_scale_00={inp_dec_scale_00.shape}\")\n",
    "        out_dec_scale_00 = torch.cat([inp_dec_scale_00, out_enc_scale_00], 1)\n",
    "        # print(f\"out_dec_scale_00={out_dec_scale_00.shape}\")\n",
    "        out_dec_scale_00 = self.combine_channels_00(out_dec_scale_00)\n",
    "        # print(f\"out_dec_scale_00={out_dec_scale_00.shape}\")\n",
    "        out_dec_scale_00 = self.decoder_scale_00(out_dec_scale_00)\n",
    "        # print(f\"out_dec_scale_00={out_dec_scale_00.shape}\")\n",
    "\n",
    "        output = self.refining_block(out_dec_scale_00)\n",
    "        # print(f\"output={output.shape}\")\n",
    "        output = self.linear_output(output) \n",
    "        # print(f\"output={output.shape}\")\n",
    "        output = self.skip_weight_output[0] * img + self.skip_weight_output[1] * output\n",
    "        # print(f\"output={output.shape}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb744294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74c18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "VERBOSE_RATE = 1000\n",
    "\n",
    "(H_train01, W_train01) = (64, 64)\n",
    "(H_train02, W_train02) = (128, 128)\n",
    "(H_train03, W_train03) = (256, 256)\n",
    "(H_train04, W_train04) = (512, 512)\n",
    "\n",
    "(H_val, W_val) = (128, 128)\n",
    "(H_test, W_test) = (496, 496)\n",
    "\n",
    "train_dataset01 = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"vary_addictive_noise\",\n",
    "    lambda_noise=[[1.0, 10.0, 15.0, 20.0, 25.0], [0.1, 0.1, 0.1, 0.1, 0.6]],\n",
    "    use_data_aug=True,\n",
    "    patch_size=(H_train02,H_train02),\n",
    "    patch_overlap_size=(H_train02//4,H_train02//4),\n",
    "    max_num_patchs=3200000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset01, batch_size=4, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c9a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AbtractMultiScaleGraphFilter(\n",
    "#     n_channels_in=3, \n",
    "#     n_channels_out=3, \n",
    "#     dims=[48, 64, 96, 128],\n",
    "#     num_blocks=[4, 6, 6, 8], \n",
    "#     num_blocks_out=4\n",
    "# ).to(DEVICE)\n",
    "# model.compile()\n",
    "# s = 0\n",
    "# for p in model.parameters():\n",
    "#     s += np.prod(np.array(p.shape))\n",
    "# print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "# for patchs_noisy, patchs_true in data_train_batched01:\n",
    "#     s = time.time()\n",
    "#     patchs_noisy = patchs_noisy.to(DEVICE)\n",
    "#     patchs_true = patchs_true.to(DEVICE) \n",
    "#     reconstruct_patchs = model(patchs_noisy.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e4bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba76ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.eval()\n",
    "# csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "# img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "# paths = img_infos[\"path\"].tolist()\n",
    "# paths = [\n",
    "#     os.path.join(ROOT_DATASET,path)\n",
    "#     for path in paths\n",
    "# ]\n",
    "\n",
    "# sigma_test = 25.0\n",
    "# factor = 8\n",
    "# list_test_mse = []\n",
    "# random_state = np.random.RandomState(seed=2204)\n",
    "# test_i = 0\n",
    "# s = time.time()\n",
    "# for file_ in paths:\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     img = Image.open(file_)\n",
    "#     img_true_255 = np.array(img).astype(np.float32)\n",
    "#     img_true = img_true_255 / 255.0\n",
    "\n",
    "#     noisy_img_raw = img_true.copy()\n",
    "#     noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "#     noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "#     noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "#     h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "#     H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "#     padh = H-h if h%factor!=0 else 0\n",
    "#     padw = W-w if w%factor!=0 else 0\n",
    "#     noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "#     with torch.no_grad():\n",
    "#         restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9f836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970c3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 3720138\n",
      "iter=0 time=51.583767890930176 psnr=9.512264487630633 mse=0.1118854341060725\n",
      "iter=100 time=0.03967165946960449 psnr=17.344542698256443 mse=0.020597555287676133\n",
      "iter=200 time=0.03960895538330078 psnr=20.77634539749856 mse=0.009518438215172054\n",
      "iter=300 time=0.1084437370300293 psnr=23.204233175909334 mse=0.004921384402501978\n",
      "iter=400 time=0.03794431686401367 psnr=23.89153499996663 mse=0.004239912172654279\n",
      "iter=500 time=0.04765176773071289 psnr=23.75865634630084 mse=0.004391514320253632\n",
      "iter=600 time=0.036995649337768555 psnr=23.999545281778587 mse=0.004074403911062467\n",
      "iter=700 time=0.045567989349365234 psnr=24.471600503360307 mse=0.0037402041878354803\n",
      "iter=800 time=0.03797101974487305 psnr=24.846363833598925 mse=0.0033698196445720464\n",
      "iter=900 time=0.04056382179260254 psnr=24.76799831968464 mse=0.0034587656686208496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 22:06:21.177000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:21.726000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.056000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.158000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.239000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.303000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.807000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.301000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.457000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.567000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.660000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.752000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.827000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:24.421000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:24.958000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.122000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.236000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.326000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.419000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.493000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.086000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.606000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.770000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.883000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.977000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.074000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.163000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.253000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.329000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.902000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.398000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.547000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.637000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.735000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.832000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.912000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:29.549000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.014000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.157000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.246000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.337000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.426000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.503000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.126000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.606000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.747000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.835000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.925000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:32.011000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:32.099000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:32.184000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH TESTING - iter=1000 -  psnr_testing=23.80134391784668\n",
      "iter=1000 time=0.07832860946655273 psnr=25.141954204837685 mse=0.003118523203622476\n",
      "iter=1100 time=0.036153316497802734 psnr=24.780601666127268 mse=0.003419877452834531\n",
      "iter=1200 time=0.0366976261138916 psnr=25.19416581003319 mse=0.003099143445443492\n",
      "iter=1300 time=0.036928415298461914 psnr=25.215822753530606 mse=0.003088459560266289\n",
      "iter=1400 time=0.039591312408447266 psnr=25.538570764187966 mse=0.0028549606949842187\n",
      "iter=1500 time=0.03767681121826172 psnr=25.273694271556554 mse=0.003040062083207427\n",
      "iter=1600 time=0.039666175842285156 psnr=25.382411187296647 mse=0.00296520713247309\n",
      "iter=1700 time=0.036934852600097656 psnr=25.415672066211187 mse=0.002985649055764809\n",
      "iter=1800 time=0.038036346435546875 psnr=25.96111087227655 mse=0.002629404718555788\n",
      "iter=1900 time=0.03574967384338379 psnr=25.982652482408113 mse=0.0025854721416759645\n",
      "FINISH TESTING - iter=2000 -  psnr_testing=23.975055694580078\n",
      "iter=2000 time=0.06890320777893066 psnr=26.001139071604417 mse=0.002614787317340572\n",
      "iter=2100 time=0.0368809700012207 psnr=26.068082836699 mse=0.002572372646509784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m list_train_mse = []\n\u001b[32m     33\u001b[39m list_train_psnr = []\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpatchs_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatchs_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_train_batched01\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    945\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    950\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "\n",
    "model = AbtractMultiScaleGraphFilter(\n",
    "    n_channels_in=3, \n",
    "    n_channels_out=3, \n",
    "    dims=[48, 64, 96, 128],\n",
    "    num_blocks=[4, 6, 6, 8], \n",
    "    num_blocks_out=4\n",
    ").to(DEVICE)\n",
    "model.compile()\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "criterian = nn.L1Loss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    eps=1e-08\n",
    ")\n",
    "lr_scheduler = MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[200000, 500000, 650000], gamma=0.5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "i = 0\n",
    "### TRAINING\n",
    "list_train_mse = []\n",
    "list_train_psnr = []\n",
    "for patchs_noisy, patchs_true in data_train_batched01:\n",
    "    s = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    patchs_noisy = patchs_noisy.to(DEVICE)\n",
    "    patchs_true = patchs_true.to(DEVICE) \n",
    "    reconstruct_patchs = model(patchs_noisy.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "    loss_value = criterian(reconstruct_patchs, patchs_true)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    img_true = np.clip(patchs_true.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    img_recon = np.clip(reconstruct_patchs.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    train_mse_value = np.square(img_true- img_recon).mean()\n",
    "    train_psnr = 10 * np.log10(1/train_mse_value)\n",
    "    list_train_psnr.append(train_psnr)\n",
    "    list_train_mse.append(train_mse_value)\n",
    "    if (i%100 == 0):\n",
    "        print(f\"iter={i} time={time.time()-s} psnr={np.mean(list_train_psnr[-100:])} mse={np.mean(list_train_mse[-100:])}\")\n",
    "    i+=1\n",
    "\n",
    "    if (i%1000 == 0):\n",
    "\n",
    "        model.eval()\n",
    "        csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "        img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "        paths = img_infos[\"path\"].tolist()\n",
    "        paths = [\n",
    "            os.path.join(ROOT_DATASET,path)\n",
    "            for path in paths\n",
    "        ]\n",
    "\n",
    "        sigma_test = 25.0\n",
    "        factor = 8\n",
    "        list_test_mse = []\n",
    "        random_state = np.random.RandomState(seed=2204)\n",
    "        test_i = 0\n",
    "        s = time.time()\n",
    "        for file_ in paths:\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            img = Image.open(file_)\n",
    "            img_true_255 = np.array(img).astype(np.float32)\n",
    "            img_true = img_true_255 / 255.0\n",
    "\n",
    "            noisy_img_raw = img_true.copy()\n",
    "            noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "            noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "            noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "            h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "            H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "            padh = H-h if h%factor!=0 else 0\n",
    "            padw = W-w if w%factor!=0 else 0\n",
    "            noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "            restored = restored[:,:,:h,:w]\n",
    "            restored = torch.clamp(restored,0,1).cpu().detach().permute(0, 2, 3, 1).squeeze(0).numpy().copy()\n",
    "\n",
    "            restored = img_as_ubyte(restored).astype(np.float32)\n",
    "            test_mse_value = np.square(img_true_255- restored).mean()\n",
    "            list_test_mse.append(test_mse_value)\n",
    "            # print(f\"test_i={test_i} time={time.time()-s} test_i_psnr_value={20 * np.log10(255.0 / np.sqrt(test_mse_value))}\")  \n",
    "            test_i += 1\n",
    "            s = time.time()\n",
    "\n",
    "        psnr_testing = 20 * np.log10(255.0 / np.sqrt(list_test_mse))\n",
    "        print(f\"FINISH TESTING - iter={i} -  psnr_testing={np.mean(psnr_testing)}\")\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003deaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31659b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf907b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9995edd2",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72601b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader import ImageSuperResolution\n",
    "import baselineRestormer as model_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75be36a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 26111668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = model_structure.Restormer(\n",
    "    inp_channels=3,\n",
    "    out_channels=3,\n",
    "    dim=48,\n",
    "    num_blocks=[4,6,6,8],\n",
    "    num_refinement_blocks=4,\n",
    "    heads=[1,2,4,8],\n",
    "    ffn_expansion_factor=2.66,\n",
    "    bias=False,\n",
    "    LayerNorm_type=\"BiasFree\",\n",
    "    dual_pixel_task=False\n",
    ").to(DEVICE)\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "    # print(p.dtype, np.array(p.shape), s)\n",
    "\n",
    "print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "criterian = nn.L1Loss()\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0006,\n",
    "    weight_decay=0.0001,\n",
    "    eps=1e-08\n",
    ")\n",
    "# [100000, 200000, 350000, 500000, 575000, 650000]\n",
    "lr_scheduler01 = MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[25000, 50000, 75000, 100000],\n",
    "    gamma=np.sqrt(np.sqrt(0.5))\n",
    ")\n",
    "lr_scheduler02 = CosineAnnealingLR(optimizer, T_max=700000, eta_min=0.000001)\n",
    "lr_scheduler02.base_lrs = [0.0003 for group in optimizer.param_groups]\n",
    "\n",
    "lr_scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[lr_scheduler01, lr_scheduler02],\n",
    "    milestones=[160000],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6bf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a93bfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "for i in range(700000):\n",
    "    lr_list.append(lr_scheduler.get_last_lr())\n",
    "    # if (i%10000==0):\n",
    "    #     print(f\"i={i}\")\n",
    "    # optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bd3027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f853c6a50>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1455b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(size=(4, 16, 4, 4))\n",
    "m = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2, padding=0, output_padding=(0, 0), groups=1, bias=False)\n",
    "m(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa356a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a4b84f3",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "torch.set_default_device(DEVICE)\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader_v2 import ImageSuperResolution\n",
    "import model_GLR_GTV_deep_v13 as model_structure\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a433371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/result/model_test31_v13/checkpoints/\")\n",
    "training_state_path = os.path.join(CHECKPOINT_DIR, 'checkpoints_epoch00_iter0300k.pt')\n",
    "training_state = torch.load(training_state_path, weights_only=False)\n",
    "\n",
    "model =  model_structure.AbtractMultiScaleGraphFilter(\n",
    "    n_channels_in=3, \n",
    "    n_channels_out=3, \n",
    "    dims=[48, 96, 192, 384],\n",
    "    hidden_dims=[96, 192, 384, 768],\n",
    "    nsubnets=[1, 1, 1, 1],\n",
    "    ngraphs=[8, 16, 16, 32], #[1, 2, 4, 8], \n",
    "    num_blocks=[4, 6, 6, 8], \n",
    "    num_blocks_out=4\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(training_state[\"model\"])\n",
    "# model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38bdaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_i=0 time=0.6890604496002197 test_i_psnr_value=29.24049186706543\n",
      "test_i=1 time=0.7794215679168701 test_i_psnr_value=33.78015899658203\n",
      "test_i=2 time=0.8811635971069336 test_i_psnr_value=33.9612922668457\n",
      "test_i=3 time=0.8006758689880371 test_i_psnr_value=35.194183349609375\n",
      "test_i=4 time=0.7746658325195312 test_i_psnr_value=37.8023681640625\n",
      "test_i=5 time=0.7949795722961426 test_i_psnr_value=35.089908599853516\n",
      "test_i=6 time=0.7424466609954834 test_i_psnr_value=34.82344436645508\n",
      "test_i=7 time=0.8088281154632568 test_i_psnr_value=30.306798934936523\n",
      "test_i=8 time=0.7753331661224365 test_i_psnr_value=30.580310821533203\n",
      "test_i=9 time=0.7984919548034668 test_i_psnr_value=31.884693145751953\n",
      "test_i=10 time=0.7922468185424805 test_i_psnr_value=31.932828903198242\n",
      "test_i=11 time=0.792827844619751 test_i_psnr_value=30.69775390625\n",
      "test_i=12 time=0.7955715656280518 test_i_psnr_value=33.99251937866211\n",
      "test_i=13 time=0.7767887115478516 test_i_psnr_value=31.96942138671875\n",
      "test_i=14 time=0.769406795501709 test_i_psnr_value=33.029808044433594\n",
      "test_i=15 time=0.7855827808380127 test_i_psnr_value=31.23883056640625\n",
      "test_i=16 time=0.47870969772338867 test_i_psnr_value=34.20487594604492\n",
      "test_i=17 time=0.8238661289215088 test_i_psnr_value=33.70917892456055\n",
      "FINISH TESTING  -  psnr_testing=32.96882629394531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AbtractMultiScaleGraphFilter(\n",
       "  (patch_3x3_embeding): ReginalPixelEmbeding(\n",
       "    (channels_local_linear_op01): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
       "  )\n",
       "  (encoder_scale_00): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_sample_00_01): Downsampling(\n",
       "    (local_linear): Conv2d(48, 96, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (encoder_scale_01): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_sample_01_02): Downsampling(\n",
       "    (local_linear): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (encoder_scale_02): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_sample_02_03): Downsampling(\n",
       "    (local_linear): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (encoder_scale_03): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (6): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (7): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (localfilter_scale_00): LocalLowpassFilteringBlock(\n",
       "    (local_filter): MixtureGTVGLR(\n",
       "      (patchs_features_extraction00): Sequential(\n",
       "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule00): GTVFast()\n",
       "      (GLRmodule00): GLRFast()\n",
       "      (patchs_features_extraction01): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule01): GTVFast()\n",
       "      (GLRmodule01): GLRFast()\n",
       "    )\n",
       "  )\n",
       "  (localfilter_scale_01): LocalLowpassFilteringBlock(\n",
       "    (local_filter): MixtureGTVGLR(\n",
       "      (patchs_features_extraction00): Sequential(\n",
       "        (0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule00): GTVFast()\n",
       "      (GLRmodule00): GLRFast()\n",
       "      (patchs_features_extraction01): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule01): GTVFast()\n",
       "      (GLRmodule01): GLRFast()\n",
       "    )\n",
       "  )\n",
       "  (localfilter_scale_02): LocalLowpassFilteringBlock(\n",
       "    (local_filter): MixtureGTVGLR(\n",
       "      (patchs_features_extraction00): Sequential(\n",
       "        (0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule00): GTVFast()\n",
       "      (GLRmodule00): GLRFast()\n",
       "      (patchs_features_extraction01): Sequential(\n",
       "        (0): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule01): GTVFast()\n",
       "      (GLRmodule01): GLRFast()\n",
       "    )\n",
       "  )\n",
       "  (localfilter_scale_03): LocalLowpassFilteringBlock(\n",
       "    (local_filter): MixtureGTVGLR(\n",
       "      (patchs_features_extraction00): Sequential(\n",
       "        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule00): GTVFast()\n",
       "      (GLRmodule00): GLRFast()\n",
       "      (patchs_features_extraction01): Sequential(\n",
       "        (0): Conv2d(384, 384, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (GTVmodule01): GTVFast()\n",
       "      (GLRmodule01): GLRFast()\n",
       "    )\n",
       "  )\n",
       "  (up_sample_03_02): Upsampling(\n",
       "    (local_linear): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (combine_channels_02): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (decoder_scale_02): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_sample_02_01): Upsampling(\n",
       "    (local_linear): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (combine_channels_01): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (decoder_scale_01): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_sample_01_00): Upsampling(\n",
       "    (local_linear): ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (combine_channels_00): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (decoder_scale_00): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (refining_block): Sequential(\n",
       "    (0): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): LocalNonLinearBlock(\n",
       "      (norm): CustomLayerNorm(\n",
       "        (weighted_transform): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "      )\n",
       "      (local_linear): LocalGatedLinearBlock(\n",
       "        (channels_linear_op): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (channels_local_linear_op): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False, padding_mode=replicate)\n",
       "        (project_out): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_output): Conv2d(48, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "paths = img_infos[\"path\"].tolist()\n",
    "paths = [\n",
    "    os.path.join(ROOT_DATASET,path)\n",
    "    for path in paths\n",
    "]\n",
    "\n",
    "sigma_test = 25.0\n",
    "factor = 16\n",
    "list_test_mse = []\n",
    "random_state = np.random.RandomState(seed=2204)\n",
    "test_i = 0\n",
    "s = time.time()\n",
    "for file_ in paths:\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    img = Image.open(file_)\n",
    "    img_true_255 = np.array(img).astype(np.float32)\n",
    "    img_true = img_true_255 / 255.0\n",
    "\n",
    "    noisy_img_raw = img_true.copy()\n",
    "    noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "    noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "    noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "    h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "    H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "    padh = H-h if h%factor!=0 else 0\n",
    "    padw = W-w if w%factor!=0 else 0\n",
    "    noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "    restored = restored[:,:,:h,:w]\n",
    "    restored = torch.clamp(restored,0,1).cpu().detach().permute(0, 2, 3, 1).squeeze(0).numpy().copy()\n",
    "\n",
    "    restored = img_as_ubyte(restored).astype(np.float32)\n",
    "    test_mse_value = np.square(img_true_255- restored).mean()\n",
    "    list_test_mse.append(test_mse_value)\n",
    "    print(f\"test_i={test_i} time={time.time()-s} test_i_psnr_value={20 * np.log10(255.0 / np.sqrt(test_mse_value))}\")  \n",
    "    test_i += 1\n",
    "    s = time.time()\n",
    "\n",
    "psnr_testing = 20 * np.log10(255.0 / np.sqrt(list_test_mse))\n",
    "print(f\"FINISH TESTING  -  psnr_testing={np.mean(psnr_testing)}\")\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b8b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc2398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a760df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "VERBOSE_RATE = 1000\n",
    "\n",
    "(H_train01, W_train01) = (64, 64)\n",
    "(H_train02, W_train02) = (128, 128)\n",
    "(H_train03, W_train03) = (256, 256)\n",
    "(H_train04, W_train04) = (512, 512)\n",
    "\n",
    "\n",
    "train_dataset01 = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"vary_addictive_noise\",\n",
    "    lambda_noise=[[1.0, 10.0, 15.0, 20.0, 25.0], [0.1, 0.1, 0.1, 0.1, 0.6]],\n",
    "    use_data_aug=True,\n",
    "    patch_size=(H_train02,W_train02),\n",
    "    max_num_patchs=800000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset01, batch_size=16, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f9106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561dba72",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e1ca71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader_v3 import ImageSuperResolution\n",
    "import model_GLR_GTV_deep_v13 as model_structure\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5665f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 9891168\n"
     ]
    }
   ],
   "source": [
    "model = model_structure.AbtractMultiScaleGraphFilter(\n",
    "    n_channels_in=3, \n",
    "    n_channels_abtract=48, \n",
    "    n_channels_out=3, \n",
    "    dims=[48, 96, 192, 384],\n",
    "    hidden_dims=[128, 256, 512, 1024],\n",
    "    nsubnets=[1, 1, 1, 1],\n",
    "    ngraphs=[8],\n",
    "    num_blocks=[4, 4, 4, 4], \n",
    "    num_blocks_out=4\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "    # print(p.dtype, np.array(p.shape), s)\n",
    "\n",
    "print(f\"Init model with total parameters: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21d7c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "train_dataset = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"addictive_noise\",\n",
    "    lambda_noise=25.0,\n",
    "    use_data_aug=True,\n",
    "    patch_size=(128,128),\n",
    "    max_num_patchs=200000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f420767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>resize</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>nchannels</th>\n",
       "      <th>path</th>\n",
       "      <th>row</th>\n",
       "      <th>col</th>\n",
       "      <th>padding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>170766</td>\n",
       "      <td>False</td>\n",
       "      <td>343</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/02204.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>142792</td>\n",
       "      <td>False</td>\n",
       "      <td>322</td>\n",
       "      <td>450</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/00284.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8311</td>\n",
       "      <td>False</td>\n",
       "      <td>321</td>\n",
       "      <td>481</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/BSD400/107072.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64914</td>\n",
       "      <td>False</td>\n",
       "      <td>375</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/00571.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>103156</td>\n",
       "      <td>False</td>\n",
       "      <td>315</td>\n",
       "      <td>441</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/04080.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199966</th>\n",
       "      <td>199289</td>\n",
       "      <td>False</td>\n",
       "      <td>305</td>\n",
       "      <td>460</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/04679.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199969</th>\n",
       "      <td>57351</td>\n",
       "      <td>False</td>\n",
       "      <td>448</td>\n",
       "      <td>331</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/01693.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199973</th>\n",
       "      <td>121116</td>\n",
       "      <td>False</td>\n",
       "      <td>306</td>\n",
       "      <td>460</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/04671.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>14570</td>\n",
       "      <td>False</td>\n",
       "      <td>379</td>\n",
       "      <td>469</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/02338.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>57766</td>\n",
       "      <td>False</td>\n",
       "      <td>370</td>\n",
       "      <td>455</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/02109.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56444 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  resize  height  width  nchannels  \\\n",
       "6       170766   False     343    500          3   \n",
       "11      142792   False     322    450          3   \n",
       "14        8311   False     321    481          3   \n",
       "21       64914   False     375    500          3   \n",
       "22      103156   False     315    441          3   \n",
       "...        ...     ...     ...    ...        ...   \n",
       "199966  199289   False     305    460          3   \n",
       "199969   57351   False     448    331          3   \n",
       "199973  121116   False     306    460          3   \n",
       "199985   14570   False     379    469          3   \n",
       "199991   57766   False     370    455          3   \n",
       "\n",
       "                                                                                 path  \\\n",
       "6       /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/02204.bmp   \n",
       "11      /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/00284.bmp   \n",
       "14         /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/BSD400/107072.jpg   \n",
       "21      /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/00571.bmp   \n",
       "22      /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/04080.bmp   \n",
       "...                                                                               ...   \n",
       "199966  /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/04679.bmp   \n",
       "199969  /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/01693.bmp   \n",
       "199973  /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/04671.bmp   \n",
       "199985  /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/02338.bmp   \n",
       "199991  /home/jovyan/shared/Thuc/hoodsgatedrive/projects/dataset/WaterlooED/02109.bmp   \n",
       "\n",
       "        row  col  padding  \n",
       "6         0    0     True  \n",
       "11        0    0     True  \n",
       "14        0    0     True  \n",
       "21        0    0     True  \n",
       "22        0    0     True  \n",
       "...     ...  ...      ...  \n",
       "199966    0    0     True  \n",
       "199969    0    0     True  \n",
       "199973    0    0     True  \n",
       "199985    0    0     True  \n",
       "199991    0    0     True  \n",
       "\n",
       "[56444 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.patchs_data.reset_index().query(\"resize==False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08b9be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=16, num_workers=4\n",
    ")\n",
    "for patchs_noisy, patchs_true in data_train_batched01:\n",
    "\n",
    "    img_true = patchs_true.detach().cpu().numpy()\n",
    "    # img_true = np.clip(patchs_true.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    img_noise = patchs_noisy.detach().cpu().numpy()\n",
    "    # img_noise = np.clip(patchs_noisy.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f005a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "patchs_noisy, patchs_true = train_dataset[21]\n",
    "\n",
    "img_true = patchs_true.detach().cpu().numpy()\n",
    "img_noise = patchs_noisy.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb95dfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (16, 128, 128, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m16\u001b[39m):\n\u001b[32m      2\u001b[39m     fig, ax = plt.subplots(ncols=\u001b[32m2\u001b[39m, nrows=\u001b[32m1\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_true\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     ax[\u001b[32m0\u001b[39m].set_title(\u001b[33m\"\u001b[39m\u001b[33mTrue Image\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     ax[\u001b[32m1\u001b[39m].imshow((img_noise*\u001b[32m255\u001b[39m).astype(np.uint8))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/matplotlib/__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5979\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5976\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5977\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5979\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5980\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5982\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/matplotlib/image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/matplotlib/image.py:653\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    651\u001b[39m     A = A.squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A.shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for image data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[32m    659\u001b[39m     high = \u001b[32m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.issubdtype(A.dtype, np.integer) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Invalid shape (16, 128, 128, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15, 8))\n",
    "    ax[0].imshow((img_true[i]*255).astype(np.uint8))\n",
    "    ax[0].set_title(\"True Image\")\n",
    "    ax[1].imshow((img_noise[i]*255).astype(np.uint8))\n",
    "    ax[1].set_title(\"Noisy Image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1124885",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c8835",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606ceba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8a0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee1aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da24bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
