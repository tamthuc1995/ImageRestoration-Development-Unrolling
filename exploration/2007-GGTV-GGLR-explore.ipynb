{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6e5485",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca97087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "# ROOT_PROJECT = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling/\"\n",
    "# ROOT_DATASET = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling\"\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader import ImageSuperResolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe5ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, nchannels):\n",
    "        super(CustomLayerNorm, self).__init__()\n",
    "        \n",
    "        self.nchannels = nchannels\n",
    "        self.weighted_transform = nn.Conv2d(nchannels, nchannels, kernel_size=1, stride=1, groups=nchannels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bz, nchannels, h, w = x.shape\n",
    "        sigma = x.var(dim=1, keepdim=True, correction=1)\n",
    "        # bz, 1, h, w = sigma.shape\n",
    "        return self.weighted_transform(x / torch.sqrt(sigma+1e-5))\n",
    "    \n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "        # hidden_features = dim\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = nn.functional.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class FFBlock(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FFBlock, self).__init__()\n",
    "\n",
    "        self.norm = CustomLayerNorm(dim)\n",
    "\n",
    "        self.skip_connect_weight_final = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32) * torch.tensor([0.5, 0.5]),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.skip_connect_weight_final[0]*x + self.skip_connect_weight_final[1]*self.ffn(self.norm(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "        # self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        #                           nn.PixelUnshuffle(2))\n",
    "        \n",
    "        # self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat, kernel_size=3, stride=2, padding=1, padding_mode=\"replicate\", bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "        # self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        #                           nn.PixelShuffle(2))\n",
    "\n",
    "        # self.body = nn.Sequential(nn.ConvTranspose2d(n_feat, n_feat, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "##########################################################################\n",
    "# class FeatureExtraction(nn.Module):\n",
    "#     def __init__(self, \n",
    "#         inp_channels=3, \n",
    "#         out_channels=48, \n",
    "#         dim = 48,\n",
    "#         num_blocks = [2,2,2,2], \n",
    "#         num_refinement_blocks = 4,\n",
    "#         ffn_expansion_factor = 2.0,\n",
    "#         bias = False,\n",
    "#     ):\n",
    "\n",
    "#         super(FeatureExtraction, self).__init__()\n",
    "\n",
    "#         self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "#         self.encoder_level1 = nn.Sequential(*[FFBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "#         self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "#         self.encoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "#         self.down2_3 = Downsample(int(dim)) ## From Level 2 to Level 3\n",
    "#         self.encoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "#         self.down3_4 = Downsample(int(dim)) ## From Level 3 to Level 4\n",
    "#         self.latent = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[3])])\n",
    "        \n",
    "#         self.up4_3 = Upsample(int(dim)) ## From Level 4 to Level 3\n",
    "#         self.reduce_chan_level3 = nn.Conv2d(int(dim*2), int(dim), kernel_size=1, bias=bias)\n",
    "#         self.decoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "#         self.up3_2 = Upsample(int(dim)) ## From Level 3 to Level 2\n",
    "#         self.reduce_chan_level2 = nn.Conv2d(int(dim*2), int(dim), kernel_size=1, bias=bias)\n",
    "#         self.decoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "#         self.up2_1 = Upsample(int(dim))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "\n",
    "#         self.decoder_level1 = nn.Sequential(*[FFBlock(dim=int(dim*2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "#         self.refinement = nn.Sequential(*[FFBlock(dim=int(dim*2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_refinement_blocks)])\n",
    "\n",
    "#         ###########################\n",
    "#         self.output = nn.Conv2d(int(dim*2), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "#     def forward(self, inp_img):\n",
    "\n",
    "#         inp_enc_level1 = self.patch_embed(inp_img)\n",
    "#         out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "        \n",
    "#         inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "#         out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "#         inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "#         out_enc_level3 = self.encoder_level3(inp_enc_level3) \n",
    "\n",
    "#         inp_enc_level4 = self.down3_4(out_enc_level3)        \n",
    "#         latent = self.latent(inp_enc_level4) \n",
    "                        \n",
    "#         inp_dec_level3 = self.up4_3(latent)\n",
    "#         inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
    "#         inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
    "#         out_dec_level3 = self.decoder_level3(inp_dec_level3) \n",
    "\n",
    "#         inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "#         inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
    "#         inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
    "#         out_dec_level2 = self.decoder_level2(inp_dec_level2) \n",
    "\n",
    "#         inp_dec_level1 = self.up2_1(out_dec_level2)\n",
    "#         inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "#         out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "        \n",
    "#         out_dec_level1 = self.refinement(out_dec_level1)\n",
    "#         out_dec_level1 = self.output(out_dec_level1)\n",
    "\n",
    "#         # return [latent, out_dec_level3, out_dec_level2, out_dec_level1]\n",
    "#         return [out_dec_level1]\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(self, \n",
    "        inp_channels=3, \n",
    "        out_channels=48, \n",
    "        dim = 48,\n",
    "        num_blocks = [1,2,2,4], \n",
    "        num_refinement_blocks = 4,\n",
    "        ffn_expansion_factor = 2.66,\n",
    "        bias = False,\n",
    "    ):\n",
    "\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[FFBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "        self.encoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "        # self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
    "        # self.encoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim*2**2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "        # self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
    "        # self.latent = nn.Sequential(*[FFBlock(dim=int(dim*2**3), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[3])])\n",
    "        \n",
    "        # self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
    "        # self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
    "        # self.decoder_level3 = nn.Sequential(*[FFBlock(dim=int(dim*2**2), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[2])])\n",
    "\n",
    "        # self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
    "        # self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
    "        # self.decoder_level2 = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[1])])\n",
    "        \n",
    "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "\n",
    "        self.decoder_level1 = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.refinement = nn.Sequential(*[FFBlock(dim=int(dim*2**1), ffn_expansion_factor=ffn_expansion_factor, bias=bias) for i in range(num_refinement_blocks)])\n",
    "\n",
    "        ###########################\n",
    "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "        \n",
    "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "        latent = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "        # inp_enc_level3 = self.down2_3(out_enc_level2)\n",
    "        # out_enc_level3 = self.encoder_level3(inp_enc_level3) \n",
    "\n",
    "        # inp_enc_level4 = self.down3_4(out_enc_level3)        \n",
    "        # latent = self.latent(inp_enc_level4) \n",
    "                        \n",
    "        # inp_dec_level3 = self.up4_3(latent)\n",
    "        # inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
    "        # inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
    "        # out_dec_level3 = self.decoder_level3(inp_dec_level3) \n",
    "\n",
    "        # inp_dec_level2 = self.up3_2(out_dec_level3)\n",
    "        # inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
    "        # inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
    "        # out_dec_level2 = self.decoder_level2(inp_dec_level2) \n",
    "\n",
    "        inp_dec_level1 = self.up2_1(latent)\n",
    "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
    "        \n",
    "        out_dec_level1 = self.refinement(out_dec_level1)\n",
    "        out_dec_level1 = self.output(out_dec_level1)\n",
    "\n",
    "        return [out_dec_level1]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GLRFast(nn.Module):\n",
    "    def __init__(self, \n",
    "            n_channels, n_node_fts, n_graphs, connection_window, device, M_diag_init=0.4\n",
    "        ):\n",
    "        super(GLRFast, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.n_channels        = n_channels\n",
    "        self.n_node_fts        = n_node_fts\n",
    "        self.n_graphs          = n_graphs\n",
    "        self.n_edges           = (connection_window == 1).sum()\n",
    "        self.connection_window = connection_window\n",
    "        self.buffer_size       = connection_window.sum()\n",
    "\n",
    "        # edges type from connection_window\n",
    "        window_size = connection_window.shape[0]\n",
    "        connection_window = connection_window.reshape((-1))\n",
    "        m = np.arange(window_size)-window_size//2\n",
    "        edge_delta = np.array(\n",
    "            list(itertools.product(m, m)),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.edge_delta = edge_delta[connection_window == 1]\n",
    "        \n",
    "        self.pad_dim_hw = np.abs(self.edge_delta.min(axis=0))\n",
    "\n",
    "\n",
    "        kernel01 = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel01[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p01 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 1.0,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel01 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02a = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 1.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02a[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02a = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02a = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02b = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02b[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02b = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02b = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel03 = torch.tensor([\n",
    "            [0.0, -1.0, 0.0],\n",
    "            [-1.0, 4.0, -1.0],\n",
    "            [0.0, -1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel03[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p03 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel03 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        ### Trainable parameters\n",
    "        # features on nodes #self.n_node_fts\n",
    "        self.multiM = Parameter(\n",
    "            torch.ones((self.n_graphs, self.n_node_fts), device=self.device, dtype=torch.float32)*M_diag_init,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def get_neighbors_pixels(self, img_features):\n",
    "        _, _, H, W = img_features.shape\n",
    "        padH, padW = self.pad_dim_hw\n",
    "        img_features_frame = nn.functional.pad(img_features, (padW, padW, padH, padH), \"replicate\")\n",
    "        # neighbors_pixels = []\n",
    "        # for shift_h, shift_w in self.edge_delta:\n",
    "        #     fromh = padH + shift_h\n",
    "        #     toh = padH + shift_h + H\n",
    "        #     fromw = padW + shift_w\n",
    "        #     tow = padW + shift_w + W\n",
    "            \n",
    "        #     neighbors_pixels.append(\n",
    "        #         img_features_frame[:, :, fromh:toh, fromw:tow]\n",
    "        #     )\n",
    "        # neighbors_pixels_features = torch.stack(neighbors_pixels, axis=-3)\n",
    "\n",
    "        neighbors_pixels_features = torch.stack([\n",
    "            img_features_frame[:, :, padH + self.edge_delta[0, 0]:padH + self.edge_delta[0, 0] + H, padW + self.edge_delta[0, 1]:padW + self.edge_delta[0, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[1, 0]:padH + self.edge_delta[1, 0] + H, padW + self.edge_delta[1, 1]:padW + self.edge_delta[1, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[2, 0]:padH + self.edge_delta[2, 0] + H, padW + self.edge_delta[2, 1]:padW + self.edge_delta[2, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[3, 0]:padH + self.edge_delta[3, 0] + H, padW + self.edge_delta[3, 1]:padW + self.edge_delta[3, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[4, 0]:padH + self.edge_delta[4, 0] + H, padW + self.edge_delta[4, 1]:padW + self.edge_delta[4, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[5, 0]:padH + self.edge_delta[5, 0] + H, padW + self.edge_delta[5, 1]:padW + self.edge_delta[5, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[6, 0]:padH + self.edge_delta[6, 0] + H, padW + self.edge_delta[6, 1]:padW + self.edge_delta[6, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[7, 0]:padH + self.edge_delta[7, 0] + H, padW + self.edge_delta[7, 1]:padW + self.edge_delta[7, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[8, 0]:padH + self.edge_delta[8, 0] + H, padW + self.edge_delta[8, 1]:padW + self.edge_delta[8, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[9, 0]:padH + self.edge_delta[9, 0] + H, padW + self.edge_delta[9, 1]:padW + self.edge_delta[9, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[10, 0]:padH + self.edge_delta[10, 0] + H, padW + self.edge_delta[10, 1]:padW + self.edge_delta[10, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[11, 0]:padH + self.edge_delta[11, 0] + H, padW + self.edge_delta[11, 1]:padW + self.edge_delta[11, 1] + W]\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[12, 0]:padH + self.edge_delta[12, 0] + H, padW + self.edge_delta[12, 1]:padW + self.edge_delta[12, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[13, 0]:padH + self.edge_delta[13, 0] + H, padW + self.edge_delta[13, 1]:padW + self.edge_delta[13, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[14, 0]:padH + self.edge_delta[14, 0] + H, padW + self.edge_delta[14, 1]:padW + self.edge_delta[14, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[15, 0]:padH + self.edge_delta[15, 0] + H, padW + self.edge_delta[15, 1]:padW + self.edge_delta[15, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[16, 0]:padH + self.edge_delta[16, 0] + H, padW + self.edge_delta[16, 1]:padW + self.edge_delta[16, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[17, 0]:padH + self.edge_delta[17, 0] + H, padW + self.edge_delta[17, 1]:padW + self.edge_delta[17, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[18, 0]:padH + self.edge_delta[18, 0] + H, padW + self.edge_delta[18, 1]:padW + self.edge_delta[18, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[19, 0]:padH + self.edge_delta[19, 0] + H, padW + self.edge_delta[19, 1]:padW + self.edge_delta[19, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[20, 0]:padH + self.edge_delta[20, 0] + H, padW + self.edge_delta[20, 1]:padW + self.edge_delta[20, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[21, 0]:padH + self.edge_delta[21, 0] + H, padW + self.edge_delta[21, 1]:padW + self.edge_delta[21, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[22, 0]:padH + self.edge_delta[22, 0] + H, padW + self.edge_delta[22, 1]:padW + self.edge_delta[22, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[23, 0]:padH + self.edge_delta[23, 0] + H, padW + self.edge_delta[23, 1]:padW + self.edge_delta[23, 1] + W]\n",
    "        ], axis=-3)\n",
    "        return neighbors_pixels_features\n",
    "\n",
    "    def normalize_and_transform_features(self, img_features):\n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "        # img_features = img_features.view(batch_size, self.n_graphs, self.n_node_fts, h_size, w_size)\n",
    "        img_features = torch.nn.functional.normalize(img_features, dim=2)\n",
    "\n",
    "        img_features_transform = torch.einsum(\n",
    "            \"bhcHW, hc -> bhcHW\", img_features, self.multiM\n",
    "        )\n",
    "    \n",
    "        img_features_transform = img_features_transform.view(batch_size, self.n_graphs*self.n_node_fts, h_size, w_size)\n",
    "\n",
    "        return img_features_transform\n",
    "\n",
    "\n",
    "    def extract_edge_weights(self, img_features):\n",
    "        \n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "\n",
    "        img_features = self.normalize_and_transform_features(img_features)\n",
    "        img_features_neighbors = self.get_neighbors_pixels(img_features)\n",
    "\n",
    "        features_similarity = (img_features[:, :, None, :, :] * img_features_neighbors)\n",
    "        features_similarity = features_similarity.view(\n",
    "            batch_size, self.n_graphs, self.n_node_fts, self.n_edges, h_size, w_size\n",
    "        ).sum(axis=2)\n",
    "\n",
    "        edge_weights_norm = nn.functional.softmax(features_similarity, dim=2) \n",
    "        node_degree = edge_weights_norm.sum(axis=2)\n",
    "\n",
    "        return edge_weights_norm, node_degree\n",
    "    \n",
    "    def stats_conv(self, patchs):\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.view(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_patch = nn.functional.pad(temp_patch, (1,1,1,1), 'reflect')\n",
    "        temp_out_patch = nn.functional.conv2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "    def stats_conv_transpose(self, patchs):\n",
    "\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.reshape(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_out_patch = nn.functional.conv_transpose2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "\n",
    "    def op_L_norm(self, img_signals, edge_weights, node_degree):\n",
    "\n",
    "        batch_size, n_graphs, n_channels, h_size, w_size = img_signals.shape\n",
    "        img_features_neighbors = self.get_neighbors_pixels(\n",
    "            img_signals.view(batch_size, n_graphs*n_channels, h_size, w_size)\n",
    "        ).view(batch_size, n_graphs, n_channels, self.n_edges, h_size, w_size)\n",
    "        Wx = torch.einsum(\n",
    "            \"bhceHW, bheHW -> bhcHW\", img_features_neighbors, edge_weights\n",
    "        )\n",
    "        output = img_signals - Wx\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, patchs, edge_weights, node_degree):\n",
    "        # output_patchs = self.op_L_norm(patchs, edge_weights, node_degree)\n",
    "        patchs = self.stats_conv(patchs)\n",
    "        output_patchs = self.op_L_norm(patchs, edge_weights, node_degree)\n",
    "        output_patchs = self.stats_conv_transpose(output_patchs)\n",
    "\n",
    "        return output_patchs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class GTVFast(nn.Module):\n",
    "    def __init__(self, \n",
    "            n_channels, n_node_fts, n_graphs, connection_window, device, M_diag_init=0.4\n",
    "        ):\n",
    "        super(GTVFast, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.n_channels        = n_channels\n",
    "        self.n_node_fts        = n_node_fts\n",
    "        self.n_graphs          = n_graphs\n",
    "        self.n_edges           = (connection_window == 1).sum()\n",
    "        self.connection_window = connection_window\n",
    "        self.buffer_size       = connection_window.sum()\n",
    "\n",
    "        # edges type from connection_window\n",
    "        window_size = connection_window.shape[0]\n",
    "        connection_window = connection_window.reshape((-1))\n",
    "        m = np.arange(window_size)-window_size//2\n",
    "        edge_delta = np.array(\n",
    "            list(itertools.product(m, m)),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.edge_delta = edge_delta[connection_window == 1]\n",
    "        \n",
    "        self.pad_dim_hw = np.abs(self.edge_delta.min(axis=0))\n",
    "\n",
    "        kernel01 = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel01[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p01 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 1.0,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel01 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02a = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 1.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02a[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02a = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02a = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel02b = torch.tensor([\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0,-1.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel02b[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p02b = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel02b = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        kernel03 = torch.tensor([\n",
    "            [0.0, -1.0, 0.0],\n",
    "            [-1.0, 4.0, -1.0],\n",
    "            [0.0, -1.0, 0.0],\n",
    "        ])\n",
    "        kernel = []\n",
    "        for r in range(self.n_channels):\n",
    "            kernel.append(kernel03[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "        kernel = torch.concat(kernel, axis=0).to(self.device)\n",
    "        self.stats_kernel_p03 = Parameter(\n",
    "            torch.ones((1), device=self.device, dtype=torch.float32) * 0.5,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.stats_kernel03 = torch.ones((self.n_channels, 1, 3, 3), device=self.device, dtype=torch.float32) * kernel\n",
    "\n",
    "        ### Trainable parameters\n",
    "        # features on nodes #self.n_node_fts\n",
    "        self.multiM = Parameter(\n",
    "            torch.ones((self.n_graphs, self.n_node_fts), device=self.device, dtype=torch.float32)*M_diag_init,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_neighbors_pixels(self, img_features):\n",
    "        _, _, H, W = img_features.shape\n",
    "        padH, padW = self.pad_dim_hw\n",
    "        img_features_frame = nn.functional.pad(img_features, (padW, padW, padH, padH), \"replicate\")\n",
    "        # neighbors_pixels = []\n",
    "        # for shift_h, shift_w in self.edge_delta:\n",
    "        #     fromh = padH + shift_h\n",
    "        #     toh = padH + shift_h + H\n",
    "        #     fromw = padW + shift_w\n",
    "        #     tow = padW + shift_w + W\n",
    "            \n",
    "        #     neighbors_pixels.append(\n",
    "        #         img_features_frame[:, :, fromh:toh, fromw:tow]\n",
    "        #     )\n",
    "        # neighbors_pixels_features = torch.stack(neighbors_pixels, axis=-3)\n",
    "        neighbors_pixels_features = torch.stack([\n",
    "            img_features_frame[:, :, padH + self.edge_delta[0, 0]:padH + self.edge_delta[0, 0] + H, padW + self.edge_delta[0, 1]:padW + self.edge_delta[0, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[1, 0]:padH + self.edge_delta[1, 0] + H, padW + self.edge_delta[1, 1]:padW + self.edge_delta[1, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[2, 0]:padH + self.edge_delta[2, 0] + H, padW + self.edge_delta[2, 1]:padW + self.edge_delta[2, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[3, 0]:padH + self.edge_delta[3, 0] + H, padW + self.edge_delta[3, 1]:padW + self.edge_delta[3, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[4, 0]:padH + self.edge_delta[4, 0] + H, padW + self.edge_delta[4, 1]:padW + self.edge_delta[4, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[5, 0]:padH + self.edge_delta[5, 0] + H, padW + self.edge_delta[5, 1]:padW + self.edge_delta[5, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[6, 0]:padH + self.edge_delta[6, 0] + H, padW + self.edge_delta[6, 1]:padW + self.edge_delta[6, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[7, 0]:padH + self.edge_delta[7, 0] + H, padW + self.edge_delta[7, 1]:padW + self.edge_delta[7, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[8, 0]:padH + self.edge_delta[8, 0] + H, padW + self.edge_delta[8, 1]:padW + self.edge_delta[8, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[9, 0]:padH + self.edge_delta[9, 0] + H, padW + self.edge_delta[9, 1]:padW + self.edge_delta[9, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[10, 0]:padH + self.edge_delta[10, 0] + H, padW + self.edge_delta[10, 1]:padW + self.edge_delta[10, 1] + W],\n",
    "            img_features_frame[:, :, padH + self.edge_delta[11, 0]:padH + self.edge_delta[11, 0] + H, padW + self.edge_delta[11, 1]:padW + self.edge_delta[11, 1] + W]\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[12, 0]:padH + self.edge_delta[12, 0] + H, padW + self.edge_delta[12, 1]:padW + self.edge_delta[12, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[13, 0]:padH + self.edge_delta[13, 0] + H, padW + self.edge_delta[13, 1]:padW + self.edge_delta[13, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[14, 0]:padH + self.edge_delta[14, 0] + H, padW + self.edge_delta[14, 1]:padW + self.edge_delta[14, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[15, 0]:padH + self.edge_delta[15, 0] + H, padW + self.edge_delta[15, 1]:padW + self.edge_delta[15, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[16, 0]:padH + self.edge_delta[16, 0] + H, padW + self.edge_delta[16, 1]:padW + self.edge_delta[16, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[17, 0]:padH + self.edge_delta[17, 0] + H, padW + self.edge_delta[17, 1]:padW + self.edge_delta[17, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[18, 0]:padH + self.edge_delta[18, 0] + H, padW + self.edge_delta[18, 1]:padW + self.edge_delta[18, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[19, 0]:padH + self.edge_delta[19, 0] + H, padW + self.edge_delta[19, 1]:padW + self.edge_delta[19, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[20, 0]:padH + self.edge_delta[20, 0] + H, padW + self.edge_delta[20, 1]:padW + self.edge_delta[20, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[21, 0]:padH + self.edge_delta[21, 0] + H, padW + self.edge_delta[21, 1]:padW + self.edge_delta[21, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[22, 0]:padH + self.edge_delta[22, 0] + H, padW + self.edge_delta[22, 1]:padW + self.edge_delta[22, 1] + W],\n",
    "            # img_features_frame[:, :, padH + self.edge_delta[23, 0]:padH + self.edge_delta[23, 0] + H, padW + self.edge_delta[23, 1]:padW + self.edge_delta[23, 1] + W]\n",
    "        ], axis=-3)\n",
    "        return neighbors_pixels_features\n",
    "    \n",
    "\n",
    "    def normalize_and_transform_features(self, img_features):\n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "        # img_features = img_features.view(batch_size, self.n_graphs, self.n_node_fts, h_size, w_size)\n",
    "        img_features = torch.nn.functional.normalize(img_features, dim=2)\n",
    "\n",
    "        img_features_transform = torch.einsum(\n",
    "            \"bhcHW, hc -> bhcHW\", img_features, self.multiM\n",
    "        )\n",
    "    \n",
    "        img_features_transform = img_features_transform.view(batch_size, self.n_graphs*self.n_node_fts, h_size, w_size)\n",
    "\n",
    "        return img_features_transform\n",
    "\n",
    "\n",
    "    def extract_edge_weights(self, img_features):\n",
    "        \n",
    "        batch_size, n_graphs, n_node_fts, h_size, w_size = img_features.shape\n",
    "\n",
    "        img_features = self.normalize_and_transform_features(img_features)\n",
    "        img_features_neighbors = self.get_neighbors_pixels(img_features)\n",
    "\n",
    "        features_similarity = (img_features[:, :, None, :, :] * img_features_neighbors)\n",
    "        features_similarity = features_similarity.view(\n",
    "            batch_size, self.n_graphs, self.n_node_fts, self.n_edges, h_size, w_size\n",
    "        ).sum(axis=2)\n",
    "\n",
    "        edge_weights_norm = nn.functional.softmax(features_similarity, dim=2) \n",
    "        node_degree = edge_weights_norm.sum(axis=2)\n",
    "\n",
    "        return edge_weights_norm, node_degree\n",
    "\n",
    "\n",
    "    def stats_conv(self, patchs):\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.view(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_patch = nn.functional.pad(temp_patch, (1,1,1,1), 'reflect')\n",
    "        temp_out_patch = nn.functional.conv2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "    def stats_conv_transpose(self, patchs):\n",
    "\n",
    "        stats_kernel = (\n",
    "            self.stats_kernel_p01 * self.stats_kernel01\n",
    "            + self.stats_kernel_p02a * self.stats_kernel02a\n",
    "            + self.stats_kernel_p02b * self.stats_kernel02b\n",
    "            + self.stats_kernel_p03 * self.stats_kernel03\n",
    "        )\n",
    "        \n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape\n",
    "        temp_patch = patchs.reshape(batch_size*n_graphs, c_size, h_size, w_size)\n",
    "        temp_out_patch = nn.functional.conv_transpose2d(\n",
    "            temp_patch,\n",
    "            weight=stats_kernel,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=self.n_channels,\n",
    "        )\n",
    "        out_patch = temp_out_patch.view(batch_size, n_graphs, c_size, h_size, w_size)\n",
    "        return out_patch\n",
    "\n",
    "\n",
    "    def op_C(self, img_signals, edge_weights, node_degree):\n",
    "\n",
    "\n",
    "        batch_size, n_graphs, n_channels, h_size, w_size = img_signals.shape\n",
    "        # img_signals = self.stats_conv(img_signals)\n",
    "\n",
    "        img_signals = self.stats_conv(img_signals)\n",
    "\n",
    "        img_features_neighbors = self.get_neighbors_pixels(\n",
    "            img_signals.view(batch_size, n_graphs*n_channels, h_size, w_size)\n",
    "        ).view(batch_size, n_graphs, n_channels, self.n_edges, h_size, w_size)\n",
    "        Cx1 = img_signals[:, :, :, None, :, :] * edge_weights[:, :, None, :, :, :]\n",
    "        Cx2 = img_features_neighbors * edge_weights[:, :, None, :, :, :]\n",
    "        \n",
    "        output = Cx1 - Cx2\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def op_C_transpose(self, edge_signals, edge_weights, node_degree):\n",
    "\n",
    "        batch_size, n_graphs, n_channels, n_edges, H, W = edge_signals.shape\n",
    "        edge_signals = edge_signals * edge_weights[:, :, None, :, :, :]\n",
    "\n",
    "        output = edge_signals.sum(axis=3)\n",
    "\n",
    "        padH, padW = self.pad_dim_hw\n",
    "        output = nn.functional.pad(\n",
    "            output.view(batch_size, n_graphs*n_channels, H, W),\n",
    "            (padW, padW, padH, padH), \"replicate\"\n",
    "        ).view(batch_size, n_graphs, n_channels, H + 2*padH, W + 2*padW)\n",
    "\n",
    "        i=0\n",
    "        for shift_h, shift_w in self.edge_delta:\n",
    "            fromh = padH + shift_h\n",
    "            toh = padH + shift_h + H\n",
    "            fromw = padW + shift_w\n",
    "            tow = padW + shift_w + W\n",
    "            \n",
    "            output[:, :, :, fromh:toh, fromw:tow] = output[:, :, :, fromh:toh, fromw:tow] - edge_signals[:, :, :, i, :, :]\n",
    "            i+=1\n",
    "\n",
    "        output = output[:, :, :, padH:-padH, padW:-padW]\n",
    "\n",
    "        output = self.stats_conv_transpose(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, patchs, edge_weights, node_degree):\n",
    "        # C^T C\n",
    "        edges_signals = self.op_C(patchs, edge_weights, node_degree)\n",
    "        output_patchs = self.op_C_transpose(edges_signals, edge_weights, node_degree)\n",
    "\n",
    "        return output_patchs\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "class DCestimator(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, hidden_features):\n",
    "        super(DCestimator, self).__init__()\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim_in, hidden_features*2, kernel_size=1, bias=False)\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=False)\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim_out, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, patchs):\n",
    "\n",
    "        out = self.project_in(patchs)\n",
    "        out01, out02 = self.dwconv(out).chunk(2, dim=1)\n",
    "        out = nn.functional.gelu(out01) * out02\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MixtureGTV(nn.Module):\n",
    "    def __init__(self, \n",
    "            nchannels_in,\n",
    "            n_graphs,\n",
    "            n_node_fts,\n",
    "            n_cnn_fts,\n",
    "            connection_window,\n",
    "            n_cgd_iters,\n",
    "            alpha_init,\n",
    "            beta_init,\n",
    "            muy_init, ro_init, gamma_init,\n",
    "            device\n",
    "        ):\n",
    "        super(MixtureGTV, self).__init__()\n",
    "\n",
    "        self.device       = device\n",
    "        self.n_graphs     = n_graphs\n",
    "        self.n_node_fts   = n_node_fts\n",
    "        self.n_total_fts  = n_graphs * n_node_fts\n",
    "        self.n_cnn_fts    = n_cnn_fts\n",
    "        self.n_levels     = 4\n",
    "        self.n_cgd_iters  = n_cgd_iters\n",
    "        self.nchannels_in = nchannels_in\n",
    "        self.connection_window = connection_window\n",
    "\n",
    "        self.alphaCGD =  Parameter(\n",
    "            torch.ones((self.n_cgd_iters, n_graphs), device=self.device, dtype=torch.float32) * alpha_init,\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.betaCGD =  Parameter(\n",
    "            torch.ones((self.n_cgd_iters, n_graphs), device=self.device, dtype=torch.float32) * beta_init,\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.patchs_features_extraction = FeatureExtraction(\n",
    "            inp_channels=3, \n",
    "            out_channels=self.n_total_fts + 12, \n",
    "            dim=self.n_cnn_fts,\n",
    "            num_blocks = [2, 3, 3, 4], \n",
    "            num_refinement_blocks = 4,\n",
    "            ffn_expansion_factor = 2.6666,\n",
    "            bias = False,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.combination_weight = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.n_total_fts, \n",
    "                out_channels=self.n_graphs, \n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                padding_mode=\"zeros\",\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Softmax(dim=1)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.dc_estimator = DCestimator(12, 3, 12*2).to(self.device)\n",
    "\n",
    "        self.ro00 = Parameter(\n",
    "            torch.ones((n_graphs), device=self.device, dtype=torch.float32) * ro_init[0],\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.gamma00 = Parameter(\n",
    "            torch.ones((n_graphs), device=self.device, dtype=torch.float32) * torch.log(gamma_init[0]),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.GTVmodule00 = GTVFast(\n",
    "            n_channels=self.nchannels_in,\n",
    "            n_node_fts=self.n_node_fts,\n",
    "            n_graphs=self.n_graphs,\n",
    "            connection_window=self.connection_window,\n",
    "            device=self.device,\n",
    "            M_diag_init=1.0\n",
    "        )\n",
    "\n",
    "        self.muys00 = Parameter(\n",
    "            torch.ones((n_graphs), device=self.device, dtype=torch.float32) * muy_init[0],\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.GLRmodule00 = GLRFast(\n",
    "            n_channels=self.nchannels_in,\n",
    "            n_node_fts=self.n_node_fts,\n",
    "            n_graphs=self.n_graphs,\n",
    "            connection_window=self.connection_window,\n",
    "            device=self.device,\n",
    "            M_diag_init=1.0\n",
    "        )\n",
    "\n",
    "    def apply_lightweight_transformer(self, patchs, list_graph_weightGTV, list_graph_weightGLR):\n",
    "\n",
    "        batch_size, n_graphs, c_size, h_size, w_size = patchs.shape \n",
    "        patchs = patchs.contiguous()\n",
    "        graph_weights, graph_degree = list_graph_weightGLR[0]\n",
    "\n",
    "        Lpatchs = self.GLRmodule00(patchs, graph_weights, graph_degree)\n",
    "        Lpatchs = torch.einsum(\n",
    "            \"bHchw, H -> bHchw\", Lpatchs, self.muys00\n",
    "        )\n",
    "\n",
    "        graph_weights, graph_degree = list_graph_weightGTV[0]\n",
    "        CtCpatchs = self.GTVmodule00(patchs, graph_weights, graph_degree)\n",
    "        CtCpatchs = torch.einsum(\n",
    "            \"bHchw, H -> bHchw\", CtCpatchs, self.ro00\n",
    "        )\n",
    "\n",
    "        output = patchs + Lpatchs + CtCpatchs\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def soft_threshold(self, delta, gamma):\n",
    "        # batch_size, n_graphs, n_channels, n_edges, H, W = delta.shape\n",
    "        # n_graphs = gamma.shape\n",
    "\n",
    "        gamma = gamma[None, :, None, None, None, None]\n",
    "        # print(f\"Gamma.shape={gamma.shape}\")\n",
    "\n",
    "        condA = (delta < -gamma) \n",
    "        outputA = torch.where(\n",
    "            condA,\n",
    "            delta+gamma,\n",
    "            0.0\n",
    "        )\n",
    "        condB = (delta > gamma) \n",
    "        outputB = torch.where(\n",
    "            condB,\n",
    "            delta-gamma,\n",
    "            0.0\n",
    "        )\n",
    "        output = outputA + outputB\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, patchs):\n",
    "        # with record_function(\"MultiScaleMixtureGLR:forward\"): \n",
    "        # print(\"#\"*80)\n",
    "        # patchs = patchs.permute(dims=(0, 3, 1, 2))\n",
    "        # patchs = patchs.contiguous()\n",
    "        # patchs = self.images_domain_to_abtract_domain(patchs)\n",
    "        # print(f\"patchs.shape={patchs.shape}\")\n",
    "        batch_size, c_size, h_size, w_size = patchs.shape\n",
    "\n",
    "        #####\n",
    "        ## Graph low pass filter\n",
    "        \n",
    "        list_features_patchs = self.patchs_features_extraction(patchs)\n",
    "        list_graph_weightGTV = [None] * self.n_levels\n",
    "        list_graph_weightGLR = [None] * self.n_levels\n",
    "        bz, nfts, h, w = list_features_patchs[0].shape\n",
    "\n",
    "        list_graph_weightGTV[0] = self.GTVmodule00.extract_edge_weights(\n",
    "            list_features_patchs[0][:, :-12, :, :].view((bz, self.GTVmodule00.n_graphs, self.GTVmodule00.n_node_fts, h, w))\n",
    "        )\n",
    "        list_graph_weightGLR[0] = self.GLRmodule00.extract_edge_weights(\n",
    "            list_features_patchs[0][:, :-12, :, :].view((bz, self.GLRmodule00.n_graphs, self.GLRmodule00.n_node_fts, h, w))\n",
    "        )\n",
    "\n",
    "\n",
    "        dc_term = self.dc_estimator(list_features_patchs[0][:, -12:, :, :])\n",
    "        y_tilde = patchs - dc_term\n",
    "        ###########################################################\n",
    "\n",
    "\n",
    "        epsilon = self.GTVmodule00.op_C(y_tilde[:, None, :, :, :], list_graph_weightGTV[0][0], list_graph_weightGTV[0][1])\n",
    "        bias    = torch.zeros_like(epsilon)\n",
    "\n",
    "        left_hand_size = self.GTVmodule00.op_C_transpose(epsilon - bias, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) \n",
    "        left_hand_size *= self.ro00[None, :, None, None, None]\n",
    "        left_hand_size += y_tilde[:, None, :, :, :]\n",
    "        ############################################################\n",
    "        output = left_hand_size\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual\n",
    "        output = output + self.alphaCGD[0, None, :, None, None, None] * update\n",
    "\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual + self.betaCGD[1, None, :, None, None, None] * update\n",
    "        output = output + self.alphaCGD[1, None, :, None, None, None] * update\n",
    "\n",
    "        epsilon = self.soft_threshold(\n",
    "            self.GTVmodule00.op_C(output, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) + bias,\n",
    "            torch.exp(self.gamma00)\n",
    "        )\n",
    "        bias  = bias + (self.GTVmodule00.op_C(output, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) - epsilon)\n",
    "\n",
    "        left_hand_size = self.GTVmodule00.op_C_transpose(epsilon - bias, list_graph_weightGTV[0][0], list_graph_weightGTV[0][1]) \n",
    "        left_hand_size *= self.ro00[None, :, None, None, None]\n",
    "        left_hand_size += y_tilde[:, None, :, :, :]\n",
    "        ############################################################\n",
    "\n",
    "        output = left_hand_size\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual\n",
    "        output = output + self.alphaCGD[2, None, :, None, None, None] * update\n",
    "\n",
    "        system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        update = system_residual + self.betaCGD[3, None, :, None, None, None] * update\n",
    "        output = output + self.alphaCGD[3, None, :, None, None, None] * update\n",
    "\n",
    "        # system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        # update = system_residual + self.betaCGD[4, None, :, None, None, None] * update\n",
    "        # output = output + self.alphaCGD[4, None, :, None, None, None] * update\n",
    "        \n",
    "        # system_residual = left_hand_size -  self.apply_lightweight_transformer(output, list_graph_weightGTV, list_graph_weightGLR)\n",
    "        # update = system_residual + self.betaCGD[5, None, :, None, None, None] * update\n",
    "        # output = output + self.alphaCGD[5, None, :, None, None, None] * update\n",
    "        \n",
    "\n",
    "        score = self.combination_weight(list_features_patchs[0][:, :-12, :, :])\n",
    "        output = torch.einsum(\n",
    "            \"bgchw, bghw -> bchw\", output, score\n",
    "        ) + dc_term\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "class SharpeningBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, hidden_features):\n",
    "        super(SharpeningBlock, self).__init__()\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim_in, hidden_features*2, kernel_size=1, bias=False)\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=False)\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim_out, kernel_size=1, bias=False)\n",
    "        self.skip_connect_weight = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32) * torch.tensor([0.5, 0.5]),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, patchs):\n",
    "\n",
    "        out = self.project_in(patchs)\n",
    "        out01, out02 = self.dwconv(out).chunk(2, dim=1)\n",
    "        out = nn.functional.gelu(out01) * out02\n",
    "        out = self.project_out(out)\n",
    "        out = self.skip_connect_weight[0] * patchs + self.skip_connect_weight[1] * out\n",
    "        return out\n",
    "\n",
    "class MultiScaleSequenceDenoiser(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(MultiScaleSequenceDenoiser, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CONNECTION_FLAGS_5x5 = np.array([\n",
    "        #     1,1,1,1,1,\n",
    "        #     1,1,1,1,1,\n",
    "        #     1,1,0,1,1,\n",
    "        #     1,1,1,1,1,\n",
    "        #     1,1,1,1,1,\n",
    "        # ]).reshape((5,5))\n",
    "        CONNECTION_FLAGS_5x5_small = np.array([\n",
    "            0,0,1,0,0,\n",
    "            0,1,1,1,0,\n",
    "            1,1,0,1,1,\n",
    "            0,1,1,1,0,\n",
    "            0,0,1,0,0,\n",
    "        ]).reshape((5,5))\n",
    "        self.skip_connect_weight02 = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32, device=self.device) * torch.tensor([0.1, 0.9]).to(device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.mixtureGLR_block02 = MixtureGTV(\n",
    "            nchannels_in=3,\n",
    "            n_graphs=16,\n",
    "            n_node_fts=3,\n",
    "            n_cnn_fts=48,\n",
    "            connection_window=CONNECTION_FLAGS_5x5_small,\n",
    "            n_cgd_iters=4,\n",
    "            alpha_init=0.5,\n",
    "            beta_init=0.1,\n",
    "            muy_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            ro_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            gamma_init=torch.tensor([[0.001], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.skip_connect_weight03 = Parameter(\n",
    "            torch.ones((2), dtype=torch.float32, device=self.device) * torch.tensor([0.5, 0.5]).to(device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.mixtureGLR_block03 = MixtureGTV(\n",
    "            nchannels_in=3,\n",
    "            n_graphs=16,\n",
    "            n_node_fts=3,\n",
    "            n_cnn_fts=48,\n",
    "            connection_window=CONNECTION_FLAGS_5x5_small,\n",
    "            n_cgd_iters=4,\n",
    "            alpha_init=0.5,\n",
    "            beta_init=0.1,\n",
    "            muy_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            ro_init=torch.tensor([[0.1], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            gamma_init=torch.tensor([[0.001], [0.0], [0.0], [0.0]]).to(self.device),\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def forward(self, patchs):\n",
    "        \n",
    "        output = self.skip_connect_weight02[0] * patchs + self.skip_connect_weight02[1] * self.mixtureGLR_block03(patchs)\n",
    "        output = self.skip_connect_weight03[0] * output + self.skip_connect_weight03[1] * self.mixtureGLR_block03(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30bccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "VERBOSE_RATE = 1000\n",
    "\n",
    "(H_train01, W_train01) = (64, 64)\n",
    "(H_train02, W_train02) = (128, 128)\n",
    "(H_train03, W_train03) = (256, 256)\n",
    "(H_train04, W_train04) = (512, 512)\n",
    "\n",
    "(H_val, W_val) = (128, 128)\n",
    "(H_test, W_test) = (496, 496)\n",
    "\n",
    "train_dataset01 = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"vary_addictive_noise\",\n",
    "    lambda_noise=[[1.0, 10.0, 15.0, 20.0, 25.0], [0.1, 0.1, 0.1, 0.1, 0.6]],\n",
    "    use_data_aug=True,\n",
    "    patch_size=(H_train01,H_train01),\n",
    "    patch_overlap_size=(H_train01//4,H_train01//4),\n",
    "    max_num_patchs=3200000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset01, batch_size=16, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eca785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 1951844\n",
      "iter=0 time=1.0051274299621582 psnr=26.239173782617534 mse=0.0023772925082516183\n",
      "iter=1 time=0.566617488861084 psnr=26.69866545149302 mse=0.002150600226300952\n",
      "iter=2 time=0.5650675296783447 psnr=26.765420934834214 mse=0.00211448020051078\n",
      "iter=3 time=0.5655529499053955 psnr=26.64339318672556 mse=0.0021749872688009418\n",
      "iter=4 time=0.5646185874938965 psnr=26.50952083318059 mse=0.0022453800586613865\n",
      "iter=5 time=0.5650131702423096 psnr=26.366323327174403 mse=0.0023248985149751113\n",
      "iter=6 time=0.5675699710845947 psnr=26.239467201627587 mse=0.0023974102518003726\n",
      "iter=7 time=0.5648601055145264 psnr=26.15526484740804 mse=0.0024447305357977183\n",
      "iter=8 time=0.564903974533081 psnr=26.153788018718394 mse=0.0024432159309070783\n",
      "iter=9 time=0.5648205280303955 psnr=26.15587111698976 mse=0.0024401836580156947\n",
      "iter=10 time=0.5658564567565918 psnr=26.149005140768228 mse=0.002442516346370907\n",
      "iter=11 time=0.5651144981384277 psnr=26.150208900725374 mse=0.0024405655191493477\n",
      "iter=12 time=0.5650787353515625 psnr=26.194924969104246 mse=0.002416099137039433\n",
      "iter=13 time=0.5648431777954102 psnr=26.23765350262763 mse=0.0023929925153506155\n",
      "iter=14 time=0.5644938945770264 psnr=26.275598042718492 mse=0.0023725274797809963\n",
      "iter=15 time=0.5650649070739746 psnr=26.332777069690533 mse=0.002343597630527534\n",
      "iter=16 time=0.5648653507232666 psnr=26.383717166195563 mse=0.0023178569116647616\n",
      "iter=17 time=0.56451416015625 psnr=26.4296804375767 mse=0.0022946768644008294\n",
      "iter=18 time=0.5647897720336914 psnr=26.460633388592544 mse=0.00227848899993426\n",
      "iter=19 time=0.5661828517913818 psnr=26.488310867092462 mse=0.002264002396013043\n",
      "iter=20 time=0.5649688243865967 psnr=26.47244036718677 mse=0.002271611854083432\n",
      "iter=21 time=0.5666189193725586 psnr=26.451263163423775 mse=0.0022823615681538474\n",
      "iter=22 time=0.564777135848999 psnr=26.424371167308276 mse=0.0022966288686804203\n",
      "iter=23 time=0.5648157596588135 psnr=26.400558298963006 mse=0.002309204600285722\n",
      "iter=24 time=0.5663361549377441 psnr=26.408178517518834 mse=0.0023045271320541656\n",
      "iter=25 time=0.5648372173309326 psnr=26.400526661488463 mse=0.0023079583955494362\n",
      "iter=26 time=0.5649294853210449 psnr=26.42180407741436 mse=0.0022968033914491375\n",
      "iter=27 time=0.5649831295013428 psnr=26.431457157883294 mse=0.0022912696789301637\n",
      "iter=28 time=0.5667097568511963 psnr=26.41659861975706 mse=0.002298865994027\n",
      "iter=29 time=0.5649685859680176 psnr=26.406761125532828 mse=0.002303657177790762\n",
      "iter=30 time=0.5648837089538574 psnr=26.401626642965397 mse=0.002305884004873447\n",
      "iter=31 time=0.5649392604827881 psnr=26.393386945197108 mse=0.002309867280197902\n",
      "iter=32 time=0.5645980834960938 psnr=26.367605921840898 mse=0.0023244429401083947\n",
      "iter=33 time=0.5662307739257812 psnr=26.346795760662438 mse=0.0023359711319812363\n",
      "iter=34 time=0.56485915184021 psnr=26.330943782277398 mse=0.0023445185807330123\n",
      "iter=35 time=0.565216064453125 psnr=26.315280323937422 mse=0.002353012345502986\n",
      "iter=36 time=0.564943790435791 psnr=26.30210648400864 mse=0.0023600513603629935\n",
      "iter=37 time=0.5646588802337646 psnr=26.30519281623035 mse=0.002357962216789225\n",
      "iter=38 time=0.5664892196655273 psnr=26.314768762238838 mse=0.0023525912855504824\n",
      "iter=39 time=0.5647237300872803 psnr=26.314565773703606 mse=0.00235229256476484\n",
      "iter=40 time=0.5652024745941162 psnr=26.318460457419626 mse=0.0023498472688773513\n",
      "iter=41 time=0.5654418468475342 psnr=26.331075919547256 mse=0.002343093383694084\n",
      "iter=42 time=0.5647544860839844 psnr=26.334485954076467 mse=0.002340934053188368\n",
      "iter=43 time=0.5649902820587158 psnr=26.346072563944333 mse=0.002334733147925731\n",
      "iter=44 time=0.5647528171539307 psnr=26.337323880261923 mse=0.0023392852900540185\n",
      "iter=45 time=0.5647175312042236 psnr=26.332595394781357 mse=0.0023415515980225953\n",
      "iter=46 time=0.5666649341583252 psnr=26.33372928356238 mse=0.0023406318541359028\n",
      "iter=47 time=0.5649323463439941 psnr=26.321303619817726 mse=0.0023474628309991794\n",
      "iter=48 time=0.5652451515197754 psnr=26.310327679270188 mse=0.002353438910047639\n",
      "iter=49 time=0.5649018287658691 psnr=26.30679604732768 mse=0.0023550843108292637\n",
      "iter=50 time=0.5650622844696045 psnr=26.29833527716927 mse=0.0023595936025204273\n",
      "iter=51 time=0.5651416778564453 psnr=26.297183926989113 mse=0.002359941427175937\n",
      "iter=52 time=0.56461501121521 psnr=26.3123639566636 mse=0.0023521891372090837\n",
      "iter=53 time=0.564612627029419 psnr=26.31795930099063 mse=0.002349009025731183\n",
      "iter=54 time=0.5650622844696045 psnr=26.3210828781128 mse=0.00234709991568962\n",
      "iter=55 time=0.5667431354522705 psnr=26.330657071219928 mse=0.0023420077918972215\n",
      "iter=56 time=0.5645911693572998 psnr=26.32790574384405 mse=0.002343259073066015\n",
      "iter=57 time=0.5647263526916504 psnr=26.34243751131058 mse=0.002335932851062733\n",
      "iter=58 time=0.5651726722717285 psnr=26.349319154714557 mse=0.002332175367197504\n",
      "iter=59 time=0.5647456645965576 psnr=26.356867296790092 mse=0.002328109683316981\n",
      "iter=60 time=0.564631462097168 psnr=26.36946922828469 mse=0.0023217208600166896\n",
      "iter=61 time=0.5648548603057861 psnr=26.384276813602845 mse=0.002314393707146719\n",
      "iter=62 time=0.5652716159820557 psnr=26.402450150507196 mse=0.00230569482880011\n",
      "iter=63 time=0.5647742748260498 psnr=26.41774621776282 mse=0.0022982232166536554\n",
      "iter=64 time=0.5648279190063477 psnr=26.421678514294978 mse=0.002295960152167777\n",
      "iter=65 time=0.5646412372589111 psnr=26.42895671539479 mse=0.002292093774253956\n",
      "iter=66 time=0.5649566650390625 psnr=26.44083926420758 mse=0.0022861593201721642\n",
      "iter=67 time=0.5661337375640869 psnr=26.44614369548239 mse=0.0022832533760163837\n",
      "iter=68 time=0.5645060539245605 psnr=26.45950483680288 mse=0.0022767298788296496\n",
      "iter=69 time=0.5646171569824219 psnr=26.470920020171928 mse=0.0022710614052487217\n",
      "iter=70 time=0.5648419857025146 psnr=26.48625330522631 mse=0.002263779537696652\n",
      "iter=71 time=0.5652012825012207 psnr=26.50161774840374 mse=0.0022565160332475237\n",
      "iter=72 time=0.5644478797912598 psnr=26.500416938494606 mse=0.0022568859014638534\n",
      "iter=73 time=0.5648720264434814 psnr=26.49880288548767 mse=0.0022574810126457165\n",
      "iter=74 time=0.5651416778564453 psnr=26.500340093207047 mse=0.0022564569129848687\n",
      "iter=75 time=0.5648539066314697 psnr=26.498841641355888 mse=0.0022570038249960195\n",
      "iter=76 time=0.5645453929901123 psnr=26.50792374520262 mse=0.0022524487959659136\n",
      "iter=77 time=0.5649371147155762 psnr=26.509859100496858 mse=0.0022512417628887857\n",
      "iter=78 time=0.5647649765014648 psnr=26.515410011752863 mse=0.00224830349862508\n",
      "iter=79 time=0.56459641456604 psnr=26.525955170269164 mse=0.0022431616007925976\n",
      "iter=80 time=0.564802885055542 psnr=26.53353174011008 mse=0.0022393215904331736\n",
      "iter=81 time=0.5652227401733398 psnr=26.53931652555624 mse=0.0022363011249138124\n",
      "iter=82 time=0.5663111209869385 psnr=26.54205327116693 mse=0.0022347250975322873\n",
      "iter=83 time=0.5646042823791504 psnr=26.55014600546711 mse=0.0022306914798763656\n",
      "iter=84 time=0.5647993087768555 psnr=26.559999699854068 mse=0.0022259169644612445\n",
      "iter=85 time=0.5650784969329834 psnr=26.567280675870474 mse=0.002222261430952043\n",
      "iter=86 time=0.5649189949035645 psnr=26.57039834910902 mse=0.0022205210440759786\n",
      "iter=87 time=0.5647773742675781 psnr=26.569560930373072 mse=0.0022207471691055927\n",
      "iter=88 time=0.56475830078125 psnr=26.57272850645417 mse=0.002218993590928329\n",
      "iter=89 time=0.5647456645965576 psnr=26.582351311071502 mse=0.0022143772613017094\n",
      "iter=90 time=0.5647983551025391 psnr=26.590347108209635 mse=0.0022104590252441654\n",
      "iter=91 time=0.5649032592773438 psnr=26.600146462530294 mse=0.002205797559823042\n",
      "iter=92 time=0.5648355484008789 psnr=26.594175016815214 mse=0.002208811638748504\n",
      "iter=93 time=0.5648190975189209 psnr=26.59680232799402 mse=0.0022073306312974826\n",
      "iter=94 time=0.5646255016326904 psnr=26.597786511216874 mse=0.0022066507584601706\n",
      "iter=95 time=0.5647778511047363 psnr=26.601032764891425 mse=0.0022048868018383146\n",
      "iter=96 time=0.5650241374969482 psnr=26.599706811447334 mse=0.0022053826687311604\n",
      "iter=97 time=0.5649876594543457 psnr=26.607566687979528 mse=0.0022015759587617483\n",
      "iter=98 time=0.5655605792999268 psnr=26.61353273967748 mse=0.002198592848880973\n",
      "iter=99 time=0.5656628608703613 psnr=26.620557613385532 mse=0.0021951592302389506\n",
      "iter=100 time=0.566882848739624 psnr=26.627869204468357 mse=0.0021914756933525646\n",
      "iter=101 time=0.5657408237457275 psnr=26.633698403858567 mse=0.0021890591819703586\n",
      "iter=102 time=0.5651092529296875 psnr=26.64051382977102 mse=0.0021860930946990694\n",
      "iter=103 time=0.5650227069854736 psnr=26.659333354205287 mse=0.002177806265212904\n",
      "iter=104 time=0.564887285232544 psnr=26.666614403566406 mse=0.0021739058595795563\n",
      "iter=105 time=0.5652186870574951 psnr=26.6783897154744 mse=0.0021674402801085705\n",
      "iter=106 time=0.5650362968444824 psnr=26.686774015781467 mse=0.0021624674663971253\n",
      "iter=107 time=0.5645074844360352 psnr=26.69694313274176 mse=0.0021566723763055025\n",
      "iter=108 time=0.5646517276763916 psnr=26.697811154209376 mse=0.002156191297817501\n",
      "iter=109 time=0.5654544830322266 psnr=26.700738638096137 mse=0.0021546184268275382\n",
      "iter=110 time=0.5652065277099609 psnr=26.708276283553026 mse=0.002150689503150132\n",
      "iter=111 time=0.5646109580993652 psnr=26.715972783219897 mse=0.002146760796626623\n",
      "iter=112 time=0.5645811557769775 psnr=26.718362661939317 mse=0.002145624360126966\n",
      "iter=113 time=0.5649118423461914 psnr=26.715205033467317 mse=0.0021472025093707122\n",
      "iter=114 time=0.5648963451385498 psnr=26.712679533738484 mse=0.002148451529806343\n",
      "iter=115 time=0.5645184516906738 psnr=26.70483083102621 mse=0.002152234227989608\n",
      "iter=116 time=0.5644538402557373 psnr=26.694981170976142 mse=0.0021570864396886\n",
      "iter=117 time=0.5647153854370117 psnr=26.687825999128762 mse=0.002160490498737096\n",
      "iter=118 time=0.5646884441375732 psnr=26.681670420178474 mse=0.0021635163481410653\n",
      "iter=119 time=0.5643501281738281 psnr=26.676330039968487 mse=0.0021661185744172888\n",
      "iter=120 time=0.5644800662994385 psnr=26.691849047986537 mse=0.002158835919095769\n",
      "iter=121 time=0.5649430751800537 psnr=26.704033171526216 mse=0.002152700298230099\n",
      "iter=122 time=0.5665557384490967 psnr=26.725004699557438 mse=0.0021427020304410835\n",
      "iter=123 time=0.5645887851715088 psnr=26.739298489046483 mse=0.002135414735980637\n",
      "iter=124 time=0.5647711753845215 psnr=26.746518476749447 mse=0.002132057005444749\n",
      "iter=125 time=0.565385103225708 psnr=26.75884637320279 mse=0.002126141399128267\n",
      "iter=126 time=0.5657248497009277 psnr=26.755109995997824 mse=0.0021279443360384545\n",
      "iter=127 time=0.5650687217712402 psnr=26.76277950001024 mse=0.0021244770425371336\n",
      "iter=128 time=0.5650815963745117 psnr=26.781622997646263 mse=0.002115635967348358\n",
      "iter=129 time=0.5649316310882568 psnr=26.79391966861582 mse=0.0021096128372275706\n",
      "iter=130 time=0.5649476051330566 psnr=26.80633225697728 mse=0.0021037144529303053\n",
      "iter=131 time=0.5646066665649414 psnr=26.81960138545419 mse=0.00209730819037866\n",
      "iter=132 time=0.5647034645080566 psnr=26.835196358400253 mse=0.0020888885227684753\n",
      "iter=133 time=0.5653626918792725 psnr=26.846064475794552 mse=0.0020828746255053786\n",
      "iter=134 time=0.5647983551025391 psnr=26.856460466462032 mse=0.002077264916050192\n",
      "iter=135 time=0.5647270679473877 psnr=26.874531424585562 mse=0.002068243708232677\n",
      "iter=136 time=0.5645809173583984 psnr=26.896689043438286 mse=0.0020577997288928527\n",
      "iter=137 time=0.5648729801177979 psnr=26.91153239483935 mse=0.002051197296575677\n",
      "iter=138 time=0.5645601749420166 psnr=26.919402523990488 mse=0.0020476362675198654\n",
      "iter=139 time=0.5646755695343018 psnr=26.94223131502009 mse=0.002038067009506255\n",
      "iter=140 time=0.5649361610412598 psnr=26.94660392651067 mse=0.0020359099994812324\n",
      "iter=141 time=0.5651326179504395 psnr=26.955838094609685 mse=0.0020319524220776554\n",
      "iter=142 time=0.5648424625396729 psnr=26.968136787964124 mse=0.002026402835003952\n",
      "iter=143 time=0.5645108222961426 psnr=26.974424674920883 mse=0.0020236152343267393\n",
      "iter=144 time=0.5647726058959961 psnr=26.984614006613086 mse=0.0020183042847563212\n",
      "iter=145 time=0.5654528141021729 psnr=26.988946243511855 mse=0.0020159844054213672\n",
      "iter=146 time=0.5652568340301514 psnr=27.00325697694708 mse=0.002009532348845819\n",
      "iter=147 time=0.5653729438781738 psnr=27.014088291778247 mse=0.002003642074485076\n",
      "iter=148 time=0.5671975612640381 psnr=27.034653189128477 mse=0.0019936829896756167\n",
      "iter=149 time=0.5652987957000732 psnr=27.05508024728591 mse=0.001984543803137438\n",
      "iter=150 time=0.5652370452880859 psnr=27.074878173610283 mse=0.0019750799045479763\n",
      "iter=151 time=0.5649166107177734 psnr=27.09118210523538 mse=0.001967637913074542\n",
      "iter=152 time=0.5651557445526123 psnr=27.11032065811669 mse=0.0019606913822028844\n",
      "iter=153 time=0.5650730133056641 psnr=27.129191414092602 mse=0.0019530069619875662\n",
      "iter=154 time=0.5651545524597168 psnr=27.144690639807685 mse=0.0019462716724363636\n",
      "iter=155 time=0.5649771690368652 psnr=27.170874508581495 mse=0.0019369355991776336\n",
      "iter=156 time=0.5647485256195068 psnr=27.190002116632687 mse=0.0019283383470074511\n",
      "iter=157 time=0.5646202564239502 psnr=27.199470995078354 mse=0.0019245803634474813\n",
      "iter=158 time=0.5651447772979736 psnr=27.208562999625798 mse=0.001920586780898929\n",
      "iter=159 time=0.5646071434020996 psnr=27.22380332400487 mse=0.0019144064229942857\n",
      "iter=160 time=0.5647518634796143 psnr=27.235121324790413 mse=0.0019099594430041424\n",
      "iter=161 time=0.56494140625 psnr=27.242359316044155 mse=0.0019070926763825832\n",
      "iter=162 time=0.5651483535766602 psnr=27.254615052686795 mse=0.0019027496063686628\n",
      "iter=163 time=0.5649571418762207 psnr=27.269135244125604 mse=0.0018975560107760195\n",
      "iter=164 time=0.5648846626281738 psnr=27.29067681137836 mse=0.0018891441214928797\n",
      "iter=165 time=0.5648002624511719 psnr=27.302963580428685 mse=0.0018841153376761338\n",
      "iter=166 time=0.5651750564575195 psnr=27.320902699910445 mse=0.0018777047833476938\n",
      "iter=167 time=0.5648500919342041 psnr=27.344461773941426 mse=0.0018689602529007676\n",
      "iter=168 time=0.5650525093078613 psnr=27.35910039752296 mse=0.0018637149503857907\n",
      "iter=169 time=0.5649621486663818 psnr=27.37057328768873 mse=0.0018593505014344708\n",
      "iter=170 time=0.5645618438720703 psnr=27.378282269648167 mse=0.0018564976477721251\n",
      "iter=171 time=0.5649278163909912 psnr=27.384645597449403 mse=0.001854125059174852\n",
      "iter=172 time=0.565392017364502 psnr=27.400521092263116 mse=0.0018471333243142416\n",
      "iter=173 time=0.5652804374694824 psnr=27.424128138011337 mse=0.0018374848638865512\n",
      "iter=174 time=0.5647108554840088 psnr=27.438549034952302 mse=0.0018313233488536438\n",
      "iter=175 time=0.5649030208587646 psnr=27.462882185744128 mse=0.0018214658119815568\n",
      "iter=176 time=0.5644943714141846 psnr=27.481561733028602 mse=0.0018148021890065063\n",
      "iter=177 time=0.5649456977844238 psnr=27.504777705788438 mse=0.001805865156246688\n",
      "iter=178 time=0.5651490688323975 psnr=27.527135269221603 mse=0.0017977406046257582\n",
      "iter=179 time=0.5664498805999756 psnr=27.54110264826716 mse=0.001792688625769736\n",
      "iter=180 time=0.5651540756225586 psnr=27.56112760214113 mse=0.0017855512733408988\n",
      "iter=181 time=0.5649614334106445 psnr=27.579105092871863 mse=0.001778800318566991\n",
      "iter=182 time=0.5652506351470947 psnr=27.59472065626575 mse=0.0017724413620489815\n",
      "iter=183 time=0.5648360252380371 psnr=27.61110606199153 mse=0.0017664828975573934\n",
      "iter=184 time=0.5648653507232666 psnr=27.61983266572405 mse=0.0017631609918352376\n",
      "iter=185 time=0.5649724006652832 psnr=27.637847549198394 mse=0.0017566706802758694\n",
      "iter=186 time=0.5660970211029053 psnr=27.660382885471726 mse=0.0017482874263091911\n",
      "iter=187 time=0.5652468204498291 psnr=27.683995766734288 mse=0.0017388909295929361\n",
      "iter=188 time=0.5649158954620361 psnr=27.695502039090066 mse=0.0017340854037290012\n",
      "iter=189 time=0.5650088787078857 psnr=27.69982547977219 mse=0.0017323764542760127\n",
      "iter=190 time=0.5652940273284912 psnr=27.70187900133645 mse=0.0017315184470988252\n",
      "iter=191 time=0.5650615692138672 psnr=27.71335318779794 mse=0.0017273818779786562\n",
      "iter=192 time=0.5650651454925537 psnr=27.73820380188316 mse=0.0017165493872005305\n",
      "iter=193 time=0.5649292469024658 psnr=27.755349512989017 mse=0.0017097987435264928\n",
      "iter=194 time=0.5653235912322998 psnr=27.77580974601946 mse=0.0017017485926785556\n",
      "iter=195 time=0.565488338470459 psnr=27.78203903185382 mse=0.0016990262859401094\n",
      "iter=196 time=0.5648901462554932 psnr=27.80368591976719 mse=0.001690182838513361\n",
      "iter=197 time=0.5650203227996826 psnr=27.81890666413857 mse=0.001684765689358587\n",
      "iter=198 time=0.5654935836791992 psnr=27.82968440267611 mse=0.0016805763269114428\n",
      "iter=199 time=0.5651881694793701 psnr=27.835663992175498 mse=0.0016781899991785596\n",
      "iter=200 time=0.5651562213897705 psnr=27.841268196302185 mse=0.0016757579261818705\n",
      "iter=201 time=0.5650317668914795 psnr=27.848525085899517 mse=0.0016731692413305263\n",
      "iter=202 time=0.56508469581604 psnr=27.8521225821301 mse=0.0016717815104088354\n",
      "iter=203 time=0.5652275085449219 psnr=27.846255543248816 mse=0.0016739914164559545\n",
      "iter=204 time=0.5649459362030029 psnr=27.874126965592442 mse=0.001663870227894685\n",
      "iter=205 time=0.5649697780609131 psnr=27.888327065991636 mse=0.0016580805718338137\n",
      "iter=206 time=0.5650737285614014 psnr=27.91253188786547 mse=0.0016481030227161295\n",
      "iter=207 time=0.5650546550750732 psnr=27.925242443597217 mse=0.001642529877690733\n",
      "iter=208 time=0.5651247501373291 psnr=27.942049152995413 mse=0.0016348828280710224\n",
      "iter=209 time=0.5654268264770508 psnr=27.962303603171875 mse=0.001626475539399696\n",
      "iter=210 time=0.5649375915527344 psnr=27.983182318584696 mse=0.0016185634873146207\n",
      "iter=211 time=0.5647847652435303 psnr=28.003194086287223 mse=0.0016110823493456913\n",
      "iter=212 time=0.5653140544891357 psnr=28.02743080818284 mse=0.0016024906990046437\n",
      "iter=213 time=0.5649557113647461 psnr=28.05075698367218 mse=0.0015931387969212898\n",
      "iter=214 time=0.5650227069854736 psnr=28.060596605861942 mse=0.0015886565340353393\n",
      "iter=215 time=0.5649757385253906 psnr=28.095912746031345 mse=0.0015759229752975363\n",
      "iter=216 time=0.5654089450836182 psnr=28.121402010987495 mse=0.001565306926057246\n",
      "iter=217 time=0.5651917457580566 psnr=28.143611906287077 mse=0.001556335093912739\n",
      "iter=218 time=0.565039873123169 psnr=28.162678544860793 mse=0.0015481989992490565\n",
      "iter=219 time=0.565345048904419 psnr=28.192316312228826 mse=0.0015370752084908365\n",
      "iter=220 time=0.5655174255371094 psnr=28.21329017076055 mse=0.0015305807355217233\n",
      "iter=221 time=0.5651133060455322 psnr=28.2362094926224 mse=0.00152281194319668\n",
      "iter=222 time=0.5648763179779053 psnr=28.245675532606157 mse=0.0015196574966962152\n",
      "iter=223 time=0.5651001930236816 psnr=28.269839144626673 mse=0.0015116789685481024\n",
      "iter=224 time=0.5648660659790039 psnr=28.27082852601119 mse=0.0015112608151267146\n",
      "iter=225 time=0.5650084018707275 psnr=28.286052959934473 mse=0.0015059317115254612\n",
      "iter=226 time=0.5649027824401855 psnr=28.297745529979625 mse=0.0015007695001951116\n",
      "iter=227 time=0.5652406215667725 psnr=28.304880095904412 mse=0.001498049965581433\n",
      "iter=228 time=0.5649878978729248 psnr=28.320284502976342 mse=0.0014931901256342573\n",
      "iter=229 time=0.5650856494903564 psnr=28.334121130467008 mse=0.0014881691978723631\n",
      "iter=230 time=0.5668368339538574 psnr=28.349194629194812 mse=0.0014829409657712565\n",
      "iter=231 time=0.5651776790618896 psnr=28.357948663634456 mse=0.0014796682986864546\n",
      "iter=232 time=0.5649733543395996 psnr=28.376356168076924 mse=0.0014729353080453287\n",
      "iter=233 time=0.5649487972259521 psnr=28.3951626547393 mse=0.0014655018361896136\n",
      "iter=234 time=0.5650444030761719 psnr=28.410653045020336 mse=0.0014592792885598468\n",
      "iter=235 time=0.5653741359710693 psnr=28.43182088287382 mse=0.001452535127822666\n",
      "iter=236 time=0.5650360584259033 psnr=28.437148620760652 mse=0.0014507236578939562\n",
      "iter=237 time=0.5650162696838379 psnr=28.454955888873577 mse=0.0014452730812740582\n",
      "iter=238 time=0.565082311630249 psnr=28.466821444614013 mse=0.001440987999052187\n",
      "iter=239 time=0.5649642944335938 psnr=28.473786185584657 mse=0.0014389377368376846\n",
      "iter=240 time=0.5647187232971191 psnr=28.4818516684199 mse=0.0014354863655103877\n",
      "iter=241 time=0.565413236618042 psnr=28.493093095025596 mse=0.001431676856150203\n",
      "iter=242 time=0.5655257701873779 psnr=28.513801262236726 mse=0.0014252475323725791\n",
      "iter=243 time=0.5649392604827881 psnr=28.53274355067356 mse=0.0014189224648520308\n",
      "iter=244 time=0.5647311210632324 psnr=28.560471133031434 mse=0.0014094446216921705\n",
      "iter=245 time=0.5649895668029785 psnr=28.58692466456341 mse=0.001399356247539032\n",
      "iter=246 time=0.5653934478759766 psnr=28.598439516291947 mse=0.0013955061211682203\n",
      "iter=247 time=0.5655422210693359 psnr=28.628845862476215 mse=0.0013850362933748223\n",
      "iter=248 time=0.5650177001953125 psnr=28.64650698767408 mse=0.0013795419013861749\n",
      "iter=249 time=0.5649986267089844 psnr=28.662608172142665 mse=0.0013748277143063614\n",
      "iter=250 time=0.5649495124816895 psnr=28.694184830469116 mse=0.001366361018167921\n",
      "iter=251 time=0.5653884410858154 psnr=28.701378446795175 mse=0.0013638675408713418\n",
      "iter=252 time=0.5652177333831787 psnr=28.703682250288942 mse=0.0013632194514320523\n",
      "iter=253 time=0.565223217010498 psnr=28.70829485630268 mse=0.0013617966499431327\n",
      "iter=254 time=0.564950704574585 psnr=28.719307815643116 mse=0.0013582790206673686\n",
      "iter=255 time=0.56504225730896 psnr=28.704186510964757 mse=0.0013629784003642814\n",
      "iter=256 time=0.5649087429046631 psnr=28.72504756615646 mse=0.0013570524956788717\n",
      "iter=257 time=0.565190315246582 psnr=28.75539695992456 mse=0.0013492961628612293\n",
      "iter=258 time=0.5652194023132324 psnr=28.771854417705022 mse=0.001343887111597572\n",
      "iter=259 time=0.565169095993042 psnr=28.7979571769647 mse=0.0013372453739948797\n",
      "iter=260 time=0.565011739730835 psnr=28.818750508840257 mse=0.0013315624109491839\n",
      "iter=261 time=0.5651865005493164 psnr=28.830160996930637 mse=0.001327909979839439\n",
      "iter=262 time=0.5652132034301758 psnr=28.839770217305414 mse=0.0013252659509210664\n",
      "iter=263 time=0.5648648738861084 psnr=28.856426132936623 mse=0.0013210989729920586\n",
      "iter=264 time=0.5649266242980957 psnr=28.87176432045107 mse=0.0013172013184881161\n",
      "iter=265 time=0.565086841583252 psnr=28.898715129134136 mse=0.001310090686706318\n",
      "iter=266 time=0.5650463104248047 psnr=28.913103234822508 mse=0.0013065559003419063\n",
      "iter=267 time=0.5647664070129395 psnr=28.907866302145603 mse=0.0013081118503330028\n",
      "iter=268 time=0.5652503967285156 psnr=28.924770303682283 mse=0.0013038926442241253\n",
      "iter=269 time=0.5649428367614746 psnr=28.955439327293693 mse=0.0012965817289149076\n",
      "iter=270 time=0.5650546550750732 psnr=28.964571193046144 mse=0.001293796457642149\n",
      "iter=271 time=0.5651319026947021 psnr=28.990735848669807 mse=0.0012869923198185703\n",
      "iter=272 time=0.5656230449676514 psnr=29.012761527836773 mse=0.00128068985763583\n",
      "iter=273 time=0.5650856494903564 psnr=29.031976124337522 mse=0.0012759130010129916\n",
      "iter=274 time=0.5652716159820557 psnr=29.04438877395168 mse=0.0012720236618482845\n",
      "iter=275 time=0.5653131008148193 psnr=29.04608629915931 mse=0.0012715206309552504\n",
      "iter=276 time=0.5651354789733887 psnr=29.052330116604672 mse=0.0012698602454787801\n",
      "iter=277 time=0.5653824806213379 psnr=29.067059398138355 mse=0.0012662229177199348\n",
      "iter=278 time=0.5651423931121826 psnr=29.0719525997188 mse=0.0012649371611971514\n",
      "iter=279 time=0.5669631958007812 psnr=29.089566240825903 mse=0.0012604970589064502\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "\n",
    "model = MultiScaleSequenceDenoiser(device=DEVICE)\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "criterian = nn.L1Loss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,\n",
    "    eps=1e-08\n",
    ")\n",
    "lr_scheduler = MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[200000, 500000, 650000], gamma=0.5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "i = 0\n",
    "### TRAINING\n",
    "list_train_mse = []\n",
    "list_train_psnr = []\n",
    "for patchs_noisy, patchs_true in data_train_batched01:\n",
    "    s = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    patchs_noisy = patchs_noisy.to(DEVICE)\n",
    "    patchs_true = patchs_true.to(DEVICE) \n",
    "    reconstruct_patchs = model(patchs_noisy.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "    loss_value = criterian(reconstruct_patchs, patchs_true)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    img_true = np.clip(patchs_true.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    img_recon = np.clip(reconstruct_patchs.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    train_mse_value = np.square(img_true- img_recon).mean()\n",
    "    train_psnr = 10 * np.log10(1/train_mse_value)\n",
    "    list_train_psnr.append(train_psnr)\n",
    "    list_train_mse.append(train_mse_value)\n",
    "\n",
    "    print(f\"iter={i} time={time.time()-s} psnr={np.mean(list_train_psnr[-100:])} mse={np.mean(list_train_mse[-100:])}\")\n",
    "    i+=1\n",
    "\n",
    "    if (i%1000 == 0):\n",
    "\n",
    "        model.eval()\n",
    "        csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "        img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "        paths = img_infos[\"path\"].tolist()\n",
    "        paths = [\n",
    "            os.path.join(ROOT_DATASET,path)\n",
    "            for path in paths\n",
    "        ]\n",
    "\n",
    "        sigma_test = 25.0\n",
    "        factor = 8\n",
    "        list_test_mse = []\n",
    "        random_state = np.random.RandomState(seed=2204)\n",
    "        test_i = 0\n",
    "        s = time.time()\n",
    "        for file_ in paths:\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            img = Image.open(file_)\n",
    "            img_true_255 = np.array(img).astype(np.float32)\n",
    "            img_true = img_true_255 / 255.0\n",
    "\n",
    "            noisy_img_raw = img_true.copy()\n",
    "            noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "            noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "            noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "            h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "            H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "            padh = H-h if h%factor!=0 else 0\n",
    "            padw = W-w if w%factor!=0 else 0\n",
    "            noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "            restored = restored[:,:,:h,:w]\n",
    "            restored = torch.clamp(restored,0,1).cpu().detach().permute(0, 2, 3, 1).squeeze(0).numpy().copy()\n",
    "\n",
    "            restored = img_as_ubyte(restored).astype(np.float32)\n",
    "            test_mse_value = np.square(img_true_255- restored).mean()\n",
    "            list_test_mse.append(test_mse_value)\n",
    "            # print(f\"test_i={test_i} time={time.time()-s} test_i_psnr_value={20 * np.log10(255.0 / np.sqrt(test_mse_value))}\")  \n",
    "            test_i += 1\n",
    "            s = time.time()\n",
    "\n",
    "        psnr_testing = 20 * np.log10(255.0 / np.sqrt(list_test_mse))\n",
    "        print(f\"FINISH TESTING - iter={i} -  psnr_testing={np.mean(psnr_testing)}\")\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ad218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aaafe",
   "metadata": {},
   "source": [
    "# 02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278d112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "# ROOT_PROJECT = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling/\"\n",
    "# ROOT_DATASET = \"/home/dotamthuc/Works/Projects/ImageRestoration-Development-Unrolling\"\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader import ImageSuperResolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d23ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, nchannels):\n",
    "        super(CustomLayerNorm, self).__init__()\n",
    "        \n",
    "        self.nchannels = nchannels\n",
    "        self.weighted_transform = nn.Conv2d(nchannels, nchannels, kernel_size=1, stride=1, groups=nchannels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bz, nchannels, h, w = x.shape\n",
    "        sigma = x.var(dim=1, keepdim=True, correction=1)\n",
    "        # bz, 1, h, w = sigma.shape\n",
    "        return self.weighted_transform(x / torch.sqrt(sigma+1e-5))\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class LocalGatedLinearBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super(LocalGatedLinearBlock, self).__init__()\n",
    "\n",
    "        self.channels_linear_op       = nn.Conv2d(dim, hidden_dim*2, kernel_size=1, bias=False)\n",
    "        self.channels_local_linear_op = nn.Conv2d(\n",
    "            hidden_dim*2, hidden_dim*2, \n",
    "            kernel_size=3, stride=1, \n",
    "            padding=1, padding_mode=\"replicate\",\n",
    "            groups=hidden_dim*2, \n",
    "            bias=False\n",
    "        )\n",
    "        self.project_out = nn.Conv2d(hidden_dim, dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channels_linear_op(x)\n",
    "        mask, x = self.channels_local_linear_op(x).chunk(2, dim=1)\n",
    "        x = nn.functional.sigmoid(mask) * x\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class LocalLowpassFilteringBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LocalLowpassFilteringBlock, self).__init__()\n",
    "\n",
    "        # Filter Layer\n",
    "        # self.norm_filter = CustomLayerNorm(dim)\n",
    "        # self.local_linear_filter = nn.Conv2d(\n",
    "        #     dim, dim, \n",
    "        #     kernel_size=3, stride=1, \n",
    "        #     padding=1, padding_mode=\"replicate\",\n",
    "        #     groups=dim, \n",
    "        #     bias=False\n",
    "        # )\n",
    "        # self.skip_weight_filter = Parameter(\n",
    "        #     torch.tensor([0.5, 0.5], dtype=torch.float32),\n",
    "        #     requires_grad=True\n",
    "        # )\n",
    "\n",
    "        # Linear Layer\n",
    "        self.norm = CustomLayerNorm(dim)\n",
    "        self.local_linear = LocalGatedLinearBlock(dim, dim*2)\n",
    "        self.skip_weight= Parameter(\n",
    "            torch.tensor([0.5, 0.5], dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x = self.skip_weight_filter[0] * x + self.skip_weight_filter[1] * self.local_linear_filter(self.norm_filter(x))\n",
    "        x = self.skip_weight[0] * x + self.skip_weight[1] * self.local_linear(self.norm(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class ReginalPixelEmbeding(nn.Module):\n",
    "    def __init__(self, n_channels_in=3, dim=48, bias=False):\n",
    "        super(ReginalPixelEmbeding, self).__init__()\n",
    "\n",
    "        self.channels_local_linear_op01 = nn.Conv2d(\n",
    "            n_channels_in, dim*2, \n",
    "            kernel_size=3, stride=1, \n",
    "            padding=1, padding_mode=\"replicate\",\n",
    "            bias=False\n",
    "        )\n",
    "        # self.project_out = nn.Conv2d(dim//2, dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mask, x = self.channels_local_linear_op01(x).chunk(2, dim=1)\n",
    "        x = nn.functional.sigmoid(mask) * mask * x\n",
    "        # x = self.channels_local_linear_op01(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ##########################################################################\n",
    "# ## Down/Up Sampling\n",
    "# class Downsampling(nn.Module):\n",
    "#     def __init__(self, dim_in, dim_out):\n",
    "#         super(Downsampling, self).__init__()\n",
    "\n",
    "#         self.local_linear = nn.Conv2d(dim_in, dim_out//4, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "#         self.local_concat = nn.PixelUnshuffle(2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.local_concat(self.local_linear(x))\n",
    "#         return x\n",
    "\n",
    "# class Upsampling(nn.Module):\n",
    "#     def __init__(self, dim_in, dim_out):\n",
    "#         super(Upsampling, self).__init__()\n",
    "\n",
    "#         self.local_linear = nn.Conv2d(dim_in, dim_out*4, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "#         self.local_concat = nn.PixelShuffle(2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.local_concat(self.local_linear(x))\n",
    "#         return x\n",
    "\n",
    "##########################################################################\n",
    "## Down/Up Sampling\n",
    "class Downsampling(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Downsampling, self).__init__()\n",
    "\n",
    "        self.local_linear = nn.Conv2d(dim_in, dim_out//2, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "        self.local_concat = nn.PixelUnshuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask, x = self.local_linear(x).chunk(2, dim=1)\n",
    "        x = nn.functional.sigmoid(mask) * mask * x\n",
    "        x = self.local_concat(x)\n",
    "        return x\n",
    "\n",
    "class Upsampling(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Upsampling, self).__init__()\n",
    "\n",
    "        self.local_linear = nn.Conv2d(dim_in, dim_out*8, kernel_size=3, stride=1, padding=1, padding_mode=\"replicate\", bias=False)\n",
    "        self.local_concat = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Upsampling in:{x.shape}\")\n",
    "        mask, x = self.local_linear(x).chunk(2, dim=1)\n",
    "        # print(f\"Upsampling out//4:{x.shape}\")\n",
    "        x = nn.functional.sigmoid(mask) * mask * x\n",
    "        x = self.local_concat(x)\n",
    "        # print(f\"Upsampling out:{x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class AbtractMultiScaleGraphFilter(nn.Module):\n",
    "    def __init__(self, \n",
    "        n_channels_in=3, \n",
    "        n_channels_out=3, \n",
    "        dims=[48, 64, 96, 128],\n",
    "        num_blocks=[4, 6, 6, 8], \n",
    "        num_blocks_out=4\n",
    "    ):\n",
    "\n",
    "        super(AbtractMultiScaleGraphFilter, self).__init__()\n",
    "\n",
    "        self.patch_3x3_embeding = ReginalPixelEmbeding(n_channels_in, dims[0])\n",
    "        self.encoder_scale_00 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[0]) for i in range(num_blocks[0])\n",
    "        ])\n",
    "        \n",
    "        self.down_sample_00_01 = Downsampling(dim_in=dims[0], dim_out=dims[1]) \n",
    "        self.encoder_scale_01 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[1]) for i in range(num_blocks[1])\n",
    "        ])\n",
    "\n",
    "        self.down_sample_01_02 = Downsampling(dim_in=dims[1], dim_out=dims[2]) \n",
    "        self.encoder_scale_02 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[2]) for i in range(num_blocks[2])\n",
    "        ])\n",
    "\n",
    "        self.down_sample_02_03 = Downsampling(dim_in=dims[2], dim_out=dims[3]) \n",
    "        self.encoder_scale_03 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[3]) for i in range(num_blocks[3])\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.up_sample_03_02 = Upsampling(dim_in=dims[3], dim_out=dims[2])\n",
    "        self.combine_channels_02 = nn.Conv2d(dims[2]*2, dims[2], kernel_size=1, bias=False)\n",
    "        self.decoder_scale_02 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[2]) for i in range(num_blocks[2])\n",
    "        ])\n",
    "\n",
    "        self.up_sample_02_01 = Upsampling(dim_in=dims[2], dim_out=dims[1])\n",
    "        self.combine_channels_01 = nn.Conv2d(dims[1]*2, dims[1], kernel_size=1, bias=False)\n",
    "        self.decoder_scale_01 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[1]) for i in range(num_blocks[1])\n",
    "        ])\n",
    "\n",
    "        self.up_sample_01_00 = Upsampling(dim_in=dims[1], dim_out=dims[0])\n",
    "        self.combine_channels_00 = nn.Conv2d(dims[0]*2, dims[0], kernel_size=1, bias=False)\n",
    "        self.decoder_scale_00 = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[0]) for i in range(num_blocks[0])\n",
    "        ])\n",
    "\n",
    "        self.refining_block = nn.Sequential(*[\n",
    "            LocalLowpassFilteringBlock(dim=dims[0]) for i in range(num_blocks_out)\n",
    "        ])\n",
    "        self.linear_output = nn.Conv2d(dims[0], n_channels_out, kernel_size=1, bias=False)\n",
    "        self.skip_weight_output = Parameter(\n",
    "            torch.tensor([0.5, 0.5], dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        \n",
    "        # Downward\n",
    "        inp_enc_scale_00 = self.patch_3x3_embeding(img)\n",
    "        out_enc_scale_00 = self.encoder_scale_00(inp_enc_scale_00)\n",
    "        # print(f\"out_enc_scale_00:{out_enc_scale_00.shape}\")\n",
    "        \n",
    "        inp_enc_scale_01 = self.down_sample_00_01(out_enc_scale_00)\n",
    "        out_enc_scale_01 = self.encoder_scale_01(inp_enc_scale_01)\n",
    "        # print(f\"inp_enc_scale_01:{inp_enc_scale_01.shape}\")\n",
    "\n",
    "        inp_enc_scale_02 = self.down_sample_01_02(out_enc_scale_01)\n",
    "        out_enc_scale_02 = self.encoder_scale_02(inp_enc_scale_02)\n",
    "        # print(f\"inp_enc_scale_02:{inp_enc_scale_02.shape}\")\n",
    "\n",
    "        inp_enc_scale_03 = self.down_sample_02_03(out_enc_scale_02)\n",
    "        out_enc_scale_03 = self.encoder_scale_03(inp_enc_scale_03)\n",
    "        # print(f\"inp_enc_scale_03:{inp_enc_scale_03.shape}\")\n",
    "\n",
    "        # Upward\n",
    "        inp_dec_scale_02 = self.up_sample_03_02(out_enc_scale_03)\n",
    "        # print(f\"inp_dec_scale_02={inp_dec_scale_02.shape}\")\n",
    "        out_dec_scale_02 = torch.cat([inp_dec_scale_02, out_enc_scale_02], 1)\n",
    "        # print(f\"out_dec_scale_02={out_dec_scale_02.shape}\")\n",
    "        out_dec_scale_02 = self.combine_channels_02(out_dec_scale_02)\n",
    "        # print(f\"out_dec_scale_02={out_dec_scale_02.shape}\")\n",
    "        out_dec_scale_02 = self.decoder_scale_02(out_dec_scale_02)\n",
    "        # print(f\"out_dec_scale_02={out_dec_scale_02.shape}\")\n",
    "\n",
    "        inp_dec_scale_01 = self.up_sample_02_01(out_dec_scale_02)\n",
    "        # print(f\"inp_dec_scale_01={inp_dec_scale_01.shape}\")\n",
    "        out_dec_scale_01 = torch.cat([inp_dec_scale_01, out_enc_scale_01], 1)\n",
    "        # print(f\"out_dec_scale_01={out_dec_scale_01.shape}\")\n",
    "        out_dec_scale_01 = self.combine_channels_01(out_dec_scale_01)\n",
    "        # print(f\"out_dec_scale_01={out_dec_scale_01.shape}\")\n",
    "        out_dec_scale_01 = self.decoder_scale_01(out_dec_scale_01)\n",
    "        # print(f\"out_dec_scale_01={out_dec_scale_01.shape}\")\n",
    "\n",
    "        inp_dec_scale_00 = self.up_sample_01_00(out_dec_scale_01)\n",
    "        # print(f\"inp_dec_scale_00={inp_dec_scale_00.shape}\")\n",
    "        out_dec_scale_00 = torch.cat([inp_dec_scale_00, out_enc_scale_00], 1)\n",
    "        # print(f\"out_dec_scale_00={out_dec_scale_00.shape}\")\n",
    "        out_dec_scale_00 = self.combine_channels_00(out_dec_scale_00)\n",
    "        # print(f\"out_dec_scale_00={out_dec_scale_00.shape}\")\n",
    "        out_dec_scale_00 = self.decoder_scale_00(out_dec_scale_00)\n",
    "        # print(f\"out_dec_scale_00={out_dec_scale_00.shape}\")\n",
    "\n",
    "        output = self.refining_block(out_dec_scale_00)\n",
    "        # print(f\"output={output.shape}\")\n",
    "        output = self.linear_output(output) \n",
    "        # print(f\"output={output.shape}\")\n",
    "        output = self.skip_weight_output[0] * img + self.skip_weight_output[1] * output\n",
    "        # print(f\"output={output.shape}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb744294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74c18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "VERBOSE_RATE = 1000\n",
    "\n",
    "(H_train01, W_train01) = (64, 64)\n",
    "(H_train02, W_train02) = (128, 128)\n",
    "(H_train03, W_train03) = (256, 256)\n",
    "(H_train04, W_train04) = (512, 512)\n",
    "\n",
    "(H_val, W_val) = (128, 128)\n",
    "(H_test, W_test) = (496, 496)\n",
    "\n",
    "train_dataset01 = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"vary_addictive_noise\",\n",
    "    lambda_noise=[[1.0, 10.0, 15.0, 20.0, 25.0], [0.1, 0.1, 0.1, 0.1, 0.6]],\n",
    "    use_data_aug=True,\n",
    "    patch_size=(H_train02,H_train02),\n",
    "    patch_overlap_size=(H_train02//4,H_train02//4),\n",
    "    max_num_patchs=3200000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset01, batch_size=4, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c9a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AbtractMultiScaleGraphFilter(\n",
    "#     n_channels_in=3, \n",
    "#     n_channels_out=3, \n",
    "#     dims=[48, 64, 96, 128],\n",
    "#     num_blocks=[4, 6, 6, 8], \n",
    "#     num_blocks_out=4\n",
    "# ).to(DEVICE)\n",
    "# model.compile()\n",
    "# s = 0\n",
    "# for p in model.parameters():\n",
    "#     s += np.prod(np.array(p.shape))\n",
    "# print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "# for patchs_noisy, patchs_true in data_train_batched01:\n",
    "#     s = time.time()\n",
    "#     patchs_noisy = patchs_noisy.to(DEVICE)\n",
    "#     patchs_true = patchs_true.to(DEVICE) \n",
    "#     reconstruct_patchs = model(patchs_noisy.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e4bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba76ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.eval()\n",
    "# csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "# img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "# paths = img_infos[\"path\"].tolist()\n",
    "# paths = [\n",
    "#     os.path.join(ROOT_DATASET,path)\n",
    "#     for path in paths\n",
    "# ]\n",
    "\n",
    "# sigma_test = 25.0\n",
    "# factor = 8\n",
    "# list_test_mse = []\n",
    "# random_state = np.random.RandomState(seed=2204)\n",
    "# test_i = 0\n",
    "# s = time.time()\n",
    "# for file_ in paths:\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     img = Image.open(file_)\n",
    "#     img_true_255 = np.array(img).astype(np.float32)\n",
    "#     img_true = img_true_255 / 255.0\n",
    "\n",
    "#     noisy_img_raw = img_true.copy()\n",
    "#     noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "#     noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "#     noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "#     h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "#     H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "#     padh = H-h if h%factor!=0 else 0\n",
    "#     padw = W-w if w%factor!=0 else 0\n",
    "#     noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "#     with torch.no_grad():\n",
    "#         restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9f836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970c3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 3720138\n",
      "iter=0 time=51.583767890930176 psnr=9.512264487630633 mse=0.1118854341060725\n",
      "iter=100 time=0.03967165946960449 psnr=17.344542698256443 mse=0.020597555287676133\n",
      "iter=200 time=0.03960895538330078 psnr=20.77634539749856 mse=0.009518438215172054\n",
      "iter=300 time=0.1084437370300293 psnr=23.204233175909334 mse=0.004921384402501978\n",
      "iter=400 time=0.03794431686401367 psnr=23.89153499996663 mse=0.004239912172654279\n",
      "iter=500 time=0.04765176773071289 psnr=23.75865634630084 mse=0.004391514320253632\n",
      "iter=600 time=0.036995649337768555 psnr=23.999545281778587 mse=0.004074403911062467\n",
      "iter=700 time=0.045567989349365234 psnr=24.471600503360307 mse=0.0037402041878354803\n",
      "iter=800 time=0.03797101974487305 psnr=24.846363833598925 mse=0.0033698196445720464\n",
      "iter=900 time=0.04056382179260254 psnr=24.76799831968464 mse=0.0034587656686208496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 22:06:21.177000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:21.726000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.056000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.158000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.239000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.303000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[18, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:22.807000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.301000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.457000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.567000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.660000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.752000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:23.827000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[10, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:24.421000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:24.958000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.122000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.236000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.326000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.419000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:25.493000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[6, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.086000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.606000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.770000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.883000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:26.977000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.074000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.163000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.253000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.329000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[4, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:27.902000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.398000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.547000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.637000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.735000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.832000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:28.912000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[3, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:29.549000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.014000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.157000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.246000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.337000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.426000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:30.503000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[5, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.126000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.606000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.747000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.835000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:31.925000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:32.011000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:32.099000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n",
      "W0812 22:06:32.184000 706170 site-packages/torch/utils/_sympy/interp.py:176] [0/1] failed while executing pow_by_natural([VR[9, int_oo], VR[-1, -1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH TESTING - iter=1000 -  psnr_testing=23.80134391784668\n",
      "iter=1000 time=0.07832860946655273 psnr=25.141954204837685 mse=0.003118523203622476\n",
      "iter=1100 time=0.036153316497802734 psnr=24.780601666127268 mse=0.003419877452834531\n",
      "iter=1200 time=0.0366976261138916 psnr=25.19416581003319 mse=0.003099143445443492\n",
      "iter=1300 time=0.036928415298461914 psnr=25.215822753530606 mse=0.003088459560266289\n",
      "iter=1400 time=0.039591312408447266 psnr=25.538570764187966 mse=0.0028549606949842187\n",
      "iter=1500 time=0.03767681121826172 psnr=25.273694271556554 mse=0.003040062083207427\n",
      "iter=1600 time=0.039666175842285156 psnr=25.382411187296647 mse=0.00296520713247309\n",
      "iter=1700 time=0.036934852600097656 psnr=25.415672066211187 mse=0.002985649055764809\n",
      "iter=1800 time=0.038036346435546875 psnr=25.96111087227655 mse=0.002629404718555788\n",
      "iter=1900 time=0.03574967384338379 psnr=25.982652482408113 mse=0.0025854721416759645\n",
      "FINISH TESTING - iter=2000 -  psnr_testing=23.975055694580078\n",
      "iter=2000 time=0.06890320777893066 psnr=26.001139071604417 mse=0.002614787317340572\n",
      "iter=2100 time=0.0368809700012207 psnr=26.068082836699 mse=0.002572372646509784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m list_train_mse = []\n\u001b[32m     33\u001b[39m list_train_psnr = []\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpatchs_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatchs_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_train_batched01\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    945\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    950\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared/Thuc/hoodsgatedrive/envs/working2025_pytorch/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "\n",
    "model = AbtractMultiScaleGraphFilter(\n",
    "    n_channels_in=3, \n",
    "    n_channels_out=3, \n",
    "    dims=[48, 64, 96, 128],\n",
    "    num_blocks=[4, 6, 6, 8], \n",
    "    num_blocks_out=4\n",
    ").to(DEVICE)\n",
    "model.compile()\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "criterian = nn.L1Loss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    eps=1e-08\n",
    ")\n",
    "lr_scheduler = MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[200000, 500000, 650000], gamma=0.5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "i = 0\n",
    "### TRAINING\n",
    "list_train_mse = []\n",
    "list_train_psnr = []\n",
    "for patchs_noisy, patchs_true in data_train_batched01:\n",
    "    s = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    patchs_noisy = patchs_noisy.to(DEVICE)\n",
    "    patchs_true = patchs_true.to(DEVICE) \n",
    "    reconstruct_patchs = model(patchs_noisy.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "    loss_value = criterian(reconstruct_patchs, patchs_true)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    img_true = np.clip(patchs_true.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    img_recon = np.clip(reconstruct_patchs.detach().cpu().numpy(), a_min=0.0, a_max=1.0).astype(np.float64)\n",
    "    train_mse_value = np.square(img_true- img_recon).mean()\n",
    "    train_psnr = 10 * np.log10(1/train_mse_value)\n",
    "    list_train_psnr.append(train_psnr)\n",
    "    list_train_mse.append(train_mse_value)\n",
    "    if (i%100 == 0):\n",
    "        print(f\"iter={i} time={time.time()-s} psnr={np.mean(list_train_psnr[-100:])} mse={np.mean(list_train_mse[-100:])}\")\n",
    "    i+=1\n",
    "\n",
    "    if (i%1000 == 0):\n",
    "\n",
    "        model.eval()\n",
    "        csv_path = os.path.join(ROOT_DATASET, \"dataset/McMaster_testing_data_info.csv\")\n",
    "        img_infos = pd.read_csv(csv_path, index_col='index')\n",
    "\n",
    "        paths = img_infos[\"path\"].tolist()\n",
    "        paths = [\n",
    "            os.path.join(ROOT_DATASET,path)\n",
    "            for path in paths\n",
    "        ]\n",
    "\n",
    "        sigma_test = 25.0\n",
    "        factor = 8\n",
    "        list_test_mse = []\n",
    "        random_state = np.random.RandomState(seed=2204)\n",
    "        test_i = 0\n",
    "        s = time.time()\n",
    "        for file_ in paths:\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            img = Image.open(file_)\n",
    "            img_true_255 = np.array(img).astype(np.float32)\n",
    "            img_true = img_true_255 / 255.0\n",
    "\n",
    "            noisy_img_raw = img_true.copy()\n",
    "            noisy_img_raw += random_state.normal(0, sigma_test/255., img_true.shape)\n",
    "\n",
    "            noisy_img = torch.from_numpy(noisy_img_raw).permute(2,0,1)\n",
    "            noisy_img = noisy_img.unsqueeze(0)\n",
    "\n",
    "            h,w = noisy_img.shape[2], noisy_img.shape[3]\n",
    "            H,W = ((h+factor)//factor)*factor, ((w+factor)//factor)*factor\n",
    "            padh = H-h if h%factor!=0 else 0\n",
    "            padw = W-w if w%factor!=0 else 0\n",
    "            noisy_img = nn.functional.pad(noisy_img, (0,padw,0,padh), 'reflect')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                restored = model(noisy_img.to(DEVICE))\n",
    "\n",
    "            restored = restored[:,:,:h,:w]\n",
    "            restored = torch.clamp(restored,0,1).cpu().detach().permute(0, 2, 3, 1).squeeze(0).numpy().copy()\n",
    "\n",
    "            restored = img_as_ubyte(restored).astype(np.float32)\n",
    "            test_mse_value = np.square(img_true_255- restored).mean()\n",
    "            list_test_mse.append(test_mse_value)\n",
    "            # print(f\"test_i={test_i} time={time.time()-s} test_i_psnr_value={20 * np.log10(255.0 / np.sqrt(test_mse_value))}\")  \n",
    "            test_i += 1\n",
    "            s = time.time()\n",
    "\n",
    "        psnr_testing = 20 * np.log10(255.0 / np.sqrt(list_test_mse))\n",
    "        print(f\"FINISH TESTING - iter={i} -  psnr_testing={np.mean(psnr_testing)}\")\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003deaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31659b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf907b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9995edd2",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72601b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader import ImageSuperResolution\n",
    "import baselineRestormer as model_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75be36a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model with total parameters: 26111668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = model_structure.Restormer(\n",
    "    inp_channels=3,\n",
    "    out_channels=3,\n",
    "    dim=48,\n",
    "    num_blocks=[4,6,6,8],\n",
    "    num_refinement_blocks=4,\n",
    "    heads=[1,2,4,8],\n",
    "    ffn_expansion_factor=2.66,\n",
    "    bias=False,\n",
    "    LayerNorm_type=\"BiasFree\",\n",
    "    dual_pixel_task=False\n",
    ").to(DEVICE)\n",
    "\n",
    "s = 0\n",
    "for p in model.parameters():\n",
    "    s += np.prod(np.array(p.shape))\n",
    "    # print(p.dtype, np.array(p.shape), s)\n",
    "\n",
    "print(f\"Init model with total parameters: {s}\")\n",
    "\n",
    "criterian = nn.L1Loss()\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0006,\n",
    "    weight_decay=0.0001,\n",
    "    eps=1e-08\n",
    ")\n",
    "# [100000, 200000, 350000, 500000, 575000, 650000]\n",
    "lr_scheduler01 = MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[25000, 50000, 75000, 100000],\n",
    "    gamma=np.sqrt(np.sqrt(0.5))\n",
    ")\n",
    "lr_scheduler02 = CosineAnnealingLR(optimizer, T_max=700000, eta_min=0.000001)\n",
    "lr_scheduler02.base_lrs = [0.0003 for group in optimizer.param_groups]\n",
    "\n",
    "lr_scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[lr_scheduler01, lr_scheduler02],\n",
    "    milestones=[160000],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6bf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a93bfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "for i in range(700000):\n",
    "    lr_list.append(lr_scheduler.get_last_lr())\n",
    "    # if (i%10000==0):\n",
    "    #     print(f\"i={i}\")\n",
    "    # optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bd3027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f853c6a50>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1455b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(size=(4, 16, 4, 4))\n",
    "m = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2, padding=0, output_padding=(0, 0), groups=1, bias=False)\n",
    "m(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa356a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a4b84f3",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11e8ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "#########################################################################################################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT_PROJECT = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/ImageRestoration-Development-Unrolling/\"\n",
    "ROOT_DATASET = \"/home/jovyan/shared/Thuc/hoodsgatedrive/projects/\"\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_PROJECT, 'exploration/model_multiscale_mixture_GLR/lib'))\n",
    "from dataloader_v2 import ImageSuperResolution\n",
    "import model_GLR_GTV_deep_v13 as model_structure\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a433371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/result/model_test31_v13/checkpoints/\")\n",
    "training_state_path = os.path.join(CHECKPOINT_DIR, 'checkpoints_epoch00_iter0020k.pt')\n",
    "training_state = torch.load(training_state_path, weights_only=False)\n",
    "\n",
    "model =  model_structure.AbtractMultiScaleGraphFilter(\n",
    "    n_channels_in=3, \n",
    "    n_channels_out=3, \n",
    "    dims=[64, 128, 256, 512],\n",
    "    nsubnets=[1, 1, 1, 1],\n",
    "    ngraphs=[8, 16, 16, 32], #[1, 2, 4, 8], \n",
    "    num_blocks=[4, 6, 6, 8], \n",
    "    num_blocks_out=4\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(training_state[\"model\"])\n",
    "# model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "820f5324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('skip_weight', tensor([0.4802, 0.4866], device='cuda:0')),\n",
       "             ('local_filter.alphaCGD',\n",
       "              tensor([[0.4609, 0.4145, 0.4550, 0.5027, 0.3791, 0.4638, 0.4244, 0.3662, 0.3757,\n",
       "                       0.3956, 0.3958, 0.3889, 0.4967, 0.4185, 0.4371, 0.3575],\n",
       "                      [0.4357, 0.3023, 0.3470, 0.4177, 0.2828, 0.4451, 0.4532, 0.1977, 0.3248,\n",
       "                       0.2498, 0.4556, 0.3998, 0.4342, 0.3638, 0.4605, 0.4055],\n",
       "                      [0.4534, 0.3045, 0.3563, 0.4367, 0.2847, 0.4521, 0.4594, 0.2023, 0.3316,\n",
       "                       0.2491, 0.4550, 0.4037, 0.4538, 0.3712, 0.4721, 0.4157]],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.betaCGD',\n",
       "              tensor([[ 0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,\n",
       "                        0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000],\n",
       "                      [ 0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,\n",
       "                        0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000,  0.1000],\n",
       "                      [ 0.1133, -0.0318,  0.0125,  0.1036, -0.0690,  0.0952,  0.0816, -0.1249,\n",
       "                       -0.0153, -0.0852,  0.0578,  0.0210,  0.1697,  0.0222,  0.1073,  0.0458]],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.ro00',\n",
       "              tensor([0.0715, 0.0973, 0.1213, 0.1153, 0.1085, 0.0687, 0.0815, 0.1267, 0.1053,\n",
       "                      0.1128, 0.1001, 0.0961, 0.0652, 0.0969, 0.0791, 0.0916],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.gamma00',\n",
       "              tensor([-6.6539, -6.5309, -6.5531, -6.4945, -6.6254, -6.6552, -6.6238, -6.5393,\n",
       "                      -6.5795, -6.5894, -6.6143, -6.6188, -6.5957, -6.6685, -6.6500, -6.4691],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.muys00',\n",
       "              tensor([ 0.0399, -0.0385, -0.0488, -0.0415, -0.0464,  0.0265,  0.0344, -0.0445,\n",
       "                      -0.0439, -0.0450,  0.0300,  0.0160,  0.0595,  0.0258,  0.0477, -0.0342],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.ro01',\n",
       "              tensor([ 0.0185,  0.0555,  0.2077,  0.1529,  0.1089,  0.1538,  0.0285,  0.1351,\n",
       "                       0.1392,  0.1142, -0.0102,  0.0770,  0.0554,  0.0689, -0.0070,  0.0325],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.gamma01',\n",
       "              tensor([-9.2034, -9.1991, -9.1658, -9.1516, -9.1921, -9.2074, -9.1970, -9.1827,\n",
       "                      -9.1773, -9.1875, -9.2078, -9.1903, -9.1956, -9.1964, -9.2096, -9.1900],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.muys01',\n",
       "              tensor([-1.2505e-02, -1.0645e-02, -2.9689e-02,  9.4548e-04,  1.3636e-03,\n",
       "                       3.8217e-02, -1.7121e-02, -1.7752e-03,  2.6487e-03, -6.5064e-03,\n",
       "                       6.2092e-02,  4.2010e-02,  9.8742e-04, -1.1772e-02,  7.9261e-02,\n",
       "                      -7.3057e-05], device='cuda:0')),\n",
       "             ('local_filter.patchs_features_extraction00.0.weight',\n",
       "              tensor([[[[-0.0272]],\n",
       "              \n",
       "                       [[-0.0109]],\n",
       "              \n",
       "                       [[ 0.0031]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0338]],\n",
       "              \n",
       "                       [[-0.1114]],\n",
       "              \n",
       "                       [[ 0.0151]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0591]],\n",
       "              \n",
       "                       [[-0.0565]],\n",
       "              \n",
       "                       [[-0.0412]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1401]],\n",
       "              \n",
       "                       [[ 0.0040]],\n",
       "              \n",
       "                       [[ 0.0506]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0269]],\n",
       "              \n",
       "                       [[-0.0807]],\n",
       "              \n",
       "                       [[ 0.1233]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1113]],\n",
       "              \n",
       "                       [[-0.0066]],\n",
       "              \n",
       "                       [[ 0.0415]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0620]],\n",
       "              \n",
       "                       [[-0.0659]],\n",
       "              \n",
       "                       [[ 0.1114]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1844]],\n",
       "              \n",
       "                       [[ 0.0579]],\n",
       "              \n",
       "                       [[-0.1176]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0478]],\n",
       "              \n",
       "                       [[ 0.0567]],\n",
       "              \n",
       "                       [[-0.0176]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0511]],\n",
       "              \n",
       "                       [[ 0.0018]],\n",
       "              \n",
       "                       [[-0.0190]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0492]],\n",
       "              \n",
       "                       [[ 0.1221]],\n",
       "              \n",
       "                       [[-0.0016]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0686]],\n",
       "              \n",
       "                       [[-0.0060]],\n",
       "              \n",
       "                       [[-0.0500]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule00.stats_kernel_p01',\n",
       "              tensor([[[[1.0465]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0252]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0660]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0341]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0107]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0241]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0513]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0588]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0914]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0863]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1567]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0695]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0999]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1771]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0429]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0781]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0994]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0871]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1413]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0720]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1006]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0887]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1205]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0902]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1317]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0473]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0687]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0900]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0954]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0947]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0058]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0814]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0858]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0948]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1048]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1133]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0896]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1164]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0617]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1278]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0801]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0546]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0684]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0628]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0557]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0118]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0367]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0541]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0681]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0536]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0404]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0324]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0363]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0414]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0606]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1040]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1415]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1299]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1325]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0610]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0864]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1869]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.2011]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1125]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1400]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0517]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1132]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1419]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0892]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0980]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0414]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1189]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1476]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0768]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.2187]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0452]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1611]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0484]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1172]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1366]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0329]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0753]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0192]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0888]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9937]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0638]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0799]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0752]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0284]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0220]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0776]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1499]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1203]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1102]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0855]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0246]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0427]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0932]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9622]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9848]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0503]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9588]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0084]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9693]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0466]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1141]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0438]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0839]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0164]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1462]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0501]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0891]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0258]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0431]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0512]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0855]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0277]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0977]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0343]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0352]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0469]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1011]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0412]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0544]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0699]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0255]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1124]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0851]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule00.stats_kernel_p02a',\n",
       "              tensor([[[[0.5129]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4927]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5340]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5396]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5550]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4941]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4761]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4718]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4598]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4528]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4835]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4426]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4505]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3948]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4800]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4603]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4292]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4259]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3753]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4595]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4207]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4233]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4281]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4444]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4076]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4547]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4450]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5623]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4547]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4169]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5999]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4356]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5258]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5025]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4835]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4744]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4237]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4249]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4795]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4589]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4952]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4740]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5161]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4501]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4774]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5276]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4524]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5390]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5068]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4963]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5164]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5326]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4983]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5028]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5391]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4084]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4599]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4113]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4308]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5283]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5283]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4541]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4403]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4719]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4256]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5214]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4487]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4466]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5085]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4609]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4740]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4744]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3954]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4329]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3171]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5136]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3945]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4587]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5301]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4532]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4870]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4801]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5083]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4538]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5633]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5055]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4784]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5230]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4865]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4938]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4886]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4670]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4969]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4920]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4302]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5057]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5151]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5320]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5120]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5704]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5416]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5405]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5022]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5621]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4808]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4620]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5311]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4175]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5473]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5235]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5512]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5333]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5040]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5227]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5262]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4588]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4404]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5158]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5209]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5212]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5013]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5237]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4183]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4601]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4552]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4519]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule00.stats_kernel_p02b',\n",
       "              tensor([[[[0.5197]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5065]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5162]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4969]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5280]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5293]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4801]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4631]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4607]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4686]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4209]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4827]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4005]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3790]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5084]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4759]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4314]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4386]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3861]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4643]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4388]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4360]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4106]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4327]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3931]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4870]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4524]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4687]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4335]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4210]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5019]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4116]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4382]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4545]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5790]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4742]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4895]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4117]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4387]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6125]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5050]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5129]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4592]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4544]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4748]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5247]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5091]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4536]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5095]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5366]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5164]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4753]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4516]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4841]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5105]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3860]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4421]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3541]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4095]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4974]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4715]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4223]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4550]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4830]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4561]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4340]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4343]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4178]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4564]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4609]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5035]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4594]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3667]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4647]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3205]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4761]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3784]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5292]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4949]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4695]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5120]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4930]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4827]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4324]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4813]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4774]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5227]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4753]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4489]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5115]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4318]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4578]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5052]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4845]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4765]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4983]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4381]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5253]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5554]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5341]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5525]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5212]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5355]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5482]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4545]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5249]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4288]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4810]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4335]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4813]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4996]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5128]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5313]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5006]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4790]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4512]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4284]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6062]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4942]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4965]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4349]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5030]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4756]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4750]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4843]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4595]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4478]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule00.stats_kernel_p03',\n",
       "              tensor([[[[0.5030]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5097]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4840]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4933]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4855]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4684]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5432]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5479]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5208]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5459]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5781]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5296]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5976]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6173]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5100]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5439]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5875]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5723]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6579]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5497]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5852]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5797]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5982]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5857]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6169]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5429]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5797]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4724]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5789]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5969]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4519]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5870]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5306]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5481]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5225]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5492]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5573]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5787]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5502]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5108]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5039]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5100]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5324]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6004]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5349]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4850]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5288]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5143]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4980]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4896]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4958]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5011]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5584]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5332]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5013]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6192]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5652]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6001]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4963]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5065]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5788]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5488]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5223]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5561]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5323]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5661]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5695]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5206]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5652]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5126]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5505]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6310]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5910]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6930]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4998]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6385]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5279]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5255]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5408]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5077]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5183]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4980]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5780]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4669]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5064]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5072]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5051]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5407]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4982]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5432]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5359]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5241]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5231]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5692]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5557]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5099]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5791]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4529]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4190]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4716]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4518]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4718]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4655]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4942]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5072]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4911]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5656]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5125]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5820]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4932]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5035]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4614]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4752]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5160]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5080]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5220]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5718]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4770]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4944]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4974]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5231]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5101]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5177]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5514]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5283]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5347]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5390]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule00.multiM',\n",
       "              tensor([[1.0775, 1.0375, 0.9435, 1.0747, 1.1632, 0.9982, 1.0469, 1.0095],\n",
       "                      [1.1622, 1.0723, 1.1727, 1.0408, 1.1269, 1.0825, 1.1510, 1.1428],\n",
       "                      [1.0544, 0.9653, 1.0513, 1.0473, 0.8800, 0.9569, 1.1312, 0.9264],\n",
       "                      [1.1120, 1.0616, 1.0666, 1.0835, 1.0052, 1.0325, 0.9670, 1.0199],\n",
       "                      [1.1373, 1.1446, 0.9932, 1.0574, 1.1496, 1.0720, 1.1868, 1.0170],\n",
       "                      [1.0031, 1.0720, 1.0740, 1.0322, 1.0066, 0.9797, 1.0522, 0.9838],\n",
       "                      [0.9787, 0.9959, 1.0036, 1.1136, 1.0533, 0.9836, 1.0558, 1.1419],\n",
       "                      [1.0370, 1.0748, 1.1812, 1.0195, 1.0639, 1.0731, 1.0614, 1.0744],\n",
       "                      [1.1521, 1.1217, 0.9944, 1.1029, 1.0306, 1.0388, 1.1725, 0.9735],\n",
       "                      [0.9860, 1.0648, 1.1243, 1.0051, 1.0420, 1.0867, 1.0515, 1.0909],\n",
       "                      [0.9193, 0.9876, 1.1545, 1.2268, 1.1658, 0.9741, 1.1117, 1.1004],\n",
       "                      [1.0141, 1.0059, 0.9677, 1.1796, 0.9659, 1.0703, 0.9999, 1.1627],\n",
       "                      [0.9823, 1.0538, 1.0768, 0.9303, 0.9970, 1.0123, 1.0127, 1.0486],\n",
       "                      [1.0704, 1.1557, 1.0519, 0.9260, 1.0271, 0.9988, 1.1549, 1.0325],\n",
       "                      [1.1161, 1.0171, 1.0405, 1.0188, 1.0706, 1.0693, 1.0856, 1.0141],\n",
       "                      [1.2170, 1.2316, 1.1301, 1.1059, 0.9043, 0.9343, 1.0520, 1.1123]],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.GLRmodule00.stats_kernel_p01',\n",
       "              tensor([[[[1.0459]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9444]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0639]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9899]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0674]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0311]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9529]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0252]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9983]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0280]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0638]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0541]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1516]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9640]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0408]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0989]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0115]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1134]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0336]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1015]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0115]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0435]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1085]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0512]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0653]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0522]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0978]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0239]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0480]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9560]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1076]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0038]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0031]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0500]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9818]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0312]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0400]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0005]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0833]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0318]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0423]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0465]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9356]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0492]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0582]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9469]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0179]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0338]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0223]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0087]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0503]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8471]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9915]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9537]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1203]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0554]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0387]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0983]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9899]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0274]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0880]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0769]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0534]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0567]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0262]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0266]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0167]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0131]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0642]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9958]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0712]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0716]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0511]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1243]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9968]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0603]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0126]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0155]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0618]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9423]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9580]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0347]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0139]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0544]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9306]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9609]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0451]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9221]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9926]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0085]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9029]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9597]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9393]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0000]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8873]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9896]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0161]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0375]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1314]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0168]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0820]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1092]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0471]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8592]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9054]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0050]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0294]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9296]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9863]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9256]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9257]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0710]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9977]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0548]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0372]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0740]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0167]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0157]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1161]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0149]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0275]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0518]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0565]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0335]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9600]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0553]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0092]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule00.stats_kernel_p02a',\n",
       "              tensor([[[[0.4726]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5743]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4816]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5655]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4674]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5067]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5740]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5017]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5295]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4892]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5041]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5068]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4773]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4049]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5388]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4666]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4522]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5072]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4312]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4817]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4682]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4984]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4783]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4266]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4704]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4497]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4441]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5137]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5279]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4715]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5427]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4400]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5483]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5261]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4886]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5409]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4693]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4802]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4956]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4135]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5080]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5913]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4662]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5771]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4565]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5101]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5466]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5171]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5091]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5166]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5369]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4885]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6743]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5151]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4850]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3947]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4870]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5034]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4635]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5079]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4940]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4632]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4468]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4569]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4599]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4847]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5266]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5586]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4682]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4998]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4523]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4539]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4604]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3877]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5031]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4724]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4862]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5402]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4569]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5928]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5307]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4963]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4527]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5506]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5579]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4950]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5782]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5441]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5127]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5525]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5971]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5781]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5050]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5904]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5356]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4751]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4742]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4687]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4852]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4785]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4361]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5794]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6890]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5746]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5297]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4601]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5767]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5284]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5917]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5930]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4589]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5332]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5123]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4836]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4416]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4428]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5484]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3984]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4928]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5184]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4504]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4803]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4649]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5030]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4908]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5154]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule00.stats_kernel_p02b',\n",
       "              tensor([[[[0.5026]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6121]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5150]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5242]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4595]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5062]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5292]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5072]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5128]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5014]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4577]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5088]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4522]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3566]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5442]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4850]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4363]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5553]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4475]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5245]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4641]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5081]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4964]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4826]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4340]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4756]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4995]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4298]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4654]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4859]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5437]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3954]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5151]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5310]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5628]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5644]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5368]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5098]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4796]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4758]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5182]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4841]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4955]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4781]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4833]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4625]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5852]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5106]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4700]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4903]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4680]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4784]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6075]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5528]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5326]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3927]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4842]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4978]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4189]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5078]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4587]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4542]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4618]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4736]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4772]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5167]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5106]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4840]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4621]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5060]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4489]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4451]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5064]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4366]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5124]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4822]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5219]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5227]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4871]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5660]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5348]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5043]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5526]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5296]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5860]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5567]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5155]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6212]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5359]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5209]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6626]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5517]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5989]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5198]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6501]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4951]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4796]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5425]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4592]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6118]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4815]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3923]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4555]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6428]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6907]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5001]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4937]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5866]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4788]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5738]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6027]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4769]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5369]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4658]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4845]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5103]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4712]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5045]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4866]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4947]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4735]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4577]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4931]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5211]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4519]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5025]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule00.stats_kernel_p03',\n",
       "              tensor([[[[0.5333]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4126]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5107]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4554]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5445]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5020]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4382]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4912]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4716]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5030]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5280]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4818]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5397]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6146]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4575]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5255]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5725]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4674]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5723]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4981]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5369]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4935]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5099]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5555]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5455]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5352]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5385]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5268]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5021]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5251]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4423]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5864]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4671]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4693]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5024]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4405]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4971]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5057]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5021]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4789]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4736]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5284]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4460]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5356]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5211]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4353]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4772]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4981]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4910]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5011]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4953]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3108]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4866]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4727]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6224]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5192]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5057]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5715]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4755]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5023]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5418]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5387]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5363]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5310]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5206]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4974]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4809]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4689]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5405]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4884]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5517]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5490]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5203]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5788]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4882]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5281]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4997]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4759]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5256]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4110]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4702]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5062]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4852]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5188]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4175]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4361]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4989]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3914]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4680]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4766]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3961]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4117]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3995]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4796]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3810]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4657]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5009]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5067]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5566]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4892]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5396]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5684]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4817]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.2978]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3706]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4840]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5204]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4090]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4840]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4171]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3974]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5192]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4916]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4691]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5226]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5481]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4966]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4835]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5697]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5087]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4906]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5372]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5302]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5137]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4808]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5227]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4833]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule00.multiM',\n",
       "              tensor([[1.5054, 1.5242, 1.3888, 1.5004, 1.6596, 1.5779, 1.5075, 1.5053],\n",
       "                      [1.3724, 1.1443, 0.7835, 1.2609, 1.2603, 1.4239, 1.3241, 1.0905],\n",
       "                      [1.4249, 1.3340, 0.8266, 1.3068, 1.2245, 1.3660, 1.0420, 1.3383],\n",
       "                      [1.2290, 1.1990, 1.1933, 1.0634, 1.2662, 0.7928, 1.2308, 1.1705],\n",
       "                      [0.9843, 1.2919, 1.2726, 1.0347, 1.2620, 1.0166, 1.3878, 1.0811],\n",
       "                      [1.3915, 1.5966, 1.5422, 1.4088, 1.5507, 1.4093, 1.3846, 1.3646],\n",
       "                      [1.6310, 1.3639, 1.3512, 1.4817, 1.4481, 1.5272, 1.4936, 1.5774],\n",
       "                      [1.3607, 1.2532, 1.4907, 0.7189, 1.2303, 1.1823, 0.9628, 1.1439],\n",
       "                      [0.8197, 1.3412, 1.3070, 1.2769, 1.3642, 1.1788, 1.0037, 1.1439],\n",
       "                      [1.3104, 1.3552, 1.2946, 1.3480, 0.9392, 1.3235, 0.8710, 1.1847],\n",
       "                      [1.5562, 1.5270, 1.2881, 1.4334, 1.4191, 1.4288, 1.3631, 1.5811],\n",
       "                      [1.5146, 1.3587, 1.4188, 1.4915, 1.4931, 1.5041, 1.4825, 1.4656],\n",
       "                      [1.4060, 1.4245, 1.4923, 1.5804, 1.3872, 1.5119, 1.3997, 1.3851],\n",
       "                      [1.3615, 1.4749, 1.3417, 1.4852, 1.5046, 1.4205, 1.4505, 1.4922],\n",
       "                      [1.3587, 1.4285, 1.5052, 1.3731, 1.4928, 1.3336, 1.4176, 1.3912],\n",
       "                      [1.0807, 1.0490, 1.3814, 1.2372, 1.1076, 0.8753, 1.2665, 1.2082]],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.patchs_features_extraction01.0.weight',\n",
       "              tensor([[[[ 0.0273,  0.0041],\n",
       "                        [-0.0101,  0.0178]],\n",
       "              \n",
       "                       [[ 0.0010, -0.0233],\n",
       "                        [ 0.0316, -0.0415]],\n",
       "              \n",
       "                       [[ 0.0072, -0.0213],\n",
       "                        [ 0.0079,  0.0382]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0120,  0.0139],\n",
       "                        [ 0.0153, -0.0049]],\n",
       "              \n",
       "                       [[ 0.0074, -0.0030],\n",
       "                        [ 0.0087, -0.0387]],\n",
       "              \n",
       "                       [[-0.0387,  0.0040],\n",
       "                        [-0.0133, -0.0380]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0313, -0.0229],\n",
       "                        [ 0.0370, -0.0350]],\n",
       "              \n",
       "                       [[-0.0314,  0.0231],\n",
       "                        [-0.0481,  0.0128]],\n",
       "              \n",
       "                       [[ 0.0112,  0.0430],\n",
       "                        [ 0.0575, -0.0206]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0052, -0.0035],\n",
       "                        [-0.0454,  0.0067]],\n",
       "              \n",
       "                       [[ 0.0207,  0.0044],\n",
       "                        [ 0.0015,  0.0004]],\n",
       "              \n",
       "                       [[-0.0139,  0.0075],\n",
       "                        [ 0.0382,  0.0591]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0082, -0.0093],\n",
       "                        [ 0.0107,  0.0159]],\n",
       "              \n",
       "                       [[-0.0086, -0.0506],\n",
       "                        [ 0.0527,  0.0355]],\n",
       "              \n",
       "                       [[ 0.0217,  0.0342],\n",
       "                        [-0.0086, -0.0478]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0229, -0.0011],\n",
       "                        [-0.0216, -0.0330]],\n",
       "              \n",
       "                       [[ 0.0337, -0.0171],\n",
       "                        [ 0.0530,  0.0145]],\n",
       "              \n",
       "                       [[-0.0441, -0.0094],\n",
       "                        [-0.0141,  0.0331]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0083, -0.0053],\n",
       "                        [ 0.0402, -0.0056]],\n",
       "              \n",
       "                       [[-0.0450,  0.0024],\n",
       "                        [-0.0221, -0.0228]],\n",
       "              \n",
       "                       [[-0.0119, -0.0407],\n",
       "                        [ 0.0252, -0.0224]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0293,  0.0144],\n",
       "                        [ 0.0106, -0.0095]],\n",
       "              \n",
       "                       [[ 0.0242,  0.0133],\n",
       "                        [-0.0120,  0.0118]],\n",
       "              \n",
       "                       [[ 0.0318,  0.0003],\n",
       "                        [-0.0392,  0.0397]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0339, -0.0682],\n",
       "                        [-0.0831, -0.0293]],\n",
       "              \n",
       "                       [[-0.0174,  0.0163],\n",
       "                        [ 0.0404,  0.0313]],\n",
       "              \n",
       "                       [[-0.0321, -0.0671],\n",
       "                        [ 0.0317, -0.0437]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0207,  0.0309],\n",
       "                        [-0.0679, -0.0314]],\n",
       "              \n",
       "                       [[ 0.0137, -0.0565],\n",
       "                        [ 0.0214, -0.0357]],\n",
       "              \n",
       "                       [[ 0.0413, -0.0141],\n",
       "                        [ 0.0104,  0.0224]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0243,  0.0271],\n",
       "                        [ 0.0043,  0.0328]],\n",
       "              \n",
       "                       [[ 0.0249,  0.0281],\n",
       "                        [ 0.0490,  0.0421]],\n",
       "              \n",
       "                       [[-0.0283, -0.0496],\n",
       "                        [-0.0008,  0.0255]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0341,  0.0051],\n",
       "                        [ 0.0319, -0.0077]],\n",
       "              \n",
       "                       [[ 0.0336, -0.0048],\n",
       "                        [-0.0130,  0.0240]],\n",
       "              \n",
       "                       [[-0.0366,  0.0506],\n",
       "                        [ 0.0127, -0.0598]]]], device='cuda:0')),\n",
       "             ('local_filter.patchs_features_extraction01.1.weight',\n",
       "              tensor([[[[-0.0055]],\n",
       "              \n",
       "                       [[-0.0667]],\n",
       "              \n",
       "                       [[ 0.0252]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0094]],\n",
       "              \n",
       "                       [[ 0.0669]],\n",
       "              \n",
       "                       [[-0.0009]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0030]],\n",
       "              \n",
       "                       [[-0.0443]],\n",
       "              \n",
       "                       [[ 0.0670]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0571]],\n",
       "              \n",
       "                       [[ 0.0402]],\n",
       "              \n",
       "                       [[-0.0444]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0784]],\n",
       "              \n",
       "                       [[-0.0703]],\n",
       "              \n",
       "                       [[ 0.0239]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0227]],\n",
       "              \n",
       "                       [[ 0.0212]],\n",
       "              \n",
       "                       [[-0.0214]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0923]],\n",
       "              \n",
       "                       [[-0.0232]],\n",
       "              \n",
       "                       [[-0.0372]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0362]],\n",
       "              \n",
       "                       [[-0.0053]],\n",
       "              \n",
       "                       [[ 0.0411]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0712]],\n",
       "              \n",
       "                       [[-0.0683]],\n",
       "              \n",
       "                       [[-0.0280]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0529]],\n",
       "              \n",
       "                       [[-0.0319]],\n",
       "              \n",
       "                       [[ 0.0757]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0804]],\n",
       "              \n",
       "                       [[-0.0158]],\n",
       "              \n",
       "                       [[ 0.0557]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0638]],\n",
       "              \n",
       "                       [[ 0.0184]],\n",
       "              \n",
       "                       [[-0.0047]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule01.stats_kernel_p01',\n",
       "              tensor([[[[1.0159]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9997]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0295]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9691]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9979]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9887]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0308]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9770]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0429]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9886]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0312]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9893]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0094]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0233]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9848]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0109]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0930]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0041]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1821]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1184]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1326]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1035]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1261]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1485]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0674]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0184]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9793]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1159]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0257]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0761]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0015]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0361]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0448]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0683]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1281]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0018]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0490]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0642]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9449]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0449]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9876]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9832]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1183]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1078]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9878]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0045]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0739]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9857]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0075]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9880]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9841]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9401]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0440]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9890]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0361]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0466]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0598]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9011]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1130]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9976]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0540]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0932]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.2659]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0668]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0237]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9573]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9781]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1634]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0582]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0295]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9576]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.2159]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9939]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0753]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0630]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0435]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0518]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0314]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1187]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0033]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9922]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0029]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9963]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9967]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9956]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9956]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9937]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9983]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0084]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9688]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9961]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0903]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0274]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0318]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0147]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9755]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0560]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0427]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9731]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9972]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9823]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9949]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9677]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9894]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9574]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0212]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0211]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9866]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9616]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1091]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9917]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0538]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9968]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9958]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9951]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9902]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9992]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0013]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9993]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9968]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9774]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9841]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0153]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0684]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0324]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9491]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0345]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9951]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule01.stats_kernel_p02a',\n",
       "              tensor([[[[0.5187]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5225]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5026]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5091]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4977]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5159]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4851]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5567]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4898]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5256]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4550]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5372]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5028]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4730]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5195]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5070]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4709]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5091]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3889]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4743]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4149]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4132]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4754]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4455]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4424]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4897]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4218]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5099]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5111]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5273]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4468]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5203]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4748]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4343]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5476]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4899]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5358]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5693]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4974]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5212]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5257]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4393]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4443]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5465]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4924]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4895]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5198]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5193]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5131]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5069]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5728]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4904]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5391]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4747]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4936]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5358]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5655]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4576]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5643]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5032]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5024]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3409]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5474]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5349]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5540]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5494]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3749]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5418]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5124]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5429]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3988]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5683]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4370]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4577]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4727]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4365]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4564]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5068]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5423]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5124]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4965]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5028]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5012]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5078]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5066]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5085]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5008]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4973]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5299]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5157]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4621]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4922]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4679]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4611]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5293]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4582]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4785]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5381]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5061]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5293]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4958]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5416]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5141]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5436]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5103]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5197]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5440]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5584]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4451]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5044]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4765]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5043]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5051]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5059]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5118]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5011]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4991]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5028]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5063]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5265]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5606]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5120]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4615]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4743]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5414]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5056]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5184]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule01.stats_kernel_p02b',\n",
       "              tensor([[[[0.4964]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5146]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4862]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5436]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5090]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5256]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5002]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5444]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5207]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5329]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4774]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5596]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4788]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4766]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5180]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5001]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4521]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4819]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4240]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4235]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4483]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4346]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4503]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4177]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4706]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5041]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5046]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4176]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4925]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5028]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5275]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4323]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4721]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4773]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4110]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5490]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4439]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5481]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5324]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5025]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5124]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4038]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4781]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5018]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4853]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4492]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5057]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5046]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4940]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5384]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5949]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4675]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5349]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4928]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4785]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4960]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5003]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4277]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5409]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5379]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5897]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3659]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5311]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5578]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5439]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5175]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4547]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5136]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5090]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5489]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3705]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5375]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4585]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4432]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4875]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4317]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5153]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4590]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5111]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4998]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5015]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5031]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5056]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5063]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5060]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5058]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5260]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5237]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5116]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5107]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4749]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4588]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4913]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4858]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4306]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4767]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5439]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5101]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5250]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5092]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5412]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5246]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5620]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5118]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5199]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5460]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5661]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4493]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5088]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5050]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5048]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5069]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5087]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5136]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5015]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5003]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5024]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5036]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5320]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5523]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5094]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4350]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4945]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5444]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5092]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5516]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule01.stats_kernel_p03',\n",
       "              tensor([[[[0.4942]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4450]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5409]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4346]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4979]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4427]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5317]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3944]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5055]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4385]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5579]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3996]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5344]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5620]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4542]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4868]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5794]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4938]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6751]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5960]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6408]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6344]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5865]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6149]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5784]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5189]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4723]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6668]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5083]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5328]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4453]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6137]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4949]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5522]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6399]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4228]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5448]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4772]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3682]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5350]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4592]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4276]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6454]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6142]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4469]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5289]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5835]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4633]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4808]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4874]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4572]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3151]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5638]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4165]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5595]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5606]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5065]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4531]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6354]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4319]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4820]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5064]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.7151]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4769]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4613]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4022]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4316]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6590]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5147]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4790]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4238]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6941]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4277]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5852]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6036]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5421]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6459]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5291]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5395]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4680]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5037]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4899]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4894]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4744]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4755]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4725]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4879]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4880]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4231]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4756]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5646]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5324]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5637]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5276]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4814]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6074]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5815]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3957]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4733]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4319]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4916]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3871]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4485]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3709]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4967]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4699]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4216]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3778]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6049]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4787]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5446]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4818]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4773]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4722]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4532]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4940]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5025]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4908]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4820]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4526]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4224]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4823]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6361]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5376]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3726]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5002]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4364]]]], device='cuda:0')),\n",
       "             ('local_filter.GTVmodule01.multiM',\n",
       "              tensor([[1.0204, 1.0033, 1.0209, 1.0060, 1.0022, 1.0139, 1.0233, 1.0281],\n",
       "                      [1.0482, 1.0218, 1.0267, 1.0390, 1.0348, 1.0156, 1.0792, 1.0404],\n",
       "                      [1.0161, 1.0370, 0.9967, 1.0833, 1.0230, 1.0670, 0.9940, 1.0308],\n",
       "                      [1.0237, 0.9856, 0.9830, 1.0057, 1.0215, 0.9990, 0.9989, 1.0296],\n",
       "                      [1.0890, 1.0261, 1.0711, 1.0829, 1.0216, 1.1914, 1.2114, 1.2176],\n",
       "                      [1.0661, 1.0255, 1.0371, 1.0245, 1.0977, 1.0681, 1.0545, 1.0263],\n",
       "                      [1.0236, 1.0374, 1.0190, 1.0410, 1.0452, 1.0222, 1.0827, 1.0291],\n",
       "                      [1.1664, 1.0958, 1.1517, 1.1273, 1.1507, 1.1452, 1.1586, 1.0499],\n",
       "                      [1.0779, 1.1079, 1.1673, 1.0859, 1.0944, 1.0415, 1.1365, 1.0944],\n",
       "                      [1.1236, 1.1699, 1.0893, 1.0449, 1.0354, 1.1516, 1.1507, 1.0831],\n",
       "                      [1.0021, 1.0016, 1.0005, 1.0030, 1.0030, 1.0014, 1.0007, 1.0014],\n",
       "                      [1.0560, 1.0243, 1.0286, 1.1123, 1.0665, 1.0313, 1.0542, 1.1190],\n",
       "                      [0.9954, 1.0261, 1.0282, 1.0097, 1.0403, 1.0163, 1.0061, 1.0192],\n",
       "                      [1.0376, 1.0590, 1.0271, 1.0291, 1.0143, 1.0588, 1.0447, 1.1040],\n",
       "                      [1.0013, 1.0015, 1.0014, 0.9988, 1.0030, 1.0004, 1.0022, 1.0018],\n",
       "                      [1.0330, 1.0267, 1.1032, 1.0257, 1.0268, 1.1545, 1.0288, 1.0612]],\n",
       "                     device='cuda:0')),\n",
       "             ('local_filter.GLRmodule01.stats_kernel_p01',\n",
       "              tensor([[[[1.0696]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9938]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0990]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9029]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0231]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9379]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0860]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9096]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9669]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9284]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9645]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9559]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9621]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9610]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9466]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9619]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0347]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9411]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1302]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0391]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0700]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0347]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9963]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0473]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8820]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9617]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9345]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0634]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9327]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9795]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0116]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9849]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9873]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9716]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0339]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9574]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9720]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9722]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9080]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0043]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0743]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0968]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8001]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8072]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1001]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0476]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8607]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0725]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9907]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9617]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8665]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0840]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9532]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1360]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1159]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9797]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9187]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0454]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9400]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9927]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9559]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0677]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0083]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9721]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9610]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9292]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0552]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9810]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9297]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9616]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0022]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9282]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9874]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0261]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9828]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0215]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9990]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0522]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9696]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9950]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9601]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0273]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0236]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0275]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0750]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0191]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0562]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0326]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0817]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0747]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8357]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9193]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9817]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0012]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0362]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9765]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9861]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9357]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9741]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9364]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9748]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9252]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9794]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9100]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0127]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0444]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9308]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9229]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0982]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9520]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0757]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0868]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9942]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0632]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0494]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0475]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8561]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0072]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0628]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9169]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9418]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9772]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0792]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0182]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.8697]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9796]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.9466]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule01.stats_kernel_p02a',\n",
       "              tensor([[[[0.5439]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6148]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4910]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5403]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4818]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5911]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4872]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6761]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5621]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5861]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5341]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5738]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5481]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5377]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5502]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5584]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5260]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5690]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4405]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5575]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4920]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4899]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5967]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5235]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6481]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5252]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5674]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4825]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5894]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5780]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5049]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4970]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5658]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5751]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5211]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5920]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5568]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6166]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6211]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5463]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4533]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3613]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5829]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6547]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3613]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4741]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6153]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4456]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5406]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5084]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5110]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6526]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5316]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6198]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4294]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5355]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6039]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5409]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5236]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5981]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5484]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6173]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5371]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5957]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5611]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5364]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6021]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4646]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6008]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6109]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5409]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5695]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6097]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5430]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5106]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5491]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4985]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4773]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5389]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5589]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5234]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5765]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5118]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5108]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4543]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3864]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4798]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4907]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5038]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4266]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4395]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5581]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5459]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5442]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5438]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4737]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5431]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5537]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5918]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5434]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5800]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5212]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5788]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5267]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6036]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5526]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5493]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6218]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6228]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4874]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5334]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5025]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4131]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4799]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4649]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4661]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4650]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5506]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4561]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4506]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5865]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6253]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5625]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4635]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5128]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5958]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5753]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5822]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule01.stats_kernel_p02b',\n",
       "              tensor([[[[0.4646]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5588]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4460]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6017]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5043]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6398]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5364]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6141]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5857]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5944]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5566]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5856]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5335]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5331]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5436]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5547]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5239]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5646]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4733]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5097]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5567]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5543]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5323]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5073]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6431]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5635]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5866]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4671]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5806]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5781]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5423]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5101]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5223]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5799]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5156]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5961]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5311]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6219]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5855]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5642]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4449]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3862]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6597]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5670]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4465]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4845]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6086]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5006]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4968]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4786]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6050]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6917]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4914]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6124]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4781]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5030]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5761]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5049]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5004]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5889]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5844]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6798]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5743]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5645]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6016]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5429]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5898]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5171]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5845]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6040]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5584]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5642]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6056]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5360]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4915]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5539]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5047]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5430]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5073]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5855]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5026]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4807]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5126]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5341]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4782]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3965]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5013]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4415]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4265]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4217]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4314]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5199]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5906]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5663]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4973]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5183]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5079]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5471]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5734]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5373]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5751]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5453]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5849]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5571]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6270]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5660]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5775]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6361]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6312]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4926]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5363]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5394]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3976]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4897]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4791]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4267]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4783]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5771]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4556]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4116]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6002]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6027]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5780]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4664]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5322]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6032]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5741]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6069]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule01.stats_kernel_p03',\n",
       "              tensor([[[[0.5003]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3774]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5615]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3957]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5173]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3440]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5062]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3316]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3750]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3731]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4185]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3484]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4131]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4224]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3999]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3956]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4764]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4214]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5546]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4616]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4906]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4969]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4263]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4875]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3072]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4234]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3640]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5151]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3576]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3845]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4425]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4729]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4428]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3882]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4786]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3387]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4228]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3549]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3161]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4264]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5662]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6604]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3230]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.2877]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6216]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5380]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3353]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5287]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4793]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5023]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4296]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.2608]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5144]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3203]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5898]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5159]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4066]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4707]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4914]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3850]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4257]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3423]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4427]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3946]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4015]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4216]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3427]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5195]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3897]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3478]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4351]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4227]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3609]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4334]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5076]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4444]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5142]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4745]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4653]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4234]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4714]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4391]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4820]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4810]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5259]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6310]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4956]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5576]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5356]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5992]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5859]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3741]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4122]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4457]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4711]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4878]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4156]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4172]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3483]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4096]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3521]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4298]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3542]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4206]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.2983]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4412]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4139]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3408]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3242]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5246]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4547]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4837]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.6127]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4867]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5536]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5556]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5459]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3394]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5435]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5824]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3773]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3564]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3978]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.5399]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4629]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.2946]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.4118]]],\n",
       "              \n",
       "              \n",
       "                      [[[0.3589]]]], device='cuda:0')),\n",
       "             ('local_filter.GLRmodule01.multiM',\n",
       "              tensor([[1.1493, 1.1196, 1.1291, 1.0352, 1.0764, 1.1261, 1.0789, 1.1229],\n",
       "                      [1.0660, 1.0423, 1.1008, 1.0175, 1.0382, 1.0498, 1.0180, 1.0139],\n",
       "                      [1.4344, 1.3350, 1.1142, 1.2601, 1.4210, 1.2201, 1.2286, 1.3357],\n",
       "                      [1.0606, 1.0476, 1.1745, 1.0357, 1.0390, 1.0272, 1.0314, 1.0502],\n",
       "                      [1.0644, 1.0298, 1.0715, 1.0830, 1.1980, 1.1807, 1.0657, 1.1150],\n",
       "                      [1.1598, 1.0725, 1.1831, 1.1688, 1.0738, 0.9932, 1.0191, 1.0711],\n",
       "                      [1.0668, 1.1300, 1.3836, 1.1554, 1.0876, 1.0847, 1.2428, 1.0954],\n",
       "                      [1.0755, 1.2087, 1.1692, 1.1244, 1.2747, 1.1743, 1.1113, 1.0737],\n",
       "                      [1.1076, 1.1090, 1.1644, 1.0704, 1.0648, 1.1314, 1.0267, 1.0551],\n",
       "                      [1.0349, 1.0679, 1.0665, 1.1408, 1.2511, 1.1690, 1.1086, 1.1473],\n",
       "                      [1.0008, 0.9791, 0.9762, 1.0965, 1.0103, 1.2778, 1.0289, 1.0378],\n",
       "                      [1.0924, 1.1208, 1.0867, 1.0299, 1.0913, 1.0067, 1.1362, 1.0657],\n",
       "                      [1.0179, 1.0256, 1.0210, 1.0082, 1.0639, 1.0226, 1.0274, 1.0543],\n",
       "                      [1.3145, 1.1572, 1.0659, 1.1385, 1.0408, 1.1734, 1.0514, 1.0991],\n",
       "                      [1.2531, 1.0789, 1.1446, 1.0013, 1.0634, 1.2805, 1.0789, 1.0890],\n",
       "                      [1.0941, 1.0190, 1.2030, 1.0421, 1.1277, 1.2247, 1.0404, 1.1935]],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.localfilter_scale_01.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da799157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 1.1436, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]]], grad_fn=<SelectBackward0>),\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.4134,  0.4134],\n",
       "          [ 0.0000,  0.0000,  0.0000]]], grad_fn=<SelectBackward0>),\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.4260,  0.0000],\n",
       "          [ 0.0000,  0.4260,  0.0000]]], grad_fn=<SelectBackward0>),\n",
       " tensor([[[ 0.0000, -0.5591,  0.0000],\n",
       "          [-0.5591,  2.2365, -0.5591],\n",
       "          [ 0.0000, -0.5591,  0.0000]]], grad_fn=<SelectBackward0>),\n",
       " tensor([[[ 0.0000, -0.5591,  0.0000],\n",
       "          [-0.5591,  2.5407, -0.1457],\n",
       "          [ 0.0000, -0.1331,  0.0000]]], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_kernel = (\n",
    "    model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p01.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel01,\n",
    "    model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p02a.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel02a,\n",
    "    model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p02b.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel02b,\n",
    "    model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p03.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel03,\n",
    "\n",
    "    model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p01.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel01\n",
    "    + model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p02a.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel02a\n",
    "    + model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p02b.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel02b\n",
    "    + model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel_p03.cpu() * model.localfilter_scale_00.local_filter.GLRmodule00.stats_kernel03\n",
    ")\n",
    "stats_kernel[0][0], stats_kernel[1][0], stats_kernel[2][0], stats_kernel[3][0], stats_kernel[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad114d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -5.5913e-01,  0.0000e+00],\n",
       "          [-5.5913e-01,  2.5407e+00, -1.4569e-01],\n",
       "          [ 0.0000e+00, -1.3310e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9232e-01,  0.0000e+00],\n",
       "          [-5.9232e-01,  3.0515e+00, -1.9657e-01],\n",
       "          [ 0.0000e+00, -1.9738e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.7306e-01,  0.0000e+00],\n",
       "          [-4.7306e-01,  1.7697e+00, -7.3721e-02],\n",
       "          [ 0.0000e+00,  2.3332e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.6580e-01,  0.0000e+00],\n",
       "          [-5.6580e-01,  3.1604e+00, -3.1276e-02],\n",
       "          [ 0.0000e+00, -2.2582e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.7582e-01,  0.0000e+00],\n",
       "          [-5.7582e-01,  2.7265e+00, -3.1244e-01],\n",
       "          [ 0.0000e+00, -9.5194e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.5698e-01,  0.0000e+00],\n",
       "          [-5.5698e-01,  2.4875e+00, -3.1543e-02],\n",
       "          [ 0.0000e+00, -2.3825e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1768e-01,  0.0000e+00],\n",
       "          [-6.1768e-01,  3.0835e+00, -2.5056e-01],\n",
       "          [ 0.0000e+00, -1.1593e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9038e-01,  0.0000e+00],\n",
       "          [-5.9038e-01,  2.6794e+00, -3.0918e-02],\n",
       "          [ 0.0000e+00, -2.4089e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.2902e-01,  0.0000e+00],\n",
       "          [-4.2902e-01,  1.6198e+00,  1.5503e-01],\n",
       "          [ 0.0000e+00,  8.4312e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1046e-01,  0.0000e+00],\n",
       "          [-6.1046e-01,  3.0031e+00, -1.9157e-01],\n",
       "          [ 0.0000e+00, -1.5160e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.0140e-01,  0.0000e+00],\n",
       "          [-5.0140e-01,  2.3363e+00, -2.4228e-01],\n",
       "          [ 0.0000e+00, -4.6712e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.6368e-01,  0.0000e+00],\n",
       "          [-6.6368e-01,  3.2107e+00, -3.3623e-01],\n",
       "          [ 0.0000e+00, -1.8888e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9170e-01,  0.0000e+00],\n",
       "          [-5.9170e-01,  3.2101e+00, -1.0348e-01],\n",
       "          [ 0.0000e+00, -2.3287e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.3718e-01,  0.0000e+00],\n",
       "          [-5.3718e-01,  2.1891e+00, -8.5832e-02],\n",
       "          [ 0.0000e+00,  6.6532e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.5908e-01,  0.0000e+00],\n",
       "          [-4.5908e-01,  1.8413e+00,  1.0815e-01],\n",
       "          [ 0.0000e+00, -9.3643e-03,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1229e-01,  0.0000e+00],\n",
       "          [-6.1229e-01,  3.3148e+00, -2.3190e-01],\n",
       "          [ 0.0000e+00, -1.3712e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.2085e-01,  0.0000e+00],\n",
       "          [-6.2085e-01,  3.1278e+00, -1.9724e-01],\n",
       "          [ 0.0000e+00, -1.4592e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.8995e-01,  0.0000e+00],\n",
       "          [-5.8995e-01,  2.9388e+00, -3.0206e-01],\n",
       "          [ 0.0000e+00, -1.5392e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9416e-01,  0.0000e+00],\n",
       "          [-5.9416e-01,  2.9244e+00, -2.4537e-01],\n",
       "          [ 0.0000e+00, -3.9136e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.5810e-01,  0.0000e+00],\n",
       "          [-5.5810e-01,  2.5571e+00, -1.8506e-02],\n",
       "          [ 0.0000e+00, -1.9709e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.3952e-01,  0.0000e+00],\n",
       "          [-6.3952e-01,  3.3161e+00, -2.9595e-01],\n",
       "          [ 0.0000e+00, -2.2226e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -7.3272e-01,  0.0000e+00],\n",
       "          [-7.3272e-01,  4.1970e+00, -3.9275e-01],\n",
       "          [ 0.0000e+00, -4.9524e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.6798e-01,  0.0000e+00],\n",
       "          [-5.6798e-01,  2.5609e+00, -9.4165e-02],\n",
       "          [ 0.0000e+00, -1.9432e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.2761e-01,  0.0000e+00],\n",
       "          [-4.2761e-01,  1.5833e+00,  1.5691e-01],\n",
       "          [ 0.0000e+00,  1.4739e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.6805e-01,  0.0000e+00],\n",
       "          [-5.6805e-01,  2.6318e+00, -1.2680e-01],\n",
       "          [ 0.0000e+00, -1.8737e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.4926e-01,  0.0000e+00],\n",
       "          [-4.4926e-01,  2.1164e+00,  2.4405e-01],\n",
       "          [ 0.0000e+00, -2.2058e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.8961e-01,  0.0000e+00],\n",
       "          [-5.8961e-01,  2.8550e+00, -9.6472e-02],\n",
       "          [ 0.0000e+00, -2.7925e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.6970e-01,  0.0000e+00],\n",
       "          [-5.6970e-01,  2.9294e+00,  5.3413e-03],\n",
       "          [ 0.0000e+00, -8.6994e-03,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.7505e-01,  0.0000e+00],\n",
       "          [-4.7505e-01,  2.0817e+00,  1.6101e-01],\n",
       "          [ 0.0000e+00, -9.6494e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1050e-01,  0.0000e+00],\n",
       "          [-6.1050e-01,  2.7031e+00, -1.4837e-01],\n",
       "          [ 0.0000e+00, -1.3244e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.4955e-01,  0.0000e+00],\n",
       "          [-5.4955e-01,  2.8979e+00, -2.1674e-01],\n",
       "          [ 0.0000e+00, -1.3071e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.9431e-01,  0.0000e+00],\n",
       "          [-4.9431e-01,  2.1155e+00,  8.8901e-02],\n",
       "          [ 0.0000e+00, -7.1134e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9666e-01,  0.0000e+00],\n",
       "          [-5.9666e-01,  3.0187e+00, -4.3244e-01],\n",
       "          [ 0.0000e+00, -7.3645e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.8173e-01,  0.0000e+00],\n",
       "          [-5.8173e-01,  2.7400e+00, -2.2684e-01],\n",
       "          [ 0.0000e+00, -3.8853e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.7762e-01,  0.0000e+00],\n",
       "          [-5.7762e-01,  2.8049e+00, -2.8910e-01],\n",
       "          [ 0.0000e+00, -1.6385e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.7404e-01,  0.0000e+00],\n",
       "          [-6.7404e-01,  3.1485e+00, -1.8439e-01],\n",
       "          [ 0.0000e+00, -3.4517e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -7.5707e-01,  0.0000e+00],\n",
       "          [-7.5707e-01,  3.7252e+00, -3.2015e-01],\n",
       "          [ 0.0000e+00, -5.1694e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9320e-01,  0.0000e+00],\n",
       "          [-5.9320e-01,  2.5815e+00, -2.7274e-01],\n",
       "          [ 0.0000e+00, -3.4053e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.5905e-01,  0.0000e+00],\n",
       "          [-5.5905e-01,  2.7045e+00, -1.6252e-01],\n",
       "          [ 0.0000e+00, -2.6686e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.0014e-01,  0.0000e+00],\n",
       "          [-6.0014e-01,  2.8279e+00, -2.1655e-01],\n",
       "          [ 0.0000e+00, -1.7765e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.9195e-01,  0.0000e+00],\n",
       "          [-4.9195e-01,  2.1006e+00,  6.8623e-02],\n",
       "          [ 0.0000e+00,  1.1345e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.2671e-01,  0.0000e+00],\n",
       "          [-5.2671e-01,  2.7489e+00, -1.7860e-01],\n",
       "          [ 0.0000e+00, -1.6582e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.9951e-01,  0.0000e+00],\n",
       "          [-5.9951e-01,  2.7860e+00, -1.5202e-01],\n",
       "          [ 0.0000e+00, -2.7101e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.8484e-01,  0.0000e+00],\n",
       "          [-5.8484e-01,  2.6594e+00, -2.8815e-01],\n",
       "          [ 0.0000e+00, -1.2816e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.2132e-01,  0.0000e+00],\n",
       "          [-5.2132e-01,  2.1016e+00, -7.0243e-02],\n",
       "          [ 0.0000e+00,  6.1170e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.6959e-01,  0.0000e+00],\n",
       "          [-5.6959e-01,  2.7003e+00, -3.0932e-01],\n",
       "          [ 0.0000e+00, -1.3165e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1622e-01,  0.0000e+00],\n",
       "          [-6.1622e-01,  2.8255e+00, -1.2310e-01],\n",
       "          [ 0.0000e+00, -2.0346e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.7375e-01,  0.0000e+00],\n",
       "          [-5.7375e-01,  2.5664e+00,  5.9152e-04],\n",
       "          [ 0.0000e+00, -3.1846e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.0476e-01,  0.0000e+00],\n",
       "          [-5.0476e-01,  3.0535e+00,  2.2551e-02],\n",
       "          [ 0.0000e+00, -8.3845e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.5198e-01,  0.0000e+00],\n",
       "          [-5.5198e-01,  2.3613e+00, -4.6001e-02],\n",
       "          [ 0.0000e+00, -1.2848e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.0799e-01,  0.0000e+00],\n",
       "          [-5.0799e-01,  2.1764e+00, -1.8324e-01],\n",
       "          [ 0.0000e+00,  4.8334e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.3712e-01,  0.0000e+00],\n",
       "          [-6.3712e-01,  3.2820e+00, -1.7425e-01],\n",
       "          [ 0.0000e+00, -2.1870e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.5745e-01,  0.0000e+00],\n",
       "          [-6.5745e-01,  3.2583e+00, -2.1743e-01],\n",
       "          [ 0.0000e+00, -4.6702e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -4.4852e-01,  0.0000e+00],\n",
       "          [-4.4852e-01,  1.7340e+00,  2.5397e-01],\n",
       "          [ 0.0000e+00,  2.7886e-03,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.2984e-01,  0.0000e+00],\n",
       "          [-5.2984e-01,  2.2177e+00, -1.1338e-01],\n",
       "          [ 0.0000e+00,  7.7866e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1081e-01,  0.0000e+00],\n",
       "          [-6.1081e-01,  3.3896e+00, -1.4831e-01],\n",
       "          [ 0.0000e+00, -2.4870e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -7.3392e-01,  0.0000e+00],\n",
       "          [-7.3392e-01,  3.8812e+00, -4.9062e-01],\n",
       "          [ 0.0000e+00, -5.5261e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.8084e-01,  0.0000e+00],\n",
       "          [-6.8084e-01,  3.7665e+00, -4.4841e-01],\n",
       "          [ 0.0000e+00, -1.6868e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.1673e-01,  0.0000e+00],\n",
       "          [-6.1673e-01,  3.0641e+00, -3.4058e-01],\n",
       "          [ 0.0000e+00, -2.2293e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.5588e-01,  0.0000e+00],\n",
       "          [-5.5588e-01,  2.5013e+00, -1.4694e-01],\n",
       "          [ 0.0000e+00, -1.8949e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -7.3407e-01,  0.0000e+00],\n",
       "          [-7.3407e-01,  3.7752e+00, -4.5046e-01],\n",
       "          [ 0.0000e+00, -3.6186e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -6.0152e-01,  0.0000e+00],\n",
       "          [-6.0152e-01,  2.9314e+00, -3.0931e-01],\n",
       "          [ 0.0000e+00, -2.9601e-01,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -5.4442e-01,  0.0000e+00],\n",
       "          [-5.4442e-01,  2.3423e+00, -1.4856e-01],\n",
       "          [ 0.0000e+00, -1.8425e-02,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -7.4195e-01,  0.0000e+00],\n",
       "          [-7.4195e-01,  3.7624e+00, -4.2545e-01],\n",
       "          [ 0.0000e+00, -3.5043e-01,  0.0000e+00]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_kernel[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38bdaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a760df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_PROJECT, \"exploration/model_multiscale_mixture_GLR/scripts/\")\n",
    "LOGGER = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    filename=os.path.join(LOG_DIR, 'non.log'), \n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "VERBOSE_RATE = 1000\n",
    "\n",
    "(H_train01, W_train01) = (64, 64)\n",
    "(H_train02, W_train02) = (128, 128)\n",
    "(H_train03, W_train03) = (256, 256)\n",
    "(H_train04, W_train04) = (512, 512)\n",
    "\n",
    "\n",
    "train_dataset01 = ImageSuperResolution(\n",
    "    csv_path=os.path.join(ROOT_DATASET, \"dataset/DFWB_training_data_info.csv\"),\n",
    "    dist_mode=\"vary_addictive_noise\",\n",
    "    lambda_noise=[[1.0, 10.0, 15.0, 20.0, 25.0], [0.1, 0.1, 0.1, 0.1, 0.6]],\n",
    "    use_data_aug=True,\n",
    "    patch_size=(H_train02,W_train02),\n",
    "    max_num_patchs=800000,\n",
    "    root_folder=ROOT_DATASET,\n",
    "    logger=LOGGER,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "data_train_batched01 = torch.utils.data.DataLoader(\n",
    "    train_dataset01, batch_size=16, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f9106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561dba72",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1124885",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c8835",
   "metadata": {},
   "source": [
    "# 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606ceba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8a0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee1aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da24bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
